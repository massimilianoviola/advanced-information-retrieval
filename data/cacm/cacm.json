{"DOCID": "1", "TEXT": "Preliminary Report-International Algebraic Language"}
{"DOCID": "2", "TEXT": "Extraction of Roots by Repeated Subtractions for Digital Computers"}
{"DOCID": "3", "TEXT": "Techniques Department on Matrix Program Schemes"}
{"DOCID": "4", "TEXT": "Glossary of Computer Engineering and Programming Terminology"}
{"DOCID": "5", "TEXT": "Two Square-Root Approximations"}
{"DOCID": "6", "TEXT": "The Use of Computers in Inspection Procedures"}
{"DOCID": "7", "TEXT": "Glossary of Computer Engineering and Programming Terminology"}
{"DOCID": "8", "TEXT": "On The Equivalence and Transformation of Program Schemes"}
{"DOCID": "9", "TEXT": "Proposal for an UNCOL"}
{"DOCID": "10", "TEXT": "Glossary of Computer Engineering and Programming Terminology"}
{"DOCID": "11", "TEXT": "The Problem of Programming Communication with Changing Machines A Proposed Solution-Part 2"}
{"DOCID": "12", "TEXT": "Error Estimation in Runge-Kutta Procedures"}
{"DOCID": "13", "TEXT": "Glossary of Computer Engineering and Programming Terminology"}
{"DOCID": "14", "TEXT": "The Problem of Programming Communication with Changing Machines A Proposed Solution (Part 1)"}
{"DOCID": "15", "TEXT": "Recursive Curve Fitting Technique"}
{"DOCID": "16", "TEXT": "Secant Modification of Newton's Method"}
{"DOCID": "17", "TEXT": "On Programming of Arithmetic Operations"}
{"DOCID": "18", "TEXT": "Simple Automatic Coding Systems"}
{"DOCID": "19", "TEXT": "Glossary of Computer Engineering and Programming Terminology"}
{"DOCID": "20", "TEXT": "Accelerating Convergence of Iterative Processes: A technique is discussed which, when applied to an iterative procedure for the solution of an equation, accelerates the rate of convergence if the iteration converges and induces convergence if the iteration diverges.  An illustrative example is given."}
{"DOCID": "21", "TEXT": "Algebraic Formulation of Flow Diagrams"}
{"DOCID": "22", "TEXT": "Unusual Applications Department--Automatic Implementation of Computer Logic"}
{"DOCID": "23", "TEXT": "Binary and Truth-Function Operations on a Decimal Computer with an Extract Command"}
{"DOCID": "24", "TEXT": "An Improved Decimal Redundancy Check"}
{"DOCID": "25", "TEXT": "General Purpose Programming Systems"}
{"DOCID": "26", "TEXT": "A Subroutine Method for Calculating Logarithms"}
{"DOCID": "27", "TEXT": "Note On Empirical Bounds For Generating Bessel Functions"}
{"DOCID": "28", "TEXT": "Request for Methods or Programs"}
{"DOCID": "29", "TEXT": "Need for an Algorithm"}
{"DOCID": "30", "TEXT": "Algorithm for Analyzing Logical Statements to Produce a Truth Function Table"}
{"DOCID": "31", "TEXT": "IBM 704 Code-Nundrums"}
{"DOCID": "32", "TEXT": "Variable-Width Tables with Binary-Search Facility"}
{"DOCID": "33", "TEXT": "A Programmed Binary Counter For The IBM Type 650 Calculator"}
{"DOCID": "34", "TEXT": "Tables for Automatic Computation"}
{"DOCID": "35", "TEXT": "A Machine Method for Square-Root Computation"}
{"DOCID": "36", "TEXT": "A Queue Network Simulator for the IBM 650 and Burroughs 220"}
{"DOCID": "37", "TEXT": "Impact of Computer Developments"}
{"DOCID": "38", "TEXT": "A Proposed Interpretation in ALGOL"}
{"DOCID": "39", "TEXT": "The Secant Method for Simultaneous Nonlinear Equations: A procedure for the simultaneous solution of a system of not-necessarily-linear equations, a generalization of the secant method for a single function of one variable, is given."}
{"DOCID": "40", "TEXT": "Fingers or Fists? (The Choice of Decimal or Binary Representation): The binary number system offers many advantages over a decimal representation for a high-performance, general-purpose computer.  The greater simplicity of a binary arithmetic unit and the greater compactness of binary numbers both contribute directly to arithmetic speed.  Less obvious and perhaps more important is the way binary addressing and instruction formats can increase the overall performance.  Binary addresses are also essential to certain powerful operations which are not practical with decimal instruction formats. On the other hand, decimal numbers are essential for communicating between man and the computer.  In applications requiring the processing of a large volume of inherently decimal input and output data, the time for decimal-binary conversion needed by a purely binary computer may be significant.  A slower decimal adder may take less time than a fast binary adder doing an addition and two conversions.  A careful review of the significance of decimal and binary addressing and both binary and decimal data arithmetic, supplemented by efficient conversion instructions."}
{"DOCID": "41", "TEXT": "Some Notes on Computer Research in Eastern Europe"}
{"DOCID": "42", "TEXT": "A New Method of Computation of Square Roots Without Using Division"}
{"DOCID": "43", "TEXT": "A Technique for Handling Macro Instructions"}
{"DOCID": "44", "TEXT": "RUNCIBLE-Algebraic Translation on a Limited Computer"}
{"DOCID": "45", "TEXT": "Flow Outlining-A Substitute for Flow Charting"}
{"DOCID": "46", "TEXT": "Multiprogramming STRETCH: Feasibility Considerations: The tendency towards increased parallelism in computers is noted.  Exploitation of this parallelism presents a number of new problems in machine design and in programming systems.  Minimum requirements for successful concurrent execution of several independent problem programs are discussed.  These requirements are met in the STRETCH system by a carefully balanced combination of built-in and programmed logic. Techniques are described which place the burden of the programmed logic on system programs (supervisory program and compiler) rather than on problem programs."}
{"DOCID": "47", "TEXT": "Russian Visit to U.S. Computers"}
{"DOCID": "48", "TEXT": "Shift-Register Code for Indexing Applications: In this communication the use of a shift-register code with n = 10 is described for calling 64 wireless telemetering stations in a fixed cyclical order. A high degree of redundancy is used, permitting a single-error correcting code (\"minimum-distance-three\" code) with 64 10-bit code words to be employed as the station identification code.  Embedding this in the shift-register code with period 1023 permits the code to be employed without punctuation, each of the telemetering station receivers simply putting received ones and zeros into a shift register.  Each time the given code combination arises identifying the particular station (barring for tuitous error combinations of very low probability) it has been called. The communication describes the properties and application of the code in some detail and the finding of the particular example to be employed on URAL, the Soviet-built drum computer donated to the Indian Statistical Institute by the United Nations Technical Aid Administration (UNTAA)."}
{"DOCID": "49", "TEXT": "Scientific and Business Applications (Oracle Curve Plotter)"}
{"DOCID": "50", "TEXT": "Statistical Programs for the IBM 650-Part II"}
{"DOCID": "51", "TEXT": "On the Construction of Micro-Flowcharts"}
{"DOCID": "52", "TEXT": "An Efficient Method for Generating Uniformly Distributed Points on the Surface on an n-Dimensional Sphere (Corrigendum)"}
{"DOCID": "53", "TEXT": "Recommendations of the SHARE ALGOL Committee"}
{"DOCID": "54", "TEXT": "SALE, a Simple Algebraic Language for Engineers"}
{"DOCID": "55", "TEXT": "An Algebraic Translator"}
{"DOCID": "56", "TEXT": "Proposed Standard Flow Chart Symbols"}
{"DOCID": "57", "TEXT": "J.E.I.D.A. and Its Computer Center"}
{"DOCID": "58", "TEXT": "LEM-1, Small Size General Purpose Digital Computer Using Magnetic (Ferrite) Elements: The paper examines some of the questions of development and construction of a general purpose digital computer using contactless magnetic (ferrite) and capacitive \"DEZU\" (long duration capacitive memory) elements, developed at the Laboratory of Electrical Modeling VINITYI AN SSSR, under the supervision of Professor L.I. Gutenmacher."}
{"DOCID": "59", "TEXT": "Survey of Progress and Trend of Development and Use of Automatic Data Processing in Business and Management control Systems of the Federal Government, as of December 1957-III"}
{"DOCID": "60", "TEXT": "The Alpha Vector Transformation of a System of Linear Constraints"}
{"DOCID": "61", "TEXT": "IBM 709 Tape Matrix Compiler"}
{"DOCID": "62", "TEXT": "Multi-Dimensional Least-Squares Polynomial Curve Fitting"}
{"DOCID": "63", "TEXT": "Octal Diagrams of Binary Conception and Their Applicability to Computer Design Logic: This paper dates back the genesis of binary conception circa 5000 years ago, and octal diagrams about 4800 years ago, as derived by the Chinese ancients. It analyzes the applicability of binary trinities of the octal diagrams to modern electronic-digital-computer design logic."}
{"DOCID": "64", "TEXT": "Remarks on ALGOL and Symbol Manipulation"}
{"DOCID": "65", "TEXT": "ALGOL Sub-Committee Report - Extensions"}
{"DOCID": "66", "TEXT": "A Proposal for a Generalized Card Code for 256 Characters"}
{"DOCID": "67", "TEXT": "Central-European Computers"}
{"DOCID": "68", "TEXT": "The Role of the University in Computers, Data Processing and Related Fields: A study was made of university programs in the United States in the fields of computers, data processing, operations research, and other closely related fields.  University policies, organization, administration, faculties, students, researches, curricula, equipment, and financing were investigated. An integrated university program is recommended reflecting the conviction that many present activities related to computers will develop into disciplines and as such are the legitimate province of the university scholar.  Details on a recommended Graduate School of \"Computer Sciences\" are given."}
{"DOCID": "69", "TEXT": "Statistical Programs for the IBM 650-Part I: A collection is given of brief descriptions of statistical programs now in use in university computing centers which have IBM 650's."}
{"DOCID": "70", "TEXT": "Construction of a Set of Test Matrices: This paper develops the equations and properties of a set of test matrices which are useful in the determination of the accuracy of routines for finding the inverse, determinant and/or eigenvalues of a matrix."}
{"DOCID": "71", "TEXT": "Proposal for a Feasible Programming System: This paper proposes designing a programming facility (itself involving a digital computer and a program) which will assist the preparation of large-scale real-time programs.  This facility is to be capable of preparing programs for any of a variety of machines having characteristics similar to those of the facility's computer.  One of the basic assumptions is that there will be enough random-access storage available to avoid the necessity for segmenting a constructed program in any fashion other than a trivial one.  While this assumption is somewhat unrealistic, it is intended to provide an opportunity to concentrate on the other aspects of program construction. The programming system should stress the discovery in source program statements of as many errors as possible, before attempting to construct an object program.  Among the computer characteristics which are advocated are a program interrupt scheme, a large set of characters, and indirect addressing."}
{"DOCID": "72", "TEXT": "An Educational Program in Computing"}
{"DOCID": "73", "TEXT": "A Real Time Data Assimilator"}
{"DOCID": "74", "TEXT": "A High-Speed Sorting Procedure"}
{"DOCID": "75", "TEXT": "Parameter Estimation for Simple Nonlinear Models"}
{"DOCID": "76", "TEXT": "Binary Conversion, With Fixed Decimal Precision, Of a Decimal Fraction"}
{"DOCID": "77", "TEXT": "On GAT and the Construction of Translators"}
{"DOCID": "78", "TEXT": "Remarks on the Practical Solution of Characteristic Value Problems: This paper is concerned with the practical solution of characteristic value problem for an ordinary differential equation.  It is at once apparent that sequential computers, be they digital or analog, solve initial value problems, rather than boundary value problems, and some mathematical process must be found to compensate for the machine's inadequacy. (Compensating for machine imperfection is, of course, the normal activity of the numerical analyst.) A number of other papers have applied particular devices to particular problems.  The purpose of this note is to establish a mathematical framework or model for these practical procedures and thus assist in the use and extension of the ideas in other particular problems."}
{"DOCID": "79", "TEXT": "Programming for a Machine With an Extended Address Calculational Mechanism"}
{"DOCID": "80", "TEXT": "A Technique for Computing Critical Rotational Speeds of Flexible Shafts on an Automatic Computer"}
{"DOCID": "81", "TEXT": "NORC High-Speed Printer"}
{"DOCID": "82", "TEXT": "Handling Identifiers as Internal Symbols in Language Processors: Substitution of computer-oriented symbols for programmer-oriented symbols in language processors is examined and a feasible method for doing so is presented."}
{"DOCID": "83", "TEXT": "A Visit to Computation Centers in the Soviet Union"}
{"DOCID": "84", "TEXT": "Survey of Progress and Trend of Development and Use of Automatic Data Processing in Business and Management Control Systems of the Federal Government, as of December 1957-II (Part 2 see CA590406)"}
{"DOCID": "85", "TEXT": "Error Analysis in Floating Point Arithmetic"}
{"DOCID": "86", "TEXT": "Survey of Progress and Trend of Development and Use of Automatic Data Processing in Business and Management Control Systems of the Federal Government, as of December 1957"}
{"DOCID": "87", "TEXT": "A Note on a Method for Generating Points Uniformly on N-Dimensional Spheres"}
{"DOCID": "88", "TEXT": "An Efficient Method for Generating Uniformly Distributed Points on the Surface of an n-Dimensional Sphere"}
{"DOCID": "89", "TEXT": "A Routine to Find the Solution of Simultaneous Linear Equations with Polynomial Coefficients"}
{"DOCID": "90", "TEXT": "Binary Arithmetic for Discretely Variable Word Length in a Serial Computer"}
{"DOCID": "91", "TEXT": "A Mathematical Procedure for Machine Division"}
{"DOCID": "92", "TEXT": "A Checklist of Intelligence for Programming Systems: A remarkable variation exists in the degree of sophistication of various programming systems. A particular manifestation is the jungle of assorted devices for reproducing limited human decision procedures.  An attempt is made here to begin a systematic classification of the various devices for educating the computer to take over the decision-making functions of one or many human operators, both those that have been demonstrated feasible to date and those that are highly desirable for the future."}
{"DOCID": "93", "TEXT": "From Formulas to Computer Oriented Language: A technique is shown for enabling a computer to translate simple algebraic formulas into a three address computer code."}
{"DOCID": "94", "TEXT": "An Iterative Method for Fitting the Logistic Curve: An iterative method is given for finding a logistic curve of best least squares fit to a set of two-dimensional points."}
{"DOCID": "95", "TEXT": "Elimination of Special Functions from Differential Equations: A set of ordinary differential equations which contains mathematical functions requiring the use of subroutines for numerical solution by electronic computer, tabular data for numerical solution by hand calculation or function generators when analog methods are applied can sometimes be expanded to an equivalent set of equations which do not contain the functions.  This is practical if these functions satisfy sufficiently simple differential equations. Thus among those functions which can be eliminated by this procedure are the trigonometric, inverse trigonometric, exponential, and many other transcendental functions."}
{"DOCID": "96", "TEXT": "On Computing Radiation Integrals: The relative merit and cost of four ways of evaluating typical radiation integrals containing spherical Bessel functions are investigated.  These methods are desk machine evaluation of a finite series, integration of the appropriate differential equation by a Reeves Electronic Analog Computer and by a Litton 40 IBM 704 computer.  Results are generally applicable to equations separated from a Helmholtz or wave equation."}
{"DOCID": "97", "TEXT": "Signal Corps Research and Development on Automatic Programming of Digital Computers"}
{"DOCID": "98", "TEXT": "The Arithmetic Translator-Compiler of the IBM FORTRAN Automatic Coding System"}
{"DOCID": "99", "TEXT": "Possible Modifications to the International Algebraic Language"}
{"DOCID": "100", "TEXT": "Recursive Subscripting Compilers and List-Types Memories"}
{"DOCID": "101", "TEXT": "Nuclear Reactor Codes"}
{"DOCID": "102", "TEXT": "A Comparison of 650 Programming Methods"}
{"DOCID": "103", "TEXT": "COPE (Console Operator Proficiency Examination)*: Each year electronic computers become more sophisticated, and the programs they must process become more complex.  Because of this,dependence of those in computing on the skill and experience of operators is increasing.  At the same time, selection and training of qualified operators grows more difficult.  To meet the need for a quick, accurate, uniform operator test and training aid, the authors have developed COPE (Console Operator Proficiency Examination), outlined below.  While this examination is programmed specifically for the IBM 705 Model II with two Tape Record Coordinators, similar programs could be developed for other computers."}
{"DOCID": "104", "TEXT": "Digital Simulation of Discrete Flow Systems*: The discrete flow systems discussed are characterized by the movement of randomly arriving items along interacting channels.  Programing a digital computer to simulate such systems utilizes some techniques not common in other approaches to physical problems.  The principal portion of the paper is a discussion of two simulation studies that illustrate some of the programming problems involved. One is of an extensive package-handling plant, with the objective being optimization of parameters such as storage capacities and processing rates.  In the other, air traffic flow and control procedures are simulated to compare the effects of alternative control decisions."}
{"DOCID": "105", "TEXT": "Two Methods for Word Inversion on the IBM 709"}
{"DOCID": "106", "TEXT": "A Method for Overlapping and Erasure of Lists: An important property of the Newell-Shaw-Simon scheme for computer storage of lists is that data having multiple occurrences need not be stored at more than one place in the computer.  That is, lists may be \"overlapped.\"  Unfortunately, overlapping poses a problem for subsequent erasure.  Given a list that is no longer needed, it is desired to erase just those parts that do not overlap other lists. In LISP, McCarthy employs an elegant but inefficient solution to the problem.  The present paper describes a general method which enables efficient erasure.  The method employs interspersed reference counts to describe the extent of the overlapping."}
{"DOCID": "107", "TEXT": "Multiple Precision Arithmetic"}
{"DOCID": "108", "TEXT": "Programmed Error Correction in Project Mercury"}
{"DOCID": "109", "TEXT": "A Note on Approximating e^x"}
{"DOCID": "110", "TEXT": "Fibonaccian Searching"}
{"DOCID": "111", "TEXT": "On Programming the Numerical Solution of Polynomial Equations: Numerical techniques are presented for computing the roots of polynomial equations.  By applying the recommended scaling and inversion rules, the basic Bairstow and Newton-Raphson iterative techniques can be applied with great reliability.  Both a high degree of accuracy and rapid convergence are realized. Numerical examples are shown to illustrate the pitfalls and to show how these are circumvented by application of the recommended procedures."}
{"DOCID": "112", "TEXT": "Numerical Solution of the Polynomial Equation (Algorithm 30)"}
{"DOCID": "113", "TEXT": "Survey of Coded Character Representation"}
{"DOCID": "114", "TEXT": "Survey of Punched Card Codes"}
{"DOCID": "115", "TEXT": "Optimizers: Their Structure"}
{"DOCID": "116", "TEXT": "The Sumador Chino: On a recent motor trip through Mexico, the writer came across on adding device which was referred to as a sumador chino (Chinese adder).  A survey of the more available literature on the history of mathematics and on instruments of calculation has uncovered no reference to such a device.  The purpose of this communication is to enlist the help of other members in bringing to light whatever may be known concerning the evolution and present status of the sumador chino."}
{"DOCID": "117", "TEXT": "An Estimation of the Relative Efficiency of Two Internal Sorting Methods"}
{"DOCID": "118", "TEXT": "Character Scanning on the IBM 7070"}
{"DOCID": "119", "TEXT": "Note on Eigenvalue Computation"}
{"DOCID": "120", "TEXT": "A Simple Technique for Coding Differential Equations"}
{"DOCID": "121", "TEXT": "Over-all Computation Control and Labelling"}
{"DOCID": "122", "TEXT": "Least Squares Fitting of a Great Circle Through Points on a Sphere"}
{"DOCID": "123", "TEXT": "Compilation for Two Computers with NELIAC: NELIAC, a compiler based on ALGOL, was developed at the U.S. Navy Electronics Laboratory, San Diego,California, as a\"boot-strap\" compiler for the Remington Rand Univac COUNTESS computer. This compiler was used to generate a version of itself which, running as a COUNTESS program, generated machine code for the Control Data Corporation CDC-1604.  All three versions of NELIAC accepted essentially identical input language."}
{"DOCID": "124", "TEXT": "An Algorithm for the Assignment Problem: The assignment problem is formulated and briefly discussed.  An efficient algorithm for its solution is presented in ALGOL code.  An empirical relation between solution time and the size of the problem is given, based on extensive experiments carried out on a digital computer."}
{"DOCID": "125", "TEXT": "Polynomial Transformer (Algorithm 29)"}
{"DOCID": "126", "TEXT": "Least Squares Fit By Orthogonal polynomials (Algorithm 28)"}
{"DOCID": "127", "TEXT": "ASSIGNMENT (Algorithm 27)"}
{"DOCID": "128", "TEXT": "ROOTFINDER III (Algorithm 26)"}
{"DOCID": "129", "TEXT": "ROOTFINDER II (Algorithm 15)"}
{"DOCID": "130", "TEXT": "Real Zeros of an Arbitrary Function (Algorithm 25)"}
{"DOCID": "131", "TEXT": "Solution of Tri-Diagonal Linear Equations (Algorithm 24)"}
{"DOCID": "132", "TEXT": "Math Sort (Algorithm 23)"}
{"DOCID": "133", "TEXT": "Riccati-Bessel Functions of First And Second Kind (Algorithm 22)"}
{"DOCID": "134", "TEXT": "Bessel Function for a Set of Integer Orders(Algorithm 21)"}
{"DOCID": "135", "TEXT": "Digital Computers in Universities-IV"}
{"DOCID": "136", "TEXT": "A Note on the Calculation of Interest"}
{"DOCID": "137", "TEXT": "Evaluating Numbers Expressed as Strings of English Words"}
{"DOCID": "138", "TEXT": "Some Thoughts on Reconciling Various Character Set Proposals (Corrigenda)"}
{"DOCID": "139", "TEXT": "Binomial Coefficients (Algorithm 19)"}
{"DOCID": "140", "TEXT": "Crout with Pivoting (Algorithm 16)"}
{"DOCID": "141", "TEXT": "Some Thoughts on Parallel Processing"}
{"DOCID": "142", "TEXT": "Comments on a Technique for Counting Ones"}
{"DOCID": "143", "TEXT": "A List of Computer Systems Programs for the IBM 650, DATATRON 205, and UNIVAC SS-80"}
{"DOCID": "144", "TEXT": "Do It by the Numbers-Digital Shorthand: Present communications systems transmit single characters in groups of coded pulses between simple terminal equipments.  Since English words form only a sparse set of all possible alphabetic combinations, present methods are inefficient when computer systems are substituted for these terminals.  Using numeric representations of entire words or common phrases (rather than character-by-character representations) requires approximately one-third of present transmission time.  This saving is reflected in overall costs. Other benefits accrue in code and language translation schemes. Provision is made for transmission of purely numeric and/or binary streams, and for single character-transmission of non-dictionary words such as the names of people or places."}
{"DOCID": "145", "TEXT": "Automatic Graders for Programming Classes"}
{"DOCID": "146", "TEXT": "The Use of Computers in Engineering Classroom Instruction: On April 29-30, the Computer Committee of the College of Engineering, University of Michigan, which acts as a steering committee for The Ford Foundation Project on the Use of Computers in Engineering Education, held a special conference to discuss certain timely topics pertinent to the Ford Project. This report contains a condensed transcription of the key ideas offered by the conference attendees on selected topics."}
{"DOCID": "147", "TEXT": "Report on a Conference of University Computing Center Directors"}
{"DOCID": "148", "TEXT": "Digital Computers in Universities-III"}
{"DOCID": "149", "TEXT": "A Decision Rule for Improved Efficiency in Solving Linear Programming Problems with the Simplex Algorithm"}
{"DOCID": "150", "TEXT": "Rational Interpolation by Continued Fractions (Algorithm 18)"}
{"DOCID": "151", "TEXT": "TRDIAG (Algorithm 17)"}
{"DOCID": "152", "TEXT": "CROUT With Pivoting (Algorithm 16)"}
{"DOCID": "153", "TEXT": "Comments from a FORTRAN User"}
{"DOCID": "154", "TEXT": "Rapidly Convergent Expressions for Evaluating e^x"}
{"DOCID": "155", "TEXT": "Trie Memory"}
{"DOCID": "156", "TEXT": "An Introductory Problem in Symbol Manipulation for the Student"}
{"DOCID": "157", "TEXT": "Digital Computers in Universities -II"}
{"DOCID": "158", "TEXT": "ROOTFINDER II (Algorithm 15)"}
{"DOCID": "159", "TEXT": "ROOTFINDER (Algorithm 2)"}
{"DOCID": "160", "TEXT": "ROOTFINDER II (Algorithm 15)"}
{"DOCID": "161", "TEXT": "Abbreviating Words Systematically (Corrigendum)"}
{"DOCID": "162", "TEXT": "A Variant Technique for Counting Ones"}
{"DOCID": "163", "TEXT": "Counting Ones on the IBM 7090"}
{"DOCID": "164", "TEXT": "A Short Study of Notation Efficiency"}
{"DOCID": "165", "TEXT": "NELIAC-A Dialect of ALGOL"}
{"DOCID": "166", "TEXT": "Programming Compatibility in a Family of Closely Related Digital Computers"}
{"DOCID": "167", "TEXT": "Combining ALGOL Statement Analysis with Validity Checking"}
{"DOCID": "168", "TEXT": "Multiprogram Scheduling Parts 3 and 4 Scheduling Algorithm and External Constraints"}
{"DOCID": "169", "TEXT": "The Multilingual Terminology Project"}
{"DOCID": "170", "TEXT": "Some Thoughts on Reconciling Various Character Set Proposals"}
{"DOCID": "171", "TEXT": "Digital Computers in Universities (Part I)"}
{"DOCID": "172", "TEXT": "Complex Exponential Integral (Algorithm 13)"}
{"DOCID": "173", "TEXT": "ATLAS a new concept in large computer design"}
{"DOCID": "174", "TEXT": "Interval Estimation of the Time in One State to Total Time Ratio in a DoubleExponential Process"}
{"DOCID": "175", "TEXT": "The Solution of Simultaneous Ordinary Differential Equations Using a General Purpose Digital Computer"}
{"DOCID": "176", "TEXT": "Symbol Manipulation by Threaded Lists (Corrigendum)"}
{"DOCID": "177", "TEXT": "Solution of Polynomial Equation by Bairstow Hitchcock Method, A. A. Grau Communications ACM, February, 1960 (Algorithm)"}
{"DOCID": "178", "TEXT": "ROOTFINDER (Algorithm)"}
{"DOCID": "179", "TEXT": "Evaluation of the Legendre Polynomial Pn(X) by Recursion (Algorithm)"}
{"DOCID": "180", "TEXT": "Evaluation of the Laguerre Polynomial Ln(X) by Recursion (Algorithm)"}
{"DOCID": "181", "TEXT": "Evaluation of the Hermite Polynomial Hn(X) by Recursion (Algorithm)"}
{"DOCID": "182", "TEXT": "Evaluation of the Chebyshev Polynomial Tn(X) by Recursion (Algorithm)"}
{"DOCID": "183", "TEXT": "Conversion Between Floating Point Representations"}
{"DOCID": "184", "TEXT": "A Short Method for Measuring Error in a Least-Squares Power Series"}
{"DOCID": "185", "TEXT": "Multiprogram Scheduling Parts 1 and 2.  Introduction and Theory*: In order to exploit fully a fast computer which possesses simultaneous processing abilities, it should to a large extent schedule its own workload. The scheduling routine must be capable of extremely rapid execution if it is not to prove self-defeating. The construction of a schedule entails determining which programs are to be run concurrently and which sequentially with respect to each other.  A concise scheduling algorithm is described which tends to minimize the time for executing the entire pending workload (or any subset of it), subject to external constraints such as precedence, urgency, etc.  The algorithm is applicable to a wide class of machines."}
{"DOCID": "186", "TEXT": "An Algorithm Defining ALGOL Assignment Statements (Addendum)"}
{"DOCID": "187", "TEXT": "Compiling Connectives"}
{"DOCID": "188", "TEXT": "The Department of Computer Mathematics at Moscow State University"}
{"DOCID": "189", "TEXT": "The Future of Automatic Digital Computers"}
{"DOCID": "190", "TEXT": "Bendix G-20 System"}
{"DOCID": "191", "TEXT": "Abbreviating Words Systematically"}
{"DOCID": "192", "TEXT": "A Technique for Counting Ones in a Binary Computer"}
{"DOCID": "193", "TEXT": "A Start at Automatic Storage Assignment"}
{"DOCID": "194", "TEXT": "Divisionless Computation of Square Roots Through Continued Squaring"}
{"DOCID": "195", "TEXT": "What is a Code?"}
{"DOCID": "196", "TEXT": "Report on the Algorithmic Language ALGOL 60"}
{"DOCID": "197", "TEXT": "An Imaginary Number System"}
{"DOCID": "198", "TEXT": "A High-Speed Multiplication Process for Digital Computers"}
{"DOCID": "199", "TEXT": "Euclidian Algorithm (Algorithm 7)"}
{"DOCID": "200", "TEXT": "Bessel Function I, Asymptotic Expansion (Algorithm 6)"}
{"DOCID": "201", "TEXT": "Bessel Funtion I, Series Expansion (Algorithm 5)"}
{"DOCID": "202", "TEXT": "A Control System For Logical Block Diagnosis With Data Loading: This paper describes a section of an integrated diagnostic monitor system which facilitates the checking of sections of instructions or subroutines anywhere in the object program.  A new method of specifying all diagnostic operations in a format similar to a computer program makes the system convenient to use and relatively simple to understand.  The paper also describes a number of other novel diagnostic features which can be included in the system."}
{"DOCID": "203", "TEXT": "Decoding Combinations of the First n Integers Taken k at a Time"}
{"DOCID": "204", "TEXT": "Proving Theorems by Pattern Recognition I"}
{"DOCID": "205", "TEXT": "Macro Instruction Extensions of Compiler Languages: Macroinstruction compilers constructed from a small set of functions can be made extremely powerful.  In particular, conditional assembly, nested definitions, and parenthetical notation serve to make a compiler capable of accepting very general extensions to its ground language."}
{"DOCID": "206", "TEXT": "Symbol Manipulation in XTRAN"}
{"DOCID": "207", "TEXT": "Syntactic and Semantic Augments to ALGOL"}
{"DOCID": "208", "TEXT": "An Introduction to Information Processing Language V"}
{"DOCID": "209", "TEXT": "Symbol Manipulation by Threaded Lists"}
{"DOCID": "210", "TEXT": "Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I"}
{"DOCID": "211", "TEXT": "Share Standard Flow Chart Symbols"}
{"DOCID": "212", "TEXT": "Bisection Routine (Algorithm 4)"}
{"DOCID": "213", "TEXT": "Numerical Inversion of Laplace Transforms"}
{"DOCID": "214", "TEXT": "An Algorithm Defining ALGOL Assignment Statements"}
{"DOCID": "215", "TEXT": "The Execute Operations-A Fourth Mode of Instruction Sequencing"}
{"DOCID": "216", "TEXT": "A Note on the Use of the Abacus in Number Conversion"}
{"DOCID": "217", "TEXT": "Soviet Computer Technology-1959"}
{"DOCID": "218", "TEXT": "Computer Preparation of a Poetry Concordance"}
{"DOCID": "219", "TEXT": "Marriage-with Problems"}
{"DOCID": "220", "TEXT": "A New Method of Computation of Square Roots Without Using Division"}
{"DOCID": "221", "TEXT": "The Basic Side of Tape Labeling"}
{"DOCID": "222", "TEXT": "Coding Isomorphisms: The coding of external symbols into symbols internal to a compute can sometimes be carried out in such a way that relevant informational properties are preserved, but in a form much more easily dealt with.  A case in point is presented."}
{"DOCID": "223", "TEXT": "Selfcipher: Programming"}
{"DOCID": "224", "TEXT": "Sequential Formula Translation: The syntax of an algorithmic language such as ALGOL is conveniently described as a sequence of states indicated by an element called cellar.  Transitions are controlled by admissible state-symbol pairs which may be represented by a transition matrix. This description of syntax furnishes at the same time an extremely simple rule for translating into machine programs statements in the algorithmic language. Sequential treatment, however, is not feasible in the case of certain optimizing processes such as recursive address calculation."}
{"DOCID": "225", "TEXT": "A Techniquefor Handling Macro Instructions (Corrigendum)"}
{"DOCID": "226", "TEXT": "Solution of Polynomial Equation by Bairstow-Hitchcock Method (Algorithm 3)"}
{"DOCID": "227", "TEXT": "ROOTFINDER (Algorithm 2)"}
{"DOCID": "228", "TEXT": "QUADI (Algorithm 1)"}
{"DOCID": "229", "TEXT": "A Terminology Proposal"}
{"DOCID": "230", "TEXT": "A Proposal for Character Code Compatibility"}
{"DOCID": "231", "TEXT": "A Proposal for a Set of Publication Standards for Use by the ACM"}
{"DOCID": "232", "TEXT": "A High-Speed Sorting Procedure"}
{"DOCID": "233", "TEXT": "Abstracts-Additional Nuclear Reactor Codes"}
{"DOCID": "234", "TEXT": "A SAP-Like Assembly Program for the IBM 650"}
{"DOCID": "235", "TEXT": "Two Think Pieces"}
{"DOCID": "236", "TEXT": "Soviet Cybernetics and Computer: This article records observations on Soviet research and technology in cybernetics and computer science, made by the author during a visit to the Soviet Union as a delegate to the IFAC Congress on Automatic Control held in Moscow in the summer of 1960."}
{"DOCID": "237", "TEXT": "Computer Production of Peek-A-Boo Sheets"}
{"DOCID": "238", "TEXT": "Simulation and Analysis of Biochemical Systems"}
{"DOCID": "239", "TEXT": "Inefficiency of the Use of Boolean Functions for Information Retrieval Systems"}
{"DOCID": "240", "TEXT": "Processing Magnetic Tape Files with Variable Blocks"}
{"DOCID": "241", "TEXT": "Machine Calculation of Moments of a Probability Distribution: A method is presented for the calculation on a machine of the moments of a probability distribution, necessitating little more than n additions and n references to memory for each moment, instead of the minimum of n multiplication, 2n additions, and 2n references to memory required by the most straightforward method (where n is the number of entries in the probability distribution).  The method is directly applicable when a tabulated distribution exists, as when it has been computed by repeated convolution; but in this case it conserves both time and accuracy."}
{"DOCID": "242", "TEXT": "Notes on Geometric Weighted Check Digit Verification: This note describes a method for utilizing geometric weight modulus 11 checking digits on a computer which does not have either multiplication or division.  In addition some attempt has been made to show some limitations of this system."}
{"DOCID": "243", "TEXT": "N-Dimensional Codes for Detecting and Correcting Multiple Errors: The paper introduces a new family of codes for detecting and correcting multiple errors in a binary-coded message.  The message itself is arranged (conceptually) into a multidimensional rectangular array.  The processes of encoding and error detection are based upon parity evaluations along prescribed dimensions of the array.  Effectiveness of the codes is increased by introducing a \"system check bit\", which is essentially a parity check on the other parity bits.  Only three-dimensional codes are discussed in this paper with parity evaluations along the horizontal, the vertical, and one main diagonal.  However, the family of codes is not restricted to three dimensions, as evidenced by the discussion by Minnick and Ashenhurst on a similar multidimensional single-bit selection plan used for another purpose [6]. A four-dimensional code, correcting three and detecting four errors, has been developed; the extension to higher-dimensional codes with greater correction power is straightforward."}
{"DOCID": "244", "TEXT": "Incomplete Elliptic Integrals (Algorithm 73)"}
{"DOCID": "245", "TEXT": "A Set of Associate Legendre Polynomials of the Second Kind (Algorithm 62)"}
{"DOCID": "246", "TEXT": "Least-Squares Fit by Orthogonal Polynomials (Algorithm 28)"}
{"DOCID": "247", "TEXT": "Incomplete Elliptic Integrals (Algorithm 73)"}
{"DOCID": "248", "TEXT": "What is Proprietary In Mathematical Programming?-Impressions of a Panel Discussion: A panel discussion on \"What is Proprietary in Mathematical Programming?\" was sponsored by the Special Interest Committee on Mathematical Programming of the ACM during a Hall of Discussion/on September 7th at the 16th National ACM meeting in Los Angeles.  This note consists solely of the impressions garnered by the moderator of the panel and does not necessarily represent the position of any of the panelists or other participants in the discussion."}
{"DOCID": "249", "TEXT": "Specification Languages for Mechanical Languages and Their Processors*-A Baker's Dozen"}
{"DOCID": "250", "TEXT": "An Engineering Application of Logic-Structure Tables"}
{"DOCID": "251", "TEXT": "Ballistic Cam Design: This paper presents a digital computer program for the rapid calculation of manufacturing data essential to the design of preproduction cams which are utilized in ballistic computers of tank fire control systems.  The cam profile generated introduces the superelevation angle required by tank main armament for a particular type ammunition."}
{"DOCID": "252", "TEXT": "Programming a Duplex Computer System: This paper describes a method of duplex-computer programming that has been used with two computers in a military defense system.  The method combines special programs with a basic data processing program package.  The duplex operation gives the system greater reliability.  After achieving the required level of integration, both computers do similar processing on the same inputs and continually cross-check the intermediate and final results."}
{"DOCID": "253", "TEXT": "On a Program for Ray-Chaudhuri's Algorithm for a Minimum Cover of an Abstract Complex"}
{"DOCID": "254", "TEXT": "SMALGOL-61: Prior to and during the 1961 Western Joint Computer Conference, several people in the Joint Users Groups had expressed interest in defining a \"smalgol\" language.  This is to be an ALGOL language for use with compilers on relatively small size computers. A preliminary report resulted.  At the ACM National Conference four months later, after considering several counter proposals, a final version was agreed upon by a subcommittee.  The recommendations of the Subcommittee for a standard subset of ALGOL 60 for use on small computers is presented here."}
{"DOCID": "255", "TEXT": "Augmentation (Algorithm 68)"}
{"DOCID": "256", "TEXT": "A Set of Test Matrices (Algorithm 52)"}
{"DOCID": "257", "TEXT": "Invert (Algorithm 42)"}
{"DOCID": "258", "TEXT": "Composition Generator (Algorithm 72)"}
{"DOCID": "259", "TEXT": "Permutation (Algorithm 71)"}
{"DOCID": "260", "TEXT": "Interpolation By Aitken (Algorithm 70)"}
{"DOCID": "261", "TEXT": "Tape Splitting"}
{"DOCID": "262", "TEXT": "MAP"}
{"DOCID": "263", "TEXT": "Library Loading with Alternate Routine Selection"}
{"DOCID": "264", "TEXT": "A Generalized Polyphase Merge Algorithm"}
{"DOCID": "265", "TEXT": "Low Level Language Subroutines for Use Within Fortran: This paper describes some subroutines, coded in symbolic languages and for use within Fortran coded programs, to deal with \"special arithmetic\" (e.g. multi-precision arithmetic), symbol manipulation, bit manipulation and expanded character set input-output, and visual display."}
{"DOCID": "266", "TEXT": "Fitting Spheres by the Method of Least Squares"}
{"DOCID": "267", "TEXT": "Some Proposals for Improving the Efficiency of ALGOL 60"}
{"DOCID": "268", "TEXT": "Stochastic Evaluation of a Static Storage Allocation"}
{"DOCID": "269", "TEXT": "Core Allocation Based on Probability"}
{"DOCID": "270", "TEXT": "Techniques for Storage Allocation Algorithms"}
{"DOCID": "271", "TEXT": "A Semi-Automatic Storage Allocation System at Loading Time"}
{"DOCID": "272", "TEXT": "A Storage Allocation Scheme for ALGOL 60: A storage allocation scheme for a machine with a 2048 instruction core store and a magnetic drum is described.  The use of the drum for storing program blocks and/or data must be directed by the programmer through auxiliary information in the ALGOL program.  The administrative routines controlling the storage at run time are described in full.  A detailed example is given."}
{"DOCID": "273", "TEXT": "Experience in Automatic Storage Allocation"}
{"DOCID": "274", "TEXT": "Dynamic Storage Allocation in the Atlas Computer, Including an Automatic Use of a Backing Store"}
{"DOCID": "275", "TEXT": "Dynamic Storage Allocation for an Information Retrieval System"}
{"DOCID": "276", "TEXT": "Program Organization and Record Keeping for Dynamic Storage Allocation: The material presented in this paper is part of the design plan of the core allocation portion of the ASCII-MATIC Programming System.  Project ASCII-MATIC is concerned with the application of computer techniques to the activities of certain headquarters military intelligence operations of the U.S. Army."}
{"DOCID": "277", "TEXT": "Problems of Storage Allocation in a Multiprocessor Multiprogrammed System"}
{"DOCID": "278", "TEXT": "A General Formulation of storage Allocation: Formalization of a general computer storage allocation process is attempted.  With a given computer M is associated a fictitious computer M' essentially identical to M except in respect to possession of unbounded primary storage.  Mappings of the total storage set (internal and external) of M into the direct address set of M' are introduced.  A program sequence P for M' is termed M-admissible (relative to a specific execution time period) if there is a mapping underwhich P and its effective data referents are all located in the direct address set of M.  Storage allocation is considered as a process of establishing for an arbitrary M' program  a sequence of mappings, a decoupling of the program into M-admissible subprograms and a linking set of interludes.  An existence proof in terms of a completely interpretive M program as indicated.  Some special cases are discussed.  Various restrictions on generality of M' programs are considered under which more practical realization of allocation processes becomes tractable."}
{"DOCID": "279", "TEXT": "The Case for Dynamic storage Allocation"}
{"DOCID": "280", "TEXT": "A Preplanned Approach to a Storage Allocating Compiler"}
{"DOCID": "281", "TEXT": "Putting a Hex on e^x: Recent notes on approximate natural antilogy have not considered indirect formulations for describing e^x.  In this note we produce a particular family of very fast, high precision and eminently practical exponential evaluation formulas derived from one such formulation."}
{"DOCID": "282", "TEXT": "Optimum Tape-Writing Procedures: Consider a magnetic tape system with a read check after writing.  Where an error occurs in writing a record, a programmed error routine may either bypass some or all of the area on tape or try to rewrite the record on the same area.  This paper evaluates these two procedures on the basis of expected loss of computer time and develops a decision rule for selecting the optimum procedure.  The rule depends critically on the number of times the tape being written will be used in the future.  In the case where the optimum procedure is to bypass an area, a second decision-the size of the area to be bypassed-is necessary.  A formula is developed to determine the optimum area to be bypassed for each procedure."}
{"DOCID": "283", "TEXT": "Inversion of a Complex Matrix"}
{"DOCID": "284", "TEXT": "Manipulation of Algebraic Expressions: An algorithm for algebraically manipulating expressions of the form SUM{CiPi, i=1,...,n}; has been developed in conjunction with the development of programs for systems analysis problems.  This algorithm enablesus to derive over-all system transfer functions from algebraically described block diagrams of any linear continuous multi-loop feedback system.  The machine representation of the derived expression, is, by virtue of the algorithm, in a form which simplifies the task of compiling.  The algorithm was developed for a particular purpose in connection with system analysis studies.  However, its application as a mathematical device extends far beyond the confines of the original problem."}
{"DOCID": "285", "TEXT": "Solution of Tridiagonal Matrices"}
{"DOCID": "286", "TEXT": "An Iterative Method for Inversion of Power Series"}
{"DOCID": "287", "TEXT": "The Generalized Important Event Technique"}
{"DOCID": "288", "TEXT": "A Syntactical Chart of ALGOL 60"}
{"DOCID": "289", "TEXT": "Critical Path Scheduling (Algorithm 40)"}
{"DOCID": "290", "TEXT": "Chain Tracing (Algorithm 69)"}
{"DOCID": "291", "TEXT": "Use of MOBOL in PreparingRetrieval Programs"}
{"DOCID": "292", "TEXT": "An Information Retrieval Language for Legal Studies"}
{"DOCID": "293", "TEXT": "The Applied Mathematics Laboratory of the David W. Taylor Model Basin"}
{"DOCID": "294", "TEXT": "An Imaginary Number System"}
{"DOCID": "295", "TEXT": "Rational Approximations for the Error Function and for Similar Functions"}
{"DOCID": "296", "TEXT": "A Note on Multiple Precision Arithmetic"}
{"DOCID": "297", "TEXT": "A Note on Fitting Great Circles by Least Squares"}
{"DOCID": "298", "TEXT": "A 48-Bit Pseudo-Random Number Generator: A new 48-bit pseudo-random number generator, suitable for several computers, was tested statistically for randomness to determine its adequacy for use in Monte Carlo programs.  Frequency tests, distributions of certain low-order moments, runs up and down, and runs above and below the mean were applied to one-half million generated numbers lying within the interval (0,1) and to three sets of integers obtained from specified bits within the generated numbers.  These tests substantiated the randomness of all numbers except for the set of integers coming from the least significant bits."}
{"DOCID": "299", "TEXT": "A Generalized Polyphase Merge Algorithm"}
{"DOCID": "300", "TEXT": "COBOL: A Sample Problem: A simplified Merchandise Control problem has been chosen for presenting COBOL to users and potential users of computing systems.  A mythical department store, \"E. Language Bros., Inc.\", is programming in the COBOL language one of the many runs on its computer."}
{"DOCID": "301", "TEXT": "A Set of Test Matrices (Algorithm 52)"}
{"DOCID": "302", "TEXT": "Augmentation (Algorithm 68)"}
{"DOCID": "303", "TEXT": "Some Basic Terminology Connected With Mechanical Languages and Their Processors: The suggestions in this paper are part of the terminology used in work for the University of Pennsylvania's Office of computer Research and Education. The work is jointly supported by the National Science Foundation and the Air Force Office of Scientific Research."}
{"DOCID": "304", "TEXT": "Nth Roots of a Complex Number (Algorithm 53)"}
{"DOCID": "305", "TEXT": "CRAM (Algorithm 67)"}
{"DOCID": "306", "TEXT": "INVRS (Algorithm 66)"}
{"DOCID": "307", "TEXT": "FIND (Algorithm 65)"}
{"DOCID": "308", "TEXT": "QUICKSORT (Algorithm 64)"}
{"DOCID": "309", "TEXT": "PARTITION (Algorithm 63)"}
{"DOCID": "310", "TEXT": "A Set of Associate Legendre Polynomials of the Second Kind (Algorithm 62)"}
{"DOCID": "311", "TEXT": "Procedures for Range Arithmetic (Algorithm 61)"}
{"DOCID": "312", "TEXT": "A Further Note on Approximating e^x"}
{"DOCID": "313", "TEXT": "An Iterative Method for Inversion of Power Series"}
{"DOCID": "314", "TEXT": "A Divisionless Method of Integer Conversion"}
{"DOCID": "315", "TEXT": "Solution of Tridiagonal Matrices"}
{"DOCID": "316", "TEXT": "An Algorithm for Equivalence Declarations"}
{"DOCID": "317", "TEXT": "On The Approximation of Curves by Line Segments Using Dynamic Programming"}
{"DOCID": "318", "TEXT": "Combat Vehicle Firing Stability (Active Suspension)"}
{"DOCID": "319", "TEXT": "On a Class of Iteration Formulas and Some Historical Notes: The class of iteration formulas obtainable by rational approximations of \"Euler's formula\" is derived with the corresponding error estimates. Some historical notes on iterative procedures are followed by a derivation of Euler's formula with the associated error estimate in a new notation which simplifies the error estimate and suggests generalizations. The final section considers the Pade approximants to the \"Euler polynomial\" and shows how a number of known formulas may be derived from this unified approach. There is a short discussion of the \"best\" formula."}
{"DOCID": "320", "TEXT": "Logic-Structure Tables: Logic tables are an excellent way of developing and expressing the logic required in procedures, operations, systems and circuits.  A set of rules for writing and using logic tables is explained by means of some simple examples.  Then the logic structure of a vending machine is given in which two logic tables are used.  Logic tables are two-dimensional in nature, enabling us to fully express and consider both the sequential and parallel aspects of logic.  They can be compiled directly into a computer program and so eliminate the need for flow charting and hand coding."}
{"DOCID": "321", "TEXT": "ALGOL 60 Confidential: The ALGOL 60 Report,* when first encountered, seems to describe a very complex language which will be difficult to learn.  The \"metalinguistic formulae\" admirably serve the purpose of precisely specifying a language, but they are certainly not very readable for a beginner.  However, experience has shown that once the report is explained it is in fact easy to learn ALGOL and to write algorithms in it.  The language is so general and powerful it can handle an enormous class of problems.  It is not hard to learn those parts of ALGOL present in other compiler languages: how to write assignment and go to and for statements, etc.  Indeed, a lot of the unnecessary restrictions imposed by other compiling languages have finally been lifted.  But ALGOL also allows many unobvious things to be written, as we will see later, and herein lies a problem: ALGOL seems to have become too general. So many restrictions have been lifted that a lot of technical details crop up which are hard to learn and to use correctly.  In this paper some of the more obscure features of the language are considered and their usefulness is discussed.  Remarks are based on the authors' interpretations of the ALGOL 60 Report."}
{"DOCID": "322", "TEXT": "Operational Compatibility of Systems-CONVENTIONS: The General Standards Committee of the SHARE organization has devoted considerable effort to the problem of operating a computer efficiently in view of the growing number of programming systems available.  Each of these programming systems has been coded to utilize a fixed set of hardware components without recognizing the fact that others may be occupying a storage medium required by the first.  These incompatibilities are currently resolved by manually setting up the computer for each system as required. The following set of conventions is being considered to minimize computer set-up time.  They are of sufficiently broad interest that we feel other computer users should be aware of them. -George F. Ryckman, Chairman"}
{"DOCID": "323", "TEXT": "The State of Digital Computer Technology in Europe"}
{"DOCID": "324", "TEXT": "Romberg Integration (Algorithm 60)"}
{"DOCID": "325", "TEXT": "Numerical Solution of the Polynomial Equation (Algorithm 30)"}
{"DOCID": "326", "TEXT": "MATHSORT (Algorithm 23)"}
{"DOCID": "327", "TEXT": "Zeros of a Real Polynomial by Resultant Procedure (Algorithm 59)"}
{"DOCID": "328", "TEXT": "Matrix Inversion (ALgorithm 58)"}
{"DOCID": "329", "TEXT": "Automatic Abstracting and Indexing Survey and Recommendations: In preparation for the widespread use of automatic scanners which will read documents and transmit their contents to other machines for analysis, this report presents a new concept in automatic analysis: the relative-frequency approach to measuring  the significance of words, word groups, and sentences. The relative-frequency approach is discussed in detail, as is its application to problems of automatic indexing and automatic abstracting.  Included in the report is a summary of automatic analysis studies published as of the date of writing.  Conclusions are that point toward more sophisticated mathematical and linguistic techniques for the solution of problems of automatic analysis."}
{"DOCID": "330", "TEXT": "A Method for Evaluating the Area of the Normal Function"}
{"DOCID": "331", "TEXT": "Successive Approximations and Computer Storage Problems in Ordinary Differential Equations"}
{"DOCID": "332", "TEXT": "An Indirect Chaining Method for Addressing on Secondary Keys: Methods for entering random access files on the basis of one key are briefly surveyed.  The widely used chaining method, based on a pseudo-random key transformation, is reviewed in more detail. An efficient generalization of the chaining method which permits recovery on additional keys is then presented."}
{"DOCID": "333", "TEXT": "Design of an Improved* Transmission/Data Processing Code"}
{"DOCID": "334", "TEXT": "Division and Square Root in the Quater-Imaginary Number System"}
{"DOCID": "335", "TEXT": "Some Numerical Experiments Using Newton's Method for Nonlinear Parabolic and EllipticBoundary-Value Problems: Using a generalization of Newton's method, a nonlinear parabolic equation of the form U(t)-U(xx)=g(U) and a nonlinear elliptic equation U(xx)+U(yy)=exp(U) are solved numerically Comparison of these results with results obtained using the Picard iteration procedure show that in many cases the quisi linearization method offers substantial advantages in both time and accuracy."}
{"DOCID": "336", "TEXT": "A Practical Technique for the Determination of the Optimum Relaxation Factor of the Successive Over-Relaxation Method"}
{"DOCID": "337", "TEXT": "Further Survey of Punched Card Codes"}
{"DOCID": "338", "TEXT": "GROUT II (Algorithm 43)"}
{"DOCID": "339", "TEXT": "Real Exponential Integral (Algorithm 20)"}
{"DOCID": "340", "TEXT": "Legendre Polynomial (Algorithm 13)"}
{"DOCID": "341", "TEXT": "Chebyschev Polynomial (Algorithm 10)"}
{"DOCID": "342", "TEXT": "Solution of Polynomial Equation by Barstow-Hitchcock (Algorithm 3)"}
{"DOCID": "343", "TEXT": "On Frequently Occurring Errors in ALGOL 60 Programs (Algorithm 25)"}
{"DOCID": "344", "TEXT": "Ber or Bei Function (Algorithm 57)"}
{"DOCID": "345", "TEXT": "Complete Elliptic Integral of the Second Kind (Algorithm 56)"}
{"DOCID": "346", "TEXT": "Complete Elliptic Integral of the First Kind (Algorithm 55)"}
{"DOCID": "347", "TEXT": "Gamma Function for Range 1 to 2 (Algorithm 54)"}
{"DOCID": "348", "TEXT": "Nth Roots of a Complex Number (Algorithm 53)"}
{"DOCID": "349", "TEXT": "A Set of Test Matrices"}
{"DOCID": "350", "TEXT": "Adjust Inverse of a Matrix When an Element is Perturbed (Algorithm 51)"}
{"DOCID": "351", "TEXT": "Inverse of a Finite Segment of the Hilbert Matrix (Algorithm 50)"}
{"DOCID": "352", "TEXT": "Spherical Neumant Function (Algorithm 49)"}
{"DOCID": "353", "TEXT": "Logarithm of A Complex Number (Algorithm 48)"}
{"DOCID": "354", "TEXT": "Associated Legendre Functions of the First Kind for Real or Imaginary Arguments (Algorithm 47)"}
{"DOCID": "355", "TEXT": "Exponential of a Complex Number (Algorithm 46)"}
{"DOCID": "356", "TEXT": "INTEREST (Algorithm 45)"}
{"DOCID": "357", "TEXT": "Bessel Functions Computed Recursively (Algorithm 44)"}
{"DOCID": "358", "TEXT": "Crout with Pivoting II (Algorithm 43)"}
{"DOCID": "359", "TEXT": "INVERT (Algorithm 42)"}
{"DOCID": "360", "TEXT": "Evaluation of Determinant (Algorithm 41)"}
{"DOCID": "361", "TEXT": "Programmed Error Correction on a Decimal Computer"}
{"DOCID": "362", "TEXT": "Table Look-At Techniques"}
{"DOCID": "363", "TEXT": "On Approximating Transcendental Numbers by Continued Fractions"}
{"DOCID": "364", "TEXT": "On the Compilation of Subscripted Variables"}
{"DOCID": "365", "TEXT": "Bessel Functions of Integral Order and Complex Argument"}
{"DOCID": "366", "TEXT": "Eigenvalues of a Symmetric 3 x 3 Matrix"}
{"DOCID": "367", "TEXT": "Topological Ordering of a List of Randomly-Numbered Elements of a Network: A network of directed line segments free of circular elements is assumed.  The lines are identified by their terminal nodes and the nodes are assumed to be numbered by a non-topological system.  Given a list of these lines in numeric order, a simple technique can be used to create at high speed a list in topological order."}
{"DOCID": "368", "TEXT": "Real Zeros of an Arbitrary Function (Algorithm 25)"}
{"DOCID": "369", "TEXT": "Crout with Pivoting (Algorithm 16)"}
{"DOCID": "370", "TEXT": "Bisection Routine (Algorithm 4)"}
{"DOCID": "371", "TEXT": "Remarks on Algorithms 2 and 3, Algorithm 15 and Algorithms 25 and 26"}
{"DOCID": "372", "TEXT": "Critical Path Scheduling (Algorithm 40)"}
{"DOCID": "373", "TEXT": "Correlation Coefficients with Matrix Multiplication (Algorithm 39)"}
{"DOCID": "374", "TEXT": "Telescope2 (Algorithm 38)"}
{"DOCID": "375", "TEXT": "Telescope1 (Algorithm 37)"}
{"DOCID": "376", "TEXT": "Tchebycheff (Algorithm 36)"}
{"DOCID": "377", "TEXT": "SIEVE (Algorithm 35)"}
{"DOCID": "378", "TEXT": "A Generalized Technique for Symbol Manipulation and Numerical Calculation"}
{"DOCID": "379", "TEXT": "Bitwise Operations"}
{"DOCID": "380", "TEXT": "Comparison of Iterative Methods for the Calculation of nth Roots: Three iterative methods for calculation of nth roots (including one proposed by the author) are compared in two ways: (1) Theoretical convergence estimates are given.  (2) A new macrocompiler which estimates machine running time is used to compare the running time of the three methods for a variety of input data."}
{"DOCID": "381", "TEXT": "An Alternate Form of the \"UNCOL Diagram\""}
{"DOCID": "382", "TEXT": "Statistical Programs at the University of North Carolina"}
{"DOCID": "383", "TEXT": "On Finding Minimum Routes in a Network With Turn Penalties"}
{"DOCID": "384", "TEXT": "Gamma Function (Algorithm 34)"}
{"DOCID": "385", "TEXT": "FACTORIAL (Algorithm 33)"}
{"DOCID": "386", "TEXT": "MULTINT (Algorithm 32)"}
{"DOCID": "387", "TEXT": "Gamma Function (Algorithm 31)"}
{"DOCID": "388", "TEXT": "Solution of Polynomial Equations by Bairstow Hitchcock Method (Algorithm 3)"}
{"DOCID": "389", "TEXT": "Real Exponential Integral (Algorithm 20)"}
{"DOCID": "390", "TEXT": "Complex Exponential Integral (Algorithm 13)"}
{"DOCID": "391", "TEXT": "The BKS System for the Philco-2000"}
{"DOCID": "392", "TEXT": "Comment on A Paper on Parallel Processing"}
{"DOCID": "393", "TEXT": "Two Subroutines for Symbol Manipulation with an Algebraic Compiler"}
{"DOCID": "394", "TEXT": "Multiple Programming Data Processing"}
{"DOCID": "395", "TEXT": "Multiple-Precision Division"}
{"DOCID": "396", "TEXT": "Automation of Program  Debugging: Automatic Debugging can substantially reduce lead-time between the coding and the effective use of a complex program. It also enforces analysis of debugging criteria, resulting in verifiably accurate programs. The programmer specifies the program to be debugged, memory areas, set of input data, maximum repetition of loops, and checkpoint information for each set of data. The executive debugging program the runs the program to be debugged, performing checking functions and creating a trace record of its own later analysis and location of errors. Applications are quite flexible, and the system can be used alone or in conjunction with other debugging techniques."}
{"DOCID": "397", "TEXT": "A Card Format for Reference Files in Information Processing: This paper proposes a card format suitable for a variety of reference files in information processing.  An 80-column IBM card is divided into two fields-reference material field (columns 1-67) and identification field (columns 68-80).  The format for the reference material is flexible, while the format for the identification is rigid.  The reference material includes basically an index, title, source, class, summary and cross reference for each entry. The identification includes basically codes for a matrix of descriptors, an entry number, and the kind, major interest, and source of the reference.  The identification also provides a choice to identify material for personal as well as general files.  Since this card format is sufficient to identify the material normally associated with reference files for books, articles, programming terms, hardware terms, equipment, machine systems, abbreviations, etc., it is suitable as a standard for card reference files in information processing."}
{"DOCID": "398", "TEXT": "The SLANG System"}
{"DOCID": "399", "TEXT": "Compiling Techniques for Boolean Expressions and Conditional Statements in ALGOL 60"}
{"DOCID": "400", "TEXT": "Comments on the Implementation of Recursive Procedures and Blocks in ALGOL 60"}
{"DOCID": "401", "TEXT": "Allocation of Storage for Arrays in ALGOL 60"}
{"DOCID": "402", "TEXT": "Dynamic Declarations"}
{"DOCID": "403", "TEXT": "Thunks -- A Way of Compiling Procedure Statements with Some Comments on Procedure Declarations"}
{"DOCID": "404", "TEXT": "A Syntax Directed Compiler for ALGOL 60"}
{"DOCID": "405", "TEXT": "An Algorithm for Coding Efficient Arithmetic Operations: Most existing formula translation schemes yield inefficient coding.  A method is described which reduces the number of store and fetch operations, evaluates constant subexpressions during compilation, and recognizes many equivalent subexpressions."}
{"DOCID": "406", "TEXT": "The Use of Threaded Lists in Constructing a Combined ALGOL and Machine-Like Assembly Processor"}
{"DOCID": "407", "TEXT": "MADCAP: A Scientific Compiler for a Displayed Formula Textbook Language"}
{"DOCID": "408", "TEXT": "The Internal Organization of the MAD Translator"}
{"DOCID": "409", "TEXT": "CL-1, An Environment for a Compiler: A flexible, large-scale programming system to facilitate the solution of information processing problems and to provide intercommunication between programs and/or programmers has been developed and realized on the IBM 709/7090 computer.  The system is based on a master file concept and has provisions for accepting, storing, and retrieving both descriptions and instances of large and complex data sets, as well as algorithms defined on these data sets.  Both data and algorithms may be expressed in a family of command and descriptive languages.  The concept of distinct data descriptions and the content and use of such descriptions are discussed in some detail."}
{"DOCID": "410", "TEXT": "The CLIP Translator"}
{"DOCID": "411", "TEXT": "Use of Magnetic Tape for Data Storage in the ORACLE-ALGOL Translator"}
{"DOCID": "412", "TEXT": "Recursive Processes and ALGOL Translation"}
{"DOCID": "413", "TEXT": "A Basic Compiler for Arithmetic Expressions"}
{"DOCID": "414", "TEXT": "IBM 1440 Data Processing System Features Five New Units: The IBM 1440 data processing system, announced recently by the International Business Machines Corporation, not only features the 1311 disk storage drive with interchangeable disk packs but four other newly developed units."}
{"DOCID": "415", "TEXT": "The Use of Digital Computers in Western Germany"}
{"DOCID": "416", "TEXT": "Multiple Shooting Method for Two-Point Boundary Value Problems"}
{"DOCID": "417", "TEXT": "Legal Implications of Computer Use: This paper points out a variety of ways computer systems used in business and industry can be involved in legal entanglements and suggests that computer specialists have a responsibility to call for assistance in forestalling or minimizing those entanglements during the planning stage.  Techniques are suggested for making legal clearance effective with the least burden on the new technology and for achieving a favorable legal climate for it generally. Computer specialists also are alerted to potential opportunities to interpret to lawyers the technical aspects of computer systems involved in legal situations."}
{"DOCID": "418", "TEXT": "RANDOM (Algorithm 133)"}
{"DOCID": "419", "TEXT": "Magic Square (Algorithm 118)"}
{"DOCID": "420", "TEXT": "PERM (Algorithm 115)"}
{"DOCID": "421", "TEXT": "Position of Point Relative to Polygon (Algorithm 112)"}
{"DOCID": "422", "TEXT": "COMBINATION (Algorithm 94)"}
{"DOCID": "423", "TEXT": "Matrix Inversion (Algorithm 58)"}
{"DOCID": "424", "TEXT": "Gamma Function (Algorithm 31)"}
{"DOCID": "425", "TEXT": "Complete Elliptic Integral (Algorithm 149)"}
{"DOCID": "426", "TEXT": "Term of Magic Square (Algorithm 148)"}
{"DOCID": "427", "TEXT": "PSIF (Algorithm 147)"}
{"DOCID": "428", "TEXT": "Multiple Integration (Algorithm 146)"}
{"DOCID": "429", "TEXT": "Adaptive Nimerical Integration by Simpson's Rule (Algorithm 145)"}
{"DOCID": "430", "TEXT": "TREESORT2 (Algorithm 144)"}
{"DOCID": "431", "TEXT": "TREESORT1 (Algorithm 143)"}
{"DOCID": "432", "TEXT": "Triangular Regression (Algorithm 142)"}
{"DOCID": "433", "TEXT": "Fixed-World-Length Arrays in Variable-Word-Length Computers"}
{"DOCID": "434", "TEXT": "Character Manipulation in 1620 Fortran II"}
{"DOCID": "435", "TEXT": "A Decision Matrix as the Basis for a Simple Data Input Routine: Currently a great deal of time and effort is being spent on the development of bigger and better compiler languages, multiprogram executive systems, etc.  Since the implementation of  of new methods and procedures is not instantaneous, but rather occurs by an evolutionary process, we should be concerned also with the problem of maintaining, improving and incorporating new ideas into existing systems.  It is with this somewhat neglected area that the author is interested.  A method employing a decision matrix is presented for the handling of a standard systems programming problem,that of providing a data input routine."}
{"DOCID": "436", "TEXT": "Evaluation of Polynomials by Computer"}
{"DOCID": "437", "TEXT": "Compiling Matrix Operations"}
{"DOCID": "438", "TEXT": "Mechanical Pragmatics: A Time-Motion Study of a Miniature Mechanical Linguistic System"}
{"DOCID": "439", "TEXT": "On-Line Digital Computer for Measurement of a Neurological Control System"}
{"DOCID": "440", "TEXT": "Record Linkage: Special difficulties are encountered in devising reliable systems for searching and updating any large files of documents that must be identified primarily on the basis of names and other personal particulars.  The underlying problem is that of making nearly maximum use of items of identifying information that are individually unreliable but that may collectively be of considerable discriminating power. Rules that can be applied generally to name retrieval systems have been developed in a methodological study of the linkage of vital and health records into family groupings for demographic research purposes. These rules are described, and the ways in which information utilization for matching may be optimized are discussed."}
{"DOCID": "441", "TEXT": "Topological Sorting of Large Networks: Topological Sorting is a procedure required for many problems involving analysis of networks. An example of one such problem is PERT.  The present paper presents a very general method for obtaining topological order.  It permits treatment of larger networks than can be handled on present procedures and achieves this with greater efficiency.  Although the procedure can be adapted to any machine, it is discussed in terms of the 7090.  A PERT network of 30,000 activities can be ordered in less than one hour of machine time.  The method was developed as a byproduct of procedures needed by Westinghouse, Baltimore.  It has not been programmed and at present there are no plans to implement it.  In regard to the techniques described, Westinghouse's present and anticipated needs are completely served by the Lockheed program, which is in current use."}
{"DOCID": "442", "TEXT": "Crout with Equilibration and Iteration (Algorithm 135)"}
{"DOCID": "443", "TEXT": "Complex Number to a Real Power (Algorithm 106)"}
{"DOCID": "444", "TEXT": "Evaluation of Jacobi Symbol (Algorithm 99)"}
{"DOCID": "445", "TEXT": "COMBINATION (Algorithm 94)"}
{"DOCID": "446", "TEXT": "Simpson's Integration (Algorithm 84)"}
{"DOCID": "447", "TEXT": "Certification of the Calculation of Easter"}
{"DOCID": "448", "TEXT": "Path Matrix (Algorithm 141)"}
{"DOCID": "449", "TEXT": "Matrix Inversion(Algorithm 140)"}
{"DOCID": "450", "TEXT": "Solution of the Diophantine Equation (Algorithm 139)"}
{"DOCID": "451", "TEXT": "Nesting of for Statement II (Algorithm 138)"}
{"DOCID": "452", "TEXT": "Nesting of for Statement I (Algorithm 137)"}
{"DOCID": "453", "TEXT": "Enlargement of a Group (Algorithm 136)"}
{"DOCID": "454", "TEXT": "Crout with Equilibration and Iteration (Algorithm 135)"}
{"DOCID": "455", "TEXT": "Exponentiation of Series (Algorithm 134)"}
{"DOCID": "456", "TEXT": "RANDOM (Algorithm 133)"}
{"DOCID": "457", "TEXT": "Quantum Mechanical Integrals Over all Slater-Type Integrals"}
{"DOCID": "458", "TEXT": "Coefficient Determination (Algorithm 131)"}
{"DOCID": "459", "TEXT": "PERMUTE (Algorithm 130)"}
{"DOCID": "460", "TEXT": "MINIFUN (Algorithm 129)"}
{"DOCID": "461", "TEXT": "Coding of Medical Case History Data for Computer Analysis"}
{"DOCID": "462", "TEXT": "Computer Pattern Recognition Techniques: Electrocardiographic Diagnosis: The use of programmed digital computers as general pattern classification and recognition devices is one phase of the current lively interest in artificial intelligence.  It is important to choose a class of signals which is, at present, undergoing a good deal of visual inspection by trained people for the purpose of pattern recognition.  In this way comparisons between machine and human performance may be obtained.  A practical result also serves as additional motivation.  Clinical electrocardiograms make up such a class of signals.  The approach to the problem presented here centers upon the use of multiple adaptive matched filters that classify normalized signals.  The present report fives some of the background for the application of this method."}
{"DOCID": "463", "TEXT": "On Ambiguity in Phrase Structure Languages"}
{"DOCID": "464", "TEXT": "Syntactic Analysis by Digital Computer: This paper provides an account of the Shadow language that is used to describe syntax and of a corresponding subroutine that enables a computer to perform syntactic analysis.  The input to this subroutine consists of a string to be analyzed and a description of the syntax that is to be used.  The syntax is expressed in the Shadow language.  The output consists of a trace table that expresses the results of the syntactic analysis in a tabular form.  Several versions of the subroutine and some associated programs have been in use now for over three years.  The present account of the language and the subroutine contains a summary of material that has been described previously in unpublished reports and also some additional discussion of the work in relation to the more general questions of problem-oriented languages and string transformations."}
{"DOCID": "465", "TEXT": "PERM (Algorithm 115)"}
{"DOCID": "466", "TEXT": "General Order Arithmetic (Algorithm 93)"}
{"DOCID": "467", "TEXT": "Permutation Generator (Algorithm 87)"}
{"DOCID": "468", "TEXT": "Incomplete Elliptic Integrals (Algorithm 73)"}
{"DOCID": "469", "TEXT": "Critical Path Scheduling (Algorithm 40)"}
{"DOCID": "470", "TEXT": "Summation of Fourier Series (Algorithm 128)"}
{"DOCID": "471", "TEXT": "ORTHO (Algorithm 127)"}
{"DOCID": "472", "TEXT": "Gauss' Method (Algorithm 126)"}
{"DOCID": "473", "TEXT": "WEIGHTCOEFF (Algorithm 125)"}
{"DOCID": "474", "TEXT": "Input Data Organization in Fortran"}
{"DOCID": "475", "TEXT": "A Test Matrix for Inversion Procedures"}
{"DOCID": "476", "TEXT": "Further Remarks on Sampling a Tape File-II"}
{"DOCID": "477", "TEXT": "Further Remarks on Sampling a Tape File-I"}
{"DOCID": "478", "TEXT": "Implementing a Stack"}
{"DOCID": "479", "TEXT": "A Dispersion Pass Algorithm for the Polyphase Merge: This paper presents a new manner of dispersing strings for a Polyphase merge.  If the number of strings dispersed is between two levels acceptable by Polyphase merge, a more economical technique of reaching the next level for Polyphase merge is shown and proved."}
{"DOCID": "480", "TEXT": "Quick Calculation of Jacobian Elliptic Functions (Corrigendum)"}
{"DOCID": "481", "TEXT": "A One-Day Look At Computing"}
{"DOCID": "482", "TEXT": "TALL-A List Processor for the Philco 200 Computer"}
{"DOCID": "483", "TEXT": "On the Nonexistence of a Phrase Structure Grammar for ALGOL 60: ALGOL 60 is defined partly by formal mechanisms of phrase structure grammar, partly by informally stated restrictions.  It is shown that no formal mechanisms of the type used are sufficient to define ALGOL 60."}
{"DOCID": "484", "TEXT": "Hankel Function (Algorithm 124)"}
{"DOCID": "485", "TEXT": "Real Error Function, ERF(x) (Algorithm 123)"}
{"DOCID": "486", "TEXT": "Tridiagonal Matrix (Algorithm 122)"}
{"DOCID": "487", "TEXT": "NORMDEV (Algorithm 121)"}
{"DOCID": "488", "TEXT": "A Heuristic for Page Turning In a Multiprogrammed Computer"}
{"DOCID": "489", "TEXT": "Current Status of IPL-V for the Philco 2000 Computer (June 1962)"}
{"DOCID": "490", "TEXT": "Programmed Methods for Printer Graphical Output"}
{"DOCID": "491", "TEXT": "Use of Multiprogramming in the Design of a Low Cost Digital Computer"}
{"DOCID": "492", "TEXT": "Analysis of a File Addressing Method: This paper presents a new file addressing method based on the calculation of an address from the identification of a record.  For large recirculating type files, it seems to be more advantageous than customary ones.  The probability distribution of the displacement of records from their calculated address, which is one less than the number of probes required to address a record, is computed on the basis of a Markov chain model.  For the reader not interested in the mathematics, the introduction and the summary should be sufficient."}
{"DOCID": "493", "TEXT": "The Property Classification Method of File Design and Processing"}
{"DOCID": "494", "TEXT": "A Finite Sequentially Compact Process for the Adjoints of Matrices Over Arbitrary Integral Domains"}
{"DOCID": "495", "TEXT": "A Procedure for Inverting Large Symmetric Matrices: In the least squares method for simultaneous adjustment of several parameters, the coefficients of the normal equations are the elements of a symmetric positive-definite matrix.  In order to solve the normal equations and evaluate the precision measures of the resulting parameters, inversion of this matrix of coefficients is required.  Many available procedures for matrix inversion do not take advantage of the symmetry.  Thus, when programmed for a high-speed computer, all n^2 elements must be stored and manipulated, whereas only (n + 1)/2 of them are independent. In order to allow a computer of given memory capacity to handle a larger matrix, the following procedure for inverting a symmetric matrix has been devised."}
{"DOCID": "496", "TEXT": "A Set of Matrices for Testing Computer Programs"}
{"DOCID": "497", "TEXT": "Further Remarks on Line Segment Curve-Fitting Using Dynamic Programming: In a recent paper, Bellman showed how dynamic programming could be used to determine the solution to a problem previously considered by Stone.  The problem comprises the determination, given N, of the N points of subdivision of a given interval (a,B) and the corresponding line segments, that give the best least squares fit to a function g(x) in the interval. Bellman confined himself primarily to the analytical derivation, suggesting briefly, however, how the solution of the equation derived for each particular point of subdivision u(i) could be reduced to a discrete search.  In this paper, the computational procedure is considered more fully, and the similarities to some of Stone's equations are indicated. It is further shown that an equation for u(i) involving no minimization may be found.  In addition, it is shown how Bellman's method may be applied to the curve-fitting problem when the additional constraints are added that the ends of the line segments must be on the curve."}
{"DOCID": "498", "TEXT": "Magic Square (Algorithm 117 & 118)"}
{"DOCID": "499", "TEXT": "Permutation Generator (Algorithm 87)"}
{"DOCID": "500", "TEXT": "PERMUTE (Algorithm 86)"}
{"DOCID": "501", "TEXT": "JACOBI (Algorithm 85)"}
{"DOCID": "502", "TEXT": "Simpson's Integration (Algorithm 84)"}
{"DOCID": "503", "TEXT": "Rational Roots of Polynomials with Integer Coefficients (Algorithm 78)"}
{"DOCID": "504", "TEXT": "FACTORS (Algorithm 75)"}
{"DOCID": "505", "TEXT": "Composition Generator (Algorithm 72)"}
{"DOCID": "506", "TEXT": "PERMUTATION (Algorithm 71)"}
{"DOCID": "507", "TEXT": "Partition, Quicksort, Find (Algorithm 63, 64, 65)"}
{"DOCID": "508", "TEXT": "Matrix Inversion (Algorithm 58)"}
{"DOCID": "509", "TEXT": "Matrix Inversion (Algorithm 58)"}
{"DOCID": "510", "TEXT": "Ber or Bei Function (Algorithm 57)"}
{"DOCID": "511", "TEXT": "A Set of Test Matrices (Algorithm 52)"}
{"DOCID": "512", "TEXT": "Telescope 1 (Algorithm 37)"}
{"DOCID": "513", "TEXT": "SIEVE (Algorithm 35)"}
{"DOCID": "514", "TEXT": "Binomial Coefficients (Algorithm 19)"}
{"DOCID": "515", "TEXT": "Rational Interpolation by Continued Fractions (Algorithm 18)"}
{"DOCID": "516", "TEXT": "Matrix Inversion II (Algorithm 120)"}
{"DOCID": "517", "TEXT": "Evaluation of Pert Network (Algorithm 119)"}
{"DOCID": "518", "TEXT": "Magic Square (Odd Order) (Algorithm 118)"}
{"DOCID": "519", "TEXT": "Magic Square (Even Order) (Algorithm 117)"}
{"DOCID": "520", "TEXT": "Complex Division (Algorithm 116)"}
{"DOCID": "521", "TEXT": "PERM (Algorithm 115)"}
{"DOCID": "522", "TEXT": "Generation of Partitions with Constraints (Algorithm 114)"}
{"DOCID": "523", "TEXT": "TREESORT (Algorithm 113)"}
{"DOCID": "524", "TEXT": "Position of Point Relative to Polygon (Algorithm 112)"}
{"DOCID": "525", "TEXT": "A Computer Technique for Handling Analysis of Variance"}
{"DOCID": "526", "TEXT": "Character Manipulation in Fortran"}
{"DOCID": "527", "TEXT": "The Description List of Concepts: A concept is defined as a class of objects whose members can be distinguished by processing its properties.  Property is defined to mean a partition of the set of all objects into disjoint classes. The formal definition of a concept is recursive in nature. A concept is described by a list structure. A one-to-one correspondence is established between the recursive definition of a concept and its description list structure.  Like the definition, the description list structure of a concept is also built up from elementary list structures by a recursive process. The list structures obtained this way are compared with the description list structure discussed by the author in a previous publication."}
{"DOCID": "528", "TEXT": "FORTRAN for Business Data Processing"}
{"DOCID": "529", "TEXT": "Regression and Coded Patterns in Data Editing"}
{"DOCID": "530", "TEXT": "A Computer Method for Radiation Treatment Planning"}
{"DOCID": "531", "TEXT": "Person-Matching by Electronic Methods: Record linkage in the updating of files is accomplished in many establishments through the use of a preassigned number, such as payroll number, customer number, or social security number.  In vital and health records, however, a unique number is generally not preassigned to an individual for purposes of reporting services received to the health department.  In order to determine whether different physician reports refer to the same individual, name and other identification must be compared.  This is a laborious operation which is subject to various errors because of name misspellings, changes of name upon marriage, and other problems.  We are interested in the maintenance of a psychiatric case register in Maryland, where many of the reports from over a hundred psychiatric agencies refer to the same patient. These records must be linked in order to provide unduplicated counts of individuals under care and longitudinal records of psychiatric history.  An earlier paper [1] describes our general procedures for register maintenance by use of a digital computer (Honeywell 800).  Here we present in more detail our initial procedures for the person-matching process in order to elicit comments and suggestions from persons who have had experience in matching."}
{"DOCID": "532", "TEXT": "On the Computation of Rational Approximations to Continuous Functions"}
{"DOCID": "533", "TEXT": "Digital Synthesis of Correlated Stationary Noise: In this note we propose a method of generating stationary noise with a prescribed auto-covariance function by digital methods.  The need for such a technique often arises in testing the performance of data processing and engineering systems, where inputs corrupted with correlated noise (of a known form) are required.  The technique is quite simple and produces strict-sense stationary noise which agrees approximately with R(t), the prescribed auto-covariance function (acf), over an interval [-T(0), T(0)]. The method consists of approximating the spectral density by a periodic process with spectral lines, and then synthesizing the periodic noise with random phases and appropriate amplitudes.  In order to simplify discussion of the statistical properties of the noise generated, the technique is first presented in terms of exact harmonic analysis.  In practice, discrete harmonic analysis as presented in the third section is used."}
{"DOCID": "534", "TEXT": "Quick Calculation of Jacobian Elliptic Functions"}
{"DOCID": "535", "TEXT": "Triangular Walk Pattern for the Down-hill Method of Solving a Transcendental Equation"}
{"DOCID": "536", "TEXT": "Nonlinear Regression and the Solution of Simultaneous Equations: If one has a set of observables (Z1,...,Zm) which are bound in a relation with certain parameters (A1,...,An) by an equation S(Z1,...;A1,...)=0, one frequently has the problem of determining a set of values of the Ai which minimizes the sum of squares of differences between observed and calculated values of a distinguished observable, say Zm.  If the solution of the above equation for Zm,  Zm=N(Z1,...;A1,...) gives rise to a function N which is nonlinear in the Ai, then one may rely on a version of Gaussian regression [1,2] for an iteration scheme that converges to a minimizing set of values.  It is shown here that this same minimization technique may be used for the solution of simultaneous (not necessarily linear) equations."}
{"DOCID": "537", "TEXT": "A Machine Program for Theorem-Proving: The program of a proof procedure is discussed in connection with trial runs and possible improvements."}
{"DOCID": "538", "TEXT": "Quantum Mechanical Integrals of Slater-Type Orbitals (Algorithm 110)"}
{"DOCID": "539", "TEXT": "Definite Exponential Integrals B (Algorithm 109)"}
{"DOCID": "540", "TEXT": "Definite Exponential Integrals A (Algorithm 108)"}
{"DOCID": "541", "TEXT": "Simpson's Integration (Algorithm 84)"}
{"DOCID": "542", "TEXT": "FACTORS (Algorithm 75)"}
{"DOCID": "543", "TEXT": "Interpolation by Aitken (Algorithm 70)"}
{"DOCID": "544", "TEXT": "Ber or Bei Function (Algorithm 57)"}
{"DOCID": "545", "TEXT": "Adjust Inverse of a Matrix when an Element is Perturbed (Algorithm 51)"}
{"DOCID": "546", "TEXT": "Logarithm of a Complex Number (Algorithm 48)"}
{"DOCID": "547", "TEXT": "Gamma Function (Algorithm 34)"}
{"DOCID": "548", "TEXT": "Molecular-Orbital Calculation of Molecular Interactions"}
{"DOCID": "549", "TEXT": "Quantum Mechanical Integrals of Slater-Type Orbitals"}
{"DOCID": "550", "TEXT": "Definite Exponential Integrals B (Algorithm 109)"}
{"DOCID": "551", "TEXT": "Definite Exponential Integrals A (Algorithm 108)"}
{"DOCID": "552", "TEXT": "Gauss's Method (Algorithm 107)"}
{"DOCID": "553", "TEXT": "Complex Number to a Real Power (Algorithm 106)"}
{"DOCID": "554", "TEXT": "Newton Maehly, (Algorithm 105)"}
{"DOCID": "555", "TEXT": "Reduction to Jacobi (Algorithm 104)"}
{"DOCID": "556", "TEXT": "On Translation of Boolean Expressions"}
{"DOCID": "557", "TEXT": "Simulation of Computer Timing Device"}
{"DOCID": "558", "TEXT": "A Modified Inversion Procedure for Product Form of the Inverse Linear Programming Codes: This paper describes a new algorithm for the selection of the pivot row in matrix inversion when using the product form of the inverse.  This algorithm has been developed for linear programming codes; however, it would be valuable for the inversion of any non-dense matrix.  The procedures described in this paper have been thoroughly tested and have been in operation on the Esso Research and Engineering IBM 7090 computer for nine months.  Substantial computer cost savings have been realized because of this procedure."}
{"DOCID": "559", "TEXT": "Solution of Eigenvalue Problems With Approximately Known Eigenvectors"}
{"DOCID": "560", "TEXT": "Communication Between Independently Translated Blocks"}
{"DOCID": "561", "TEXT": "Analytic Differentiation By Computer"}
{"DOCID": "562", "TEXT": "AVINT (Algorithm 77)"}
{"DOCID": "563", "TEXT": "Sorting Procedures (Algorithm 76)"}
{"DOCID": "564", "TEXT": "CRAM (Algorithm 67)"}
{"DOCID": "565", "TEXT": "INVRS (Algorithm 66)"}
{"DOCID": "566", "TEXT": "Matrix Inversion (Algorithm 58)"}
{"DOCID": "567", "TEXT": "Logarithm of a Complex Number (Algorithm 48)"}
{"DOCID": "568", "TEXT": "Exponential of a Complex Number (Algorithm 46)"}
{"DOCID": "569", "TEXT": "Binomial Coefficients (Algorithm 19)"}
{"DOCID": "570", "TEXT": "Simpson's Rule Integrator (Algorithm 103)"}
{"DOCID": "571", "TEXT": "Permutation in Lexicographical Order (Algorithm 102)"}
{"DOCID": "572", "TEXT": "Add Item to Chain-Linked List (Algorithm 100)"}
{"DOCID": "573", "TEXT": "Remove Item From Chain-Linked List (Algorithm 101)"}
{"DOCID": "574", "TEXT": "Evaluation of Jacobi Symbol (Algorithm 99)"}
{"DOCID": "575", "TEXT": "Evaluation of Definite Complex Line Integrals (Algorithm 98)"}
{"DOCID": "576", "TEXT": "Shortest Path (Algorithm 97)"}
{"DOCID": "577", "TEXT": "ANCESTOR (Algorithm 96)"}
{"DOCID": "578", "TEXT": "Generation of Partitions in Part-Count Form (Algorithm 95)"}
{"DOCID": "579", "TEXT": "COMBINATION (Algorithm 94)"}
{"DOCID": "580", "TEXT": "General Order Arithmetic (Algorithm 93)"}
{"DOCID": "581", "TEXT": "A Note on Sampling a Tape-File"}
{"DOCID": "582", "TEXT": "One Lost Bit"}
{"DOCID": "583", "TEXT": "A Redundancy Check for ALGOL Programs"}
{"DOCID": "584", "TEXT": "Report on the Algorithmic Language FORTRAN II"}
{"DOCID": "585", "TEXT": "Initial Experience With an Operating Multiprogramming System: The Lewis Research Center has been using various forms and degrees of program simultaneity in the operation of its modified Sperry-Rand Univac Scientific Model 1103 computer during the last five years.  This simultaneity has evolved from an initial achievement of self-searching input and output to the automatic time sharing of independently coded problems.  Several important machine and program system modifications were necessary to accomplish this evolution.  Several additional modifications, although not required, were added to facilitate ease of coding and operation.  All modifications had to proceed at a relatively temperate pace to insure that the basic data-reduction work load of the computing center was completed on schedule.  Some educationally valuable mistakes were made, and their suggested cures often pointed the way to useful future improvements or emphasized some of the basic principles of a multiprogramming system.  The material that follows is a description of the evolution of the programming and hardware system which has developed into the present multiprogramming system at Lewis research Center."}
{"DOCID": "586", "TEXT": "Simultaneous System of Equations and Matrix Inversion Routine (Algorithm 92)"}
{"DOCID": "587", "TEXT": "Romberg Integration (Algorithm 60)"}
{"DOCID": "588", "TEXT": "Chebyshev Curve-Fit (Algorithm 91)"}
{"DOCID": "589", "TEXT": "Evaluation of the Fresnel Cosine Integral (Algorithm 90)"}
{"DOCID": "590", "TEXT": "Evaluation of the Fresnel Sine Integral (Algorithm 89)"}
{"DOCID": "591", "TEXT": "Evaluation of Asymptotic Expression for the Fresnel Sine and Cosine Integrals (Algorithm 88)"}
{"DOCID": "592", "TEXT": "COBOL Batching Problems"}
{"DOCID": "593", "TEXT": "An Introduction to a Machine-Independent Data Division"}
{"DOCID": "594", "TEXT": "An Advanced Input-Output System for a COBOL Compiler"}
{"DOCID": "595", "TEXT": "Guides to Teaching COBOL: The teaching of COBOL can be divided into three main subject areas.  They are the syntax of COBOL, the use of such syntax in solving any given problem, and programming concepts.  It is generally accepted that some knowledge of the hardware and computer logic must be possessed by the programmer. The teaching problem arises in determining how thoroughly a student must know the hardware and logic for that computer for which he will write COBOL programs. Unfortunately, historical data concerning students' programming proficiency is almost non-existent and, at best, difficult to measure.  How then might we approach solving this problem?"}
{"DOCID": "596", "TEXT": "Floating-Point Arithmetic in COBOL: In this paper the basic operations of floating-point arithmetic are examined and COBOL procedures for carrying these out are given, along with specification of working storage.  The paper concludes with an example in which these procedures are used."}
{"DOCID": "597", "TEXT": "Modular Data Processing Systems Written in COBOL"}
{"DOCID": "598", "TEXT": "The COBOL Librarian - A Key to Object Program Efficiency: Many answers to the question \"How may a COBOL Compiler be forced into the generation of an efficient object program?\"  The purpose of this article is to present one possible answer: the creation and full utilization of a well-constructed COBOL Library."}
{"DOCID": "599", "TEXT": "A Report Writer For COBOL"}
{"DOCID": "600", "TEXT": "Syntactical Charts of COBOL 61"}
{"DOCID": "601", "TEXT": "Interim Report on Bureau of Ships COBOL Evaluation Program"}
{"DOCID": "602", "TEXT": "COBOL and Compatibility"}
{"DOCID": "603", "TEXT": "Basic Elements of COBOL 61"}
{"DOCID": "604", "TEXT": "Why COBOL?"}
{"DOCID": "605", "TEXT": "Computer Simulation Of City Traffic: In simulating traffic flow on city streets, the National Bureau of Standards has used data processing techniques to tabulate and make motion pictures of vehicle movements in the model.  Each vehicle is assigned a digital identification giving points of entry and exit, type of vehicle, desired speed, and actual speed, in proportions simulating field data. Changes in the model can be made to observe their consequences and to determine the ability of a real street to carry loads expected in the future."}
{"DOCID": "606", "TEXT": "A Method for Eliminating Ambiguity Due to Signal Coincidence in Digital Design"}
{"DOCID": "607", "TEXT": "The Calculation of Easter..."}
{"DOCID": "608", "TEXT": "Permutation (Algorithm 71)"}
{"DOCID": "609", "TEXT": "Permutation (Algorithm 71)"}
{"DOCID": "610", "TEXT": "SIEVE (Algorithm 35)"}
{"DOCID": "611", "TEXT": "Permutation Generator (Algorithm 87)"}
{"DOCID": "612", "TEXT": "Permute (Algorithm 86)"}
{"DOCID": "613", "TEXT": "JACOBI (Algorithm 85)"}
{"DOCID": "614", "TEXT": "Simpson's Integration (Algorithm 84)"}
{"DOCID": "615", "TEXT": "Addressing Multidimensional Arrays: A useful method of representing a function of n variables is to consider the function to assume its values at selected points in n-dimensional space. Although this picture is of value to the analyst, the elements of an n-dimensional array must exist in conventional storage as a linear array or vector. The means of performing the transformation of a set of indices locating on array element in n-space to the location (address) of the element in its storage vector is the subject of this paper.  It is noted that the index address transformation is computationally identical to the conversion of a number from a fixed to a mixed radix number system.  Several ways of implementing the transformation are described."}
{"DOCID": "616", "TEXT": "An Information Algebra - Phase I Report-Language Structure Group of the CODASYL Development Committee: This report represents the results of the first phase of the work of the Language Structure Group.  The goal of this work is to arrive at a proper structure for a machine-independent problem-defining language, at the systems level of data processing.  The report is based, for the most part, on a mathematical model called \"An Information Algebra\" developed primarily by R. Bosak.  It is hoped that this report will be read (a) with avid interest by programming language designers and implementors, and all those interested in developing a theoretical approach to data processing; (b) with interest and understanding by professional programmers and systems analysts; and (c) with appreciation by the businessman-analyst-manager. The authors have not attempted an exhaustive discourse in this report.  Rather, they have tried to present a philosophy to the professional people who are vitally concerned with providing a working language for the systems analyst's use.  They trust that the ideas in this report will stimulate others to think along similar lines.  Questions and comments will be welcomed, and can be addressed to any of the members of the Language Structure Group:  Robert Bosak, System Development Corporation;  Richard F. Clippinger, Honeywell EDP Division;  Carey Dobbs, Remington Rand Univac Division;  Roy Goldfinger (Chairman), IBM Corporation;  Renee B. Jasper, Navy Management Office; William Keating, National Cash Register;  George Kendrick, General Electric Company;  Jean E. Sammet, IBM Corporation."}
{"DOCID": "617", "TEXT": "POSEIDON: Any computer that forms part of a control system-whether completely automatic or partly human-must work at the same speed as the control system.  It must perform its calculations or data processing fast enough for the results to be available at the required instants in the action of the control system. This known as working in \"real time.\""}
{"DOCID": "618", "TEXT": "Computers- The Key to Total Systems Control: An Industrial Viewpoint: Man-Man-machine processes are characterized in five main types, and the markets for each type are shown for 1950 and 1960 and estimated for 1970."}
{"DOCID": "619", "TEXT": "Retrieval of Misspelled Names in an Airlines Passenger Record System: This paper discusses the limited problem of recognition and retrieval of a given misspelled name from among a roster of several hundred names, such as the reservation inventory for a given flight of a large jet airliner.  A program has been developed and operated on the Telefile (a stored-program core and drum memory solid-state computer) which will retrieve passengers' records successfully, despite significant misspellings either at original entry time or at retrieval time.  The procedure involves an automatic scoring technique which matches the names in a condensed form. Only those few names most closely resembling the requested name, with their phone numbers annexed, are presented for the agents final manual selecton.  The program has successfully isolated and retrieved names which were subjected to a number of unusual (as well as usual) misspellings."}
{"DOCID": "620", "TEXT": "RATFACT (Algorithm 78)"}
{"DOCID": "621", "TEXT": "Romberg Integration (Algorithm 60)"}
{"DOCID": "622", "TEXT": "Optimal Classification of Objects (Algorithm 83)"}
{"DOCID": "623", "TEXT": "Economising a Sequence 2 (Algorithm 82)"}
{"DOCID": "624", "TEXT": "Economising a Sequence 1 (Algorithm 81)"}
{"DOCID": "625", "TEXT": "Reciprocal Gamma Function of Real Argument (Algorithm 80)"}
{"DOCID": "626", "TEXT": "A Method of Representation, Storage and Retrieval of 13 Random Codes in a 4-Digit Number or 16 Random Codes in a 5-Digit Number"}
{"DOCID": "627", "TEXT": "Knotted List Structures"}
{"DOCID": "628", "TEXT": "On a Floating-Point Number Representation For Use with Algorithmic Languages"}
{"DOCID": "629", "TEXT": "On a Wired-In Binary-to-Decimal Conversion Scheme"}
{"DOCID": "630", "TEXT": "An Evaluation of Autocode Readability: Of the many requirements of an autocode, the pair of requirements \"easy to read\" and \"easy to write\" are not often compatible.  This paper argues that readability can be added automatically in the translation process so that the programmer can enjoy the utmost economy of expression, while for management a full and valid COBOL version is printed to give all the advantages of readability and compatibility."}
{"DOCID": "631", "TEXT": "Automatic-Programming-Language Translation Through Syntactical Analysis*"}
{"DOCID": "632", "TEXT": "Vectorcardiographic Diagnosis With The Aid of ALGOL"}
{"DOCID": "633", "TEXT": "Simulation and Analysis of Biochemical Systems (III. Analysis and Pattern Recognition)"}
{"DOCID": "634", "TEXT": "Manipulation of Trees in Information Retrieval*"}
{"DOCID": "635", "TEXT": "A Note on Multiplying Boolean Matrices"}
{"DOCID": "636", "TEXT": "Tape Splitting in an Iterative Program"}
{"DOCID": "637", "TEXT": "A NELIAC-Generated 7090-1401 Compiler: NELIAC systems for several different machines have been generated using the original NELIAC system developed at the Naval Electronics Laboratory, San Diego, in 1958.  A basic \"bootstrap\" process was used to generate all but the first, i.e. the systems were described in the NELIAC language and generated by an existing NELIAC compiler.  This experience has shown there is no inherent difficulty in \"building compilers with compilers\"; indeed, it pointed out many advantages in using a POL for constructing programming systems.  This report presents the results of a project completed in May, 1961 in which the NELIAC system was used to generate a compiler for the IBM 1401.  The 1401 compiler, which runs on the 7090 and produces 1401 programs, was described in the NELIAC language and generated with 7090 NELIAC system.  The reduction in programming time and the improvement in documentation of the system were very significant."}
{"DOCID": "638", "TEXT": "SURGE: A Recoding of the COBOL Merchandise Control Algorithm"}
{"DOCID": "639", "TEXT": "Difference Expression Coefficients (Algorithm 79)"}
{"DOCID": "640", "TEXT": "Rational Roots of Polynomials with Integer Coefficients (Algorithm 78)"}
{"DOCID": "641", "TEXT": "Interpolation, Differentiation, and Integration (Algorithm 77)"}
{"DOCID": "642", "TEXT": "An Introduction to ALGOL"}
{"DOCID": "643", "TEXT": "Simulation and Analysis of Biochemcial Systems (II. Solution of Differential Equations)"}
{"DOCID": "644", "TEXT": "A String Language for Symbol Manipulation Based on ALGOL 60: An artificial computer programming language is proposed for describing the manipulation of strings of characters and symbols.  The concept of strings, introduced in the ALGOL 60 report, is extended by adding: (1) the declaration of strings, substrings, and string arrays with explicit lengths; (2) the ability to concatenate and shift strings; and (3) the ranking of symbols for comparing stings in Boolean relations.  A primer or informal description of the language is followed by examples, a description of experiments with the language on an IBM 704 computer, and a formal description which, taken with the ALGOL 60 Report, defines the proposed string language."}
{"DOCID": "645", "TEXT": "INVRS (Algorithm 66)"}
{"DOCID": "646", "TEXT": "Inverse of a Finite Segment of the Hilbert Matrix (Algorithm 50)"}
{"DOCID": "647", "TEXT": "Numerical Solution of the Polynomial Equation (Algorithm 30)"}
{"DOCID": "648", "TEXT": "Sorting Procedures (Algorithm 76)"}
{"DOCID": "649", "TEXT": "FACTORS (Algorithm 75)"}
{"DOCID": "650", "TEXT": "Curve Fitting with Constraints (Algorithm 74)"}
{"DOCID": "651", "TEXT": "A Survey of Languages and Systems for Information Retrieval"}
{"DOCID": "652", "TEXT": "Use of Semantic Structure in Information Systems"}
{"DOCID": "653", "TEXT": "Translation of Retrieval Requests Couched in a \"Semiformal\" English-Like Language*"}
{"DOCID": "654", "TEXT": "Language Problems Posed by Heavily Structured Data"}
{"DOCID": "655", "TEXT": "COMIT as an IR Language: Many of the features that make COMIT a good all around symbol manipulation language also render it well suited to various types of information retrieval programs.  Presented here is a general discussion of this unique and different programming language and an examination of some of its applications."}
{"DOCID": "656", "TEXT": "An Information System With The Ability To Extract Intelligence From Data"}
{"DOCID": "657", "TEXT": "Information Structures for Processing and Retrieving"}
{"DOCID": "658", "TEXT": "Discussion-The Pros and Cons of a Special IR Language"}
{"DOCID": "659", "TEXT": "Reversion of Series (Algorithm 193)"}
{"DOCID": "660", "TEXT": "More Test Matrices for Determinants and Inverses (Pracnique)"}
{"DOCID": "661", "TEXT": "Indexing and the Lambda-Notation: Some methods of indexing sequentially stored elements of sparse multi-dimensional arrays are described in the scheme A notation."}
{"DOCID": "662", "TEXT": "Shuttle Sort (Algorithm 175)"}
{"DOCID": "663", "TEXT": "Determinant (Algorithm 159)"}
{"DOCID": "664", "TEXT": "Assignment (Algorithm 27)"}
{"DOCID": "665", "TEXT": "Gauss-Seidel (Algorithm 220)"}
{"DOCID": "666", "TEXT": "Topological Ordering for Pert Networks (Algorithm 219)"}
{"DOCID": "667", "TEXT": "Kutta Merson (Algorithm 218)"}
{"DOCID": "668", "TEXT": "Minimum Excess Cost Curve (Algorithm 217)"}
{"DOCID": "669", "TEXT": "A Specification of JOVIAL"}
{"DOCID": "670", "TEXT": "Some Legal Implications of the Use of Computers in the Banking Business: The introduction of computers in to the banking business has a wide variety of legal implications that merit careful attention at this very early stage. The industry is highly regulated by government and, hence, is subject to many statutes and regulations. It also is affected by important common law rules established by courts.  The legal ramifications involve not only the mechanization itself, but also the very significant, economically attractive phenomenon of off premises processing.  It is essential to identify and provide for many legal aspects right now, before systems and practices crystallize, in order to avoid the later impact of unanticipated physical complications and expense.  The legal aspects of computerization in the banking business are especially diverse.  In some states, there might be the basic question whether banks are authorized by law to invest in the new facilities, either directly or through cooperatives.  More challenging are questions relating to off-premises processors, particularly with respect to the obligation not to disclose information concerning a bank's customers, the adequacy of fidelity bond coverage, the extent of liability for improper refusal to pay a check, and susceptibility to regulation by government agencies.  Also pertinent is the propriety of data processing by banks for nonbank entities and particularly of the rendering of that service without charge for bank depositors."}
{"DOCID": "671", "TEXT": "TELEFILE-A Case Study of an On-Line Savings Bank Application: The development of an on-line computer system for a savings bank institution is traced from the early conceptual needs of the bank to the consummation of design by The Teleregister Corporation. Both bank and equipment criteria are specified which led to the development of the Telefile System of The Teleregister Corporation.  Operation of the on-line and off-line programs are described and statistics are cited for reliability and performance of the system. Benefits to the bank are discussed from the banker's point of view; an indication of future trends in the on-line savings bank field is also discussed."}
{"DOCID": "672", "TEXT": "Recent Developments Affecting ADP in Tax Administration"}
{"DOCID": "673", "TEXT": "Account Classification at Automating Banks"}
{"DOCID": "674", "TEXT": "Application of IBM 1620 EDP Methods to the Calculation of the Formation Constants of Complex Irons"}
{"DOCID": "675", "TEXT": "Coding Clinical Laboratory Data For Automatic Storage and Retrieval: A series of clinical laboratory codes have been developed to accept and store urin analysis, blood chemistry, and hematology test results for automatic data processing.  The codes, although constructed as part of a computerized hospital simulation, have been able to handle the results of every laboratory test that they have encountered.  The unique feature of these codes is that they can accept conventionally recorded qualitative as well as quantitative test results. Consequently, clinical test results need not be arbitrarily stratified, standardized, or altered in any way to be coded.  This paper describes how the codes were developed and presents a listing of the urin analysis codes.  Five criteria used in developing the codes are outlined and the problem of multiple-synonymous terminology is discussed.  A solution to the problem is described.  Flexible, computer-produced, composite laboratory reports are also discussed, along with reproduction of such a report. The paper concludes that even though many problems remain unsolved, the next ten years could witness the emergence of a practical automated information system in the laboratory."}
{"DOCID": "676", "TEXT": "On the Computation of a Certain Type of IncompleteBeta Functions"}
{"DOCID": "677", "TEXT": "Length of Strings for a Merge Sort: Detailed statistics are given on the length of maximal sorted strings which result form the first (internal sort) phase of a merge sort onto tapes. It is shown that the strings produced by an alternating method (i.e. one which produces ascending and descending strings alternately) tend to be only three-fourths as long as those in a method which produces only ascending strings, contrary to statements which have appeared previously in the literature.  A slight modification of the read-backward polyphase merge algorithm is therefore suggested."}
{"DOCID": "678", "TEXT": "Optimizing Bit-time Computer Simulation: A major component of a bit-time computer simulation program is the Boolean compiler.  The compiler accepts the Boolean functions representing the simulated computer's digital circuits, and generates corresponding sets of machine instructions which are subsequently executed on the \"host\" computer.  Techniques are discussed for increasing the sophistication of the Boolean compiler so as to optimize bit-time computer simulation.  The techniques are applicable to any general-purpose computer."}
{"DOCID": "679", "TEXT": "Recent Improvements in MADCAP: MADCAP is a programming language admitting subscripts, superscripts and certain forms of displayed formulas.  The basic implementation of this language was described in a previous paper [MADCAP: A scientific compiler for a displayed formula textbook language, Comm. ACM 4 (Jan. 61), 31-36].  This paper discusses recent improvements in the language in three areas: complex display, logical control, and subprogramming. In the area of complex display, the most prominent improvements are a notation for integration and for the binomial coefficients.  In the area of logical control the chief new feature is a notation for variably nested looping.  The discussion of subprogramming is focused on MADCAP's notation for and use of \"procedures.\""}
{"DOCID": "680", "TEXT": "An Error-Correcting Parse Algorithm"}
{"DOCID": "681", "TEXT": "Flexible Abbreviation of Words in a Computer Language"}
{"DOCID": "682", "TEXT": "Recursive programming in FORTRAN II"}
{"DOCID": "683", "TEXT": "A Serial Technique to Determine Minimum Paths"}
{"DOCID": "684", "TEXT": "Interpolation, Differentiation, and Integration (Algorithm 77)"}
{"DOCID": "685", "TEXT": "Euler Summation (Algorithm 8)"}
{"DOCID": "686", "TEXT": "Smooth (Algorithm 216)"}
{"DOCID": "687", "TEXT": "Shanks (Algorithm 215)"}
{"DOCID": "688", "TEXT": "q-Bessel Functions In(t)(Algorithm 214)"}
{"DOCID": "689", "TEXT": "Report of a Visit to Discuss Common Programming Languages in Czechoslovakia and Poland, 1963"}
{"DOCID": "690", "TEXT": "USA Participation in an International Standard glossary on Information Processing"}
{"DOCID": "691", "TEXT": "A Description of the APT Language: The APT (Automatically Programmed Tools) language for numerical control programming is described using the metalinguistic notation introduced in the ALGOL 60 report.  Examples of APT usage are included. Presented also are an historical summary of the development of APT and a statement concerning its present status."}
{"DOCID": "692", "TEXT": "On the Inverse of a Test Matrix"}
{"DOCID": "693", "TEXT": "An Extension of Fibonaccian Search To Several Variables: A technique which uses Fibonaccian search concepts has been developed to solve optimization problems involving unimodal functions of several variables. The technique has not been proven to be optimal in the sense that the one-dimensional Fibonaccian search is.  However, it is valuable for certain kinds of calculations."}
{"DOCID": "694", "TEXT": "A Comparison of Disks and Tapes: The principal characteristics of current magnetic disks and tape units are summarized and compared. Some of the characteristics of disk files are illustrated in a sorting example and compared to a tapesort. The conclusion is presented that disk files are competitive to tapes in some important applications."}
{"DOCID": "695", "TEXT": "Use of the Disk File on Stretch: The paper begins by briefly describing the Stretch (IBM 7030) computer with special emphasis given to the organization and operation of its input-output equipment.  Physical characteristics of the two-disk system (4,194,304 72-bit words, 8 usec-per-word transmission rate, etc.) are noted.  Timing limitations due to arm motion and disk rotation are discussed. Applications of disk usage are discussed separately for problem programs and for systems programs such as compilers and the supervisory program. Approximately 260,000 words of disk storage are reserved for the storage of systems programs and the subroutine library.  Problem programs, however, are not currently filed on the disk.  Certain programming techniques are discussed for transmitting words between disk and core storage with minimum delaying and interruption of the arithmetic unit.  Dumps on disk are considered for both recovery from computer malfunction and for mathematical or physical developments during the calculation.  Some comments are made regarding the reliability, economics, utility and weaknesses or limitations of the disk system.  Several possible future applications are noted which appear to have disk connotations."}
{"DOCID": "696", "TEXT": "An Automatic Data Acquisition and Inquiry System Using Disk Files: Lockheed Missiles and Space Company has installed a large-scale Automatic Data Acquisition (ADA) system which ties together the Company's manufacturing facilities located in Van Nuys and Sunnyvale, California.  The system includes over 200 remote Input Stations which collect and transmit Company operating data to a central Data Processing Center.  Two RCA 301 EDP Systems are used to record and control the flow of data transmitted to the Data Processing Center. A large capacity RCA 366 Data Disc File is used to store information required to provide up-to-date information in response to inquiries received from remotely located Inquiry Stations.  In addition to storage of data on the disk files, the system automatically records all incoming and outgoing data on magnetic tape to be used as input to the Company's conventional off-line business data processing applications."}
{"DOCID": "697", "TEXT": "A Numerical Method for the Determination of Moving Field Isodose Curves for Treatment Planning in Radiotherapy"}
{"DOCID": "698", "TEXT": "DATA-DIAL: Two-Way Communication with Computers From Ordinary dial Telephones: An operating system is described which allows users to call up a remotely located computer from ordinary dial telephones.  No special hardware or connections are required at the users' telephones. Input to the computer is through the telephone dial;output from the computer is in spoken form.  Results of a test with telephones in the Boston area are reported."}
{"DOCID": "699", "TEXT": "A Contour-Map Program for X-Ray Crystallography: A FORTRAN program is described for use with the IBM 7090 system and an X, Y-plotter to produce a contour map.  A matrix of points evenly spaced in each dimension is contoured.  Scale factors along the axes may be different and the axes need not be perpendicular."}
{"DOCID": "700", "TEXT": "Hermite Interpolation (Algorithm 210)"}
{"DOCID": "701", "TEXT": "Shuttle Sort (Algorithm 175)"}
{"DOCID": "702", "TEXT": "Assign (Algorithm 173)"}
{"DOCID": "703", "TEXT": "Assign (Algorithm 173)"}
{"DOCID": "704", "TEXT": "Combinatorial of M Things Taken One At A Time Two At A Time, Up To N At A Time (Algorithm 161)"}
{"DOCID": "705", "TEXT": "Combinatorial Of M Things Taken N At A Time (Algorithm 160)"}
{"DOCID": "706", "TEXT": "Fourier Series Approximation (Algorithm 157)"}
{"DOCID": "707", "TEXT": "Erf(x) (Algorithm 123)"}
{"DOCID": "708", "TEXT": "Evaluation of the Fresnel Integrals (Algorithm 88, 89, 90)"}
{"DOCID": "709", "TEXT": "Assignment (Algorithm 27)"}
{"DOCID": "710", "TEXT": "Fresnel Integrals (Algorithm 213)"}
{"DOCID": "711", "TEXT": "Frequency Distribution (Algorithm 212)"}
{"DOCID": "712", "TEXT": "Hermite Interpolation (Algorithm 211)"}
{"DOCID": "713", "TEXT": "Lagrangian Interpolation (Algorithm 210)"}
{"DOCID": "714", "TEXT": "Gauss (Algorithm 209)"}
{"DOCID": "715", "TEXT": "Discrete Convolution (Algorithm 208)"}
{"DOCID": "716", "TEXT": "Stringsort (Algorithm 207)"}
{"DOCID": "717", "TEXT": "Partitioning Algorithms for Finite Sets: The partitions of a set with n elements are represented by certain n-tuples of positive integers. Algorithm are described which generate without repetitions the n-tuples corresponding to: (1) all partitions of the given set, (2) all partitions of the given set into m or fewer sets (1 <= m <= n), and (3) all partitions of the given set into exactly m sets (1 <= m <= n)."}
{"DOCID": "718", "TEXT": "An Experiment in Automatic Verification of Programs: How effective is a compiler at replacing explicit verification, and what is the cost of this technique?"}
{"DOCID": "719", "TEXT": "Variable Width Stacks: Character addressable, variable field computers permit ready establishment and manipulation of variable width stacks.  Single machine commands may push variable field items down into such stacks or pop them up.  The availability of a variety of field delimiters allows the machine to push down or pop up more than one variable width item with one command. Since these stacking operations can be made the basis of compiler decoding algorithms the proper use of machines of this class for compilation has advantages over machines with fixed-length words."}
{"DOCID": "720", "TEXT": "Format-Free Input in FORTRAN"}
{"DOCID": "721", "TEXT": "Report on Proposed American Standard Flowchart Symbols for Information Processing: This paper presents the essential contents of the Proposed American Standard Flowchart Symbols for Information Processing.  This is the first proposed standard prepared by Subcommittee X3.6 on Problem Description and Analysis of the American Standards Association (ASA)."}
{"DOCID": "722", "TEXT": "ALCOR Group Representation of ALGOL Symbols"}
{"DOCID": "723", "TEXT": "ECMA Subset of ALGOL 60"}
{"DOCID": "724", "TEXT": "A Profile of the Programmer: Synopsis: 549 members of the ACM participated in a study concerned primarily with the attitudes of programmers toward their careers and jobs.  A very high percentage of programmers have apparently entered their careers by accident; it has proven a happy choice for most and they expect to remain in the field during the next five years.  Their principal job satisfactions relate to the nature of their work, and mostfind their jobs offer high level of professional interest and good working conditions. Salary and advancement prospects, however,are not as satisfactory.  More than half report a positive attitude toward programmers and programming on the part of their organizations.  Turnover among themselves is attributed primarily to poor management-salary is seen as the principal motivating factor in turnover among other programmers.  Nature of the work offered and salary are principal determinants in accepting a new job.  Programmers are less mobile than expected. Programmers tend to see their colleagues in a favorable light, on the whole.  Personalities seem to vary with function, systems programmers differing from applications programmers.  Four principal problems for programming in the immediate future are listed by participants: languages, personnel, various specific applications and techniques, and building programming as a profession."}
{"DOCID": "725", "TEXT": "Group Participation Computer Demonstration"}
{"DOCID": "726", "TEXT": "A General Program for the Analysis of Square and Rectangular Lattice Designs: This paper describes a general-purpose program that will handle those incomplete block designs known as square and rectangular lattices.  Flow diagrams are given so that the method of calculation may be programmed for any digital computer."}
{"DOCID": "727", "TEXT": "On the Approximate Solution of Delta(u)=F(u): Three-dimensional Dirichlet problems for Delta(u)=F(u), Fu >= 0, are treated numerically by an exceptionally fast, exceptionally accurate numerical method.  Programming details, numerous examples and mathematical theory are supplied.Extension of the method in a natural way to n-dimensional problems is indicated by means of a 4-dimensional example."}
{"DOCID": "728", "TEXT": "Computer-Drawn Flowcharts*: To meet the need for improved documentation of written computer programs, a simple system for effective communication is presented, which has shown great promise.  The programmer describes his program in a simple format, and the computer prepares flow charts and other cross-referenced listings from this input.  The description can be kept up-to-date easily, and the final output clearly explains the original program.  The system has also proved to be a valuable debugging and coding aid."}
{"DOCID": "729", "TEXT": "A Generalization of ALGOL"}
{"DOCID": "730", "TEXT": "MIRFAG: A Compiler Based on Standard Mathematical Notation And Plain English: A pilot version of the compiler MIRFAG, now in operation, is described.  The chief features of the system, which is intended for the solution of scientific problems, are the presentation of mathematical formulas entirely in standard textbook notation.  The use of plain English for organizational instructions, automatic error diagnosis indicating the actual location of the error in the uncompiled program, and an attempt to minimize that fragmentation of the original problem statement which is a normal feature of programming systems."}
{"DOCID": "731", "TEXT": "Symmetric List Processor: A list processing system in which each list cell contains both a forward and a backward link as well as a datum is described.  This system is intended for imbeding in higher level languages capable of calling functions and subroutines coded in machine language. The presentation is in the form of FORTRAN programs depending on only a limited set of FORTRAN programs depending on only a limited set of \"primitive\" machine language subroutines which are also defined. Finally, a set of field, particularly character, manipulation primitives are given to round out the system."}
{"DOCID": "732", "TEXT": "Monte Carlo Inverse (Algorithm 166)"}
{"DOCID": "733", "TEXT": "Newton Interpolation with Forward Divided Differences (Algorithm 169)"}
{"DOCID": "734", "TEXT": "Newton Interpolation with Backward Divided Differences (Algorithm 168)"}
{"DOCID": "735", "TEXT": "Calculation of Confluent Divided Differences (Algorithm 167)"}
{"DOCID": "736", "TEXT": "Modified Hankel Functions (Algorithm 163)"}
{"DOCID": "737", "TEXT": "Exponentiation of Series (Algorithm 158)"}
{"DOCID": "738", "TEXT": "Fourier Series Approximation (Algorithm 157)"}
{"DOCID": "739", "TEXT": "MINIFUN (Algorithm 129)"}
{"DOCID": "740", "TEXT": "INTEREST (Algorithm 45)"}
{"DOCID": "741", "TEXT": "Evaluation of Determinant (Algorithm 41)"}
{"DOCID": "742", "TEXT": "Evaluation of Determinant (Algorithm 41)"}
{"DOCID": "743", "TEXT": "ARCCOSIN (Algorithm 206)"}
{"DOCID": "744", "TEXT": "ATIVE (Algorithm 205)"}
{"DOCID": "745", "TEXT": "STEEP2 (Algorithm 204)"}
{"DOCID": "746", "TEXT": "STEEP1 (Algorithm 203)"}
{"DOCID": "747", "TEXT": "Generation of Permutations in Lexicographical Order (Algorithm 202)"}
{"DOCID": "748", "TEXT": "A Semi-Iterative Process for Evaluating Arctangents"}
{"DOCID": "749", "TEXT": "Note onStochastic Matrices"}
{"DOCID": "750", "TEXT": "PEI Matrix Eigenvectors"}
{"DOCID": "751", "TEXT": "A Note on a Set of Test Matrices for Inversion"}
{"DOCID": "752", "TEXT": "Closing Out a Print Tape"}
{"DOCID": "753", "TEXT": "A Procedure for Converting Logic Table Conditions into an Efficient Sequence of Test Instructions"}
{"DOCID": "754", "TEXT": "Ye Indiscreet Monitor"}
{"DOCID": "755", "TEXT": "An Exponential Method of Numerical Integration of Ordinary Differential Equations: A formula for numerical integration is prepared, which involves an exponential term.  This formula is compared to two standard integration methods, and it is shown that for a large class of differential equations, the exponential formula has superior stability properties for large step sizes.  Thus this formula may be used with a large step size to decrease the total computing time for a solution significantly, particularly in those engineering problems where high accuracy is not needed."}
{"DOCID": "756", "TEXT": "A Computer Program for Editing the News"}
{"DOCID": "757", "TEXT": "Simulation of a Traffic Network"}
{"DOCID": "758", "TEXT": "Skeletal Structure of PERT and CPA Computer Programs: An introduction to the inner mechanics of PERT and CPA computer programs is provided.  The major components of these programs as well as their purposes and interrelationships are outlined."}
{"DOCID": "759", "TEXT": "Continued Operation Notation for Symbol Manipulation and Array Processing: A brief account is given of a notational device that is very useful in the formal representation of syntaxes, string relationships and string transformation procedures and also of computing procedures that deal with arrays of functions of many variables. The device consists of the use of certain \"continued operation\" or \"collective\" symbols that are analogous to the summation symbol (Sigma) and continued multiplication symbol (Pi) of conventional mathematics."}
{"DOCID": "760", "TEXT": "Dialects of FORTRAN"}
{"DOCID": "761", "TEXT": "A Note on the Dangling Else in ALGOL 60: Some revisions of ALGOL 60 are proposed, which not only eliminate certain ambiguous statements but also add some convenience to the language.  A discussion of the background of the problem and a sketch of a proof that the ambiguities have been removed is included."}
{"DOCID": "762", "TEXT": "Some Remarks on the Syntax of Symbolic Programming Languages"}
{"DOCID": "763", "TEXT": "A Syntax Controlled Generator of Formal Language Processors"}
{"DOCID": "764", "TEXT": "Reduction of a Matrix Containing Polynomial Elements (Algorithm 170)"}
{"DOCID": "765", "TEXT": "Orthogonal Polynomial Least Squares Surface Fit (Algorithm 164)"}
{"DOCID": "766", "TEXT": "XY-move Plotting (Algorithm 162)"}
{"DOCID": "767", "TEXT": "Certification of Algorithm 161 Combinatorial of M Things Taken One at a Time, Two at a Time, Up to N at a Time [M. L. Wolfson and H. V. Wright, Comm. ACM, Apr. 1963]"}
{"DOCID": "768", "TEXT": "Certification of Algorithm 160 Combinatorial of M Things Taken N at a Time [M. L. Wolfson and H. V. Wright, Comm. ACM, Apr. 1963]"}
{"DOCID": "769", "TEXT": "Algebra of Sets (Algorithm 156)"}
{"DOCID": "770", "TEXT": "Combination in Any Order (Algorithm 155)"}
{"DOCID": "771", "TEXT": "Combination in Lexicographical Order (Algorithm 154)"}
{"DOCID": "772", "TEXT": "GOMORY (Algorithm 153)"}
{"DOCID": "773", "TEXT": "Matrix Inversion (Algorithm 140)"}
{"DOCID": "774", "TEXT": "Jacobi (Algorithm 85)"}
{"DOCID": "775", "TEXT": "Interpolation, Differentiation, and Integration (Algorithm 77)"}
{"DOCID": "776", "TEXT": "Partition, Quicksort, and Find (Algorithm 62, 64, & 65)"}
{"DOCID": "777", "TEXT": "A Set of Test Matrices (Algorithm 52)"}
{"DOCID": "778", "TEXT": "Associated Legendre Functions of the First Kind for Real or Imaginary Arguments (Algorithm 47)"}
{"DOCID": "779", "TEXT": "CROUT II (Algorithm 43)"}
{"DOCID": "780", "TEXT": "Algorithm 42 INVERT, Alg.107 Gauss's Method, Alg.120 Inversion II, and gjr"}
{"DOCID": "781", "TEXT": "Telescope 2 (Algorithm 38)"}
{"DOCID": "782", "TEXT": "Telescope 1 (Algorithm 37)"}
{"DOCID": "783", "TEXT": "Shellsort (Algorithm 201)"}
{"DOCID": "784", "TEXT": "Normal Random (Algorithm 200)"}
{"DOCID": "785", "TEXT": "Conversions Between Calendar Date And Julian day Number (Algorithm 199)"}
{"DOCID": "786", "TEXT": "Adaptive Integration and Multiple Integration (Algorithm 198)"}
{"DOCID": "787", "TEXT": "Matrix Division (Algorithm 197)"}
{"DOCID": "788", "TEXT": "Muller's Method for Finding Roots of an Arbitrary Function (Algorithm 196)"}
{"DOCID": "789", "TEXT": "Bandsolve (Algorithm 195)"}
{"DOCID": "790", "TEXT": "Zersol (Algorithm 194)"}
{"DOCID": "791", "TEXT": "Character Manipulation in 7090 Fortran"}
{"DOCID": "792", "TEXT": "Multiple-Precision Binary-To-Decimal Integer Conversion Using Only Addition And Subtraction"}
{"DOCID": "793", "TEXT": "Mapped List Structures"}
{"DOCID": "794", "TEXT": "A List-Type Storage Technique for Alphameric Information: A method which is economic in terms of space and time is proposed for the storage and manipulation of character strings of arbitrary length in a fixed word-length computer.  The method is illustrated in an application to Algol-type identifiers in an Algol-like block structure."}
{"DOCID": "795", "TEXT": "Debugging Systems at the Source Language Level"}
{"DOCID": "796", "TEXT": "SABRAG, A Time-Sharing Low-Cost Computer: The serial SABRAC computer designed and built in the Scientific Department of the Israel defense Ministry has a 5000-location magnetic drum, main store. To avoid a need to resort to optimum programming techniques and to increase its overall efficiency the computer has also been given a 224-word ferrite core store from which the program is obeyed.  Transfers between the core and drum stores and to and from the twin paper-tape input and output channels are all available autonomously (concurrently, time-shared). Multiplication and division orders are also autonomous, so that the machine may be executing up to three orders simultaneously.  All functions naturally are interlocked. A number of other advanced orders and facilities are also incorporated.In particular, an \"Execute\" order permits a temporary jump for up to four orders and a second modifier register permits double modification in general and relative addressing of subroutines in particular.  Thus the overall effective speed of the machine is muchhigher than its basic specification would lead one to expect and its design indicates one way in which the concepts of time sharing may be incorporated in \"low-cost\" computers."}
{"DOCID": "797", "TEXT": "American Standard Code for Information Interchange"}
{"DOCID": "798", "TEXT": "A Catalogue Entry Retrieval System"}
{"DOCID": "799", "TEXT": "Design of a Separable Transition-Diagram Compiler*: A COBOL compiler design is presented which is compact enough to permit rapid, one-pass compilation of a large subset of COBOL on a moderately large computer. Versions of the same compiler for smaller machines require only two working tapes plus a compiler tape.  The methods given are largely applicable to the construction of ALGOL compilers."}
{"DOCID": "800", "TEXT": "The Linking Segment Subprogram Language and Linking Loader"}
{"DOCID": "801", "TEXT": "Least Squares Solution with Constraints (Algorithm 177)"}
{"DOCID": "802", "TEXT": "SYMINV2 (Algorithm 150)"}
{"DOCID": "803", "TEXT": "Syminv2 (Algorithm 150)"}
{"DOCID": "804", "TEXT": "Exponentiation of Series (Algorithms 134)"}
{"DOCID": "805", "TEXT": "Newton Maehly (Algorithm 105)"}
{"DOCID": "806", "TEXT": "Remark on Certification of Matrix Inversion Procedures"}
{"DOCID": "807", "TEXT": "Reversion of Series (Algorithm 193)"}
{"DOCID": "808", "TEXT": "Confluent Hypergeometric (Algorithm 192)"}
{"DOCID": "809", "TEXT": "Hypergeometric (Algorithm 191)"}
{"DOCID": "810", "TEXT": "Complex Power (Algorithm 190)"}
{"DOCID": "811", "TEXT": "Smoothing 2 (Algorithm 189)"}
{"DOCID": "812", "TEXT": "Smoothing 1 (Algorithm 188)"}
{"DOCID": "813", "TEXT": "Differences and Derivatives (Algorithm 187)"}
{"DOCID": "814", "TEXT": "Complex Arithmetic (Algorithm 186)"}
{"DOCID": "815", "TEXT": "Normal Probability for Curve Fitting (Algorithm 185)"}
{"DOCID": "816", "TEXT": "Erlang Probability for Curve Fitting (Algorithm 184)"}
{"DOCID": "817", "TEXT": "Nexcom (Algorithm 152)"}
{"DOCID": "818", "TEXT": "Realizing Boolean Connectives on The IBM 1620"}
{"DOCID": "819", "TEXT": "Polynomial Evaluation Revised"}
{"DOCID": "820", "TEXT": "Checking for Loops in Networks"}
{"DOCID": "821", "TEXT": "Further Remarks on Sampling a Tape File-III"}
{"DOCID": "822", "TEXT": "Real-Time Programming Specifications: Problems in the implementation of large real-time applications are treated, and suggested guidelines for both program and file specifications are developed. The problems delineated also occur in systems programming."}
{"DOCID": "823", "TEXT": "A Syntactic Description of BC NELLIAC"}
{"DOCID": "824", "TEXT": "DESCRIPTRAN-Automated Descriptive Geometry*: Descriptive geometry consists of procedures originally designed to solve 3-space geometry problems by graphical constructions and measurement instead of by computation.  However, in addition to this it unifies and simplifies the approach to many such problems. When one can call subroutines that compute new coordinates that correspond to those obtainable from the graphical constructions, there is the three-way advantage of the approach of descriptive geometry, the accuracy of computation and the speed of the digital computer.  DESCRIPTRAN makes it possible to program many problems in 3-space with a few statements; it consists of 15 subroutines analogous to the procedures of descriptive geometry."}
{"DOCID": "825", "TEXT": "PIP: A Photo-Interpretive Program for the Analysis of Spark-Chamber Data*: An operating computer program that processes photographically recorded data is described. The input to the program consists of spark-chamber photographs on which tracks of high-energy particles are recorded.  The program automatically scans, measures and performs the preliminary interpretation of these photographs.  In continuous operation a processing rate of 5,000 photographic frames per hour is achieved."}
{"DOCID": "826", "TEXT": "Remarks on Fortran Subroutines for Time Series Analysis"}
{"DOCID": "827", "TEXT": "Disk File Sorting: Sorting techniques using an IBM 1401 with a random access storage device are evaluated."}
{"DOCID": "828", "TEXT": "Incompressible flow Network Calculations: A general method for the calculation of flows and pressures in fluid flow networks is presented. The method is applicable to computer use."}
{"DOCID": "829", "TEXT": "The External Language KLIPA For the URAL-2 Digital computer"}
{"DOCID": "830", "TEXT": "CORC-The Cornell Computing Language"}
{"DOCID": "831", "TEXT": "Real Error Function, ERF (Algorithm 123)"}
{"DOCID": "832", "TEXT": "Curve Fitting with Constraints (Algorithm 74)"}
{"DOCID": "833", "TEXT": "Reduction of a Symmetric Bandmatrix to Triple Diagonal Form"}
{"DOCID": "834", "TEXT": "Nonrecursive Adaptive Integration (Algorithm 182)"}
{"DOCID": "835", "TEXT": "Complementary Error Function-Large X (Algorithm 181)"}
{"DOCID": "836", "TEXT": "Error Function-Large X (Algorithm 180)"}
{"DOCID": "837", "TEXT": "Incomplete Beta Ratio (Algorithm 179)"}
{"DOCID": "838", "TEXT": "Direct Search (Algorithm 178)"}
{"DOCID": "839", "TEXT": "Least Squares Solution with Constraints (Algorithm 177)"}
{"DOCID": "840", "TEXT": "Least Squares Surface Fit (Algorithm 176)"}
{"DOCID": "841", "TEXT": "Shuttle Sort (Algorithm 175)"}
{"DOCID": "842", "TEXT": "A Posteriori Bounds on a Zero of a Polynomial (Algorithm 174)"}
{"DOCID": "843", "TEXT": "Assign (Algorithm 173)"}
{"DOCID": "844", "TEXT": "1410 Fortran Edit Feature"}
{"DOCID": "845", "TEXT": "Another Test Matrix for Determinants and Inverses"}
{"DOCID": "846", "TEXT": "Self-Inverse Conversion Table"}
{"DOCID": "847", "TEXT": "A Penny-Matching Program: The logic of a penny-matching program written for the CSX-1 is described."}
{"DOCID": "848", "TEXT": "A Note on Range Transformations for Square Root and Logarithm: There was the germ of an idea in two previous papers [1,2] which no one seems to have picked up in almost five years.  For certain functions it seems desirable to transform the argument to a short range symmetric about 10.1 will give examples of this usage for the square root and logarithm function for both binary and decimal machines."}
{"DOCID": "849", "TEXT": "Use of Tree Structures for Processing Files: In data processing problems, files are frequently used which must both be searched and altered. Binary search techniques are efficient for searching large files, but the associated file organization is not readily adapted to the file alterations.  Conversely, a chained file allocation permits efficient alteration but cannot be searched efficiently. A file organized into a tree-like structure is discussed, and it is shown that such a file may both be searched and altered with times proportional to slog(s)N, where N is the number of file items and s is a parameter of the tree.  It is also shown that optimizing the value of s leads to a search time which is only 25 per cent slower than the binary search.  The tree organization employs two data chains and may be considered to be a compromise between the organizations for the binary search and the chained file.  The relation of the tree organization to multidimensional indexing and to the trie structure is also discussed."}
{"DOCID": "850", "TEXT": "Conversion, Reconversion and Comparison Techniques In Variable-Length Sorting: The logic is described for converting highly variable input records into a format that can be easily and efficiently processed by a sorting program. The internal record formats are discussed in relation to (1) their conversion from input formats, (2) their reconversion to output formats, and (3) comparison techniques between internal formats."}
{"DOCID": "851", "TEXT": "Design and Characteristics of a Variable-Length Record Sort Using New Fixed-Length Record Sorting Techniques: This paper describes the application of several new techniques for sorting fixed-length records to the problems of variable-length record sorting. The techniques have been implemented on a Sylvania 9400 computer system with 32,000 fixed-length words of memory.  Specifically, the techniques sequence variable-length records of unrestricted size, produce long initial strings of data, merge strings of data at the power of T-1, where T is the number of work tapes in a system, and do not restrict the volume of input data."}
{"DOCID": "852", "TEXT": "A Method of Comparing the Time Requirements of Sorting Methods"}
{"DOCID": "853", "TEXT": "The COBOL Sort Verb"}
{"DOCID": "854", "TEXT": "Some Characteristics of Sorting in Computing Systems Using Random Access Storage Devices: The substantial differences in characteristics of random access storage and tape devices dictate that concepts and objectives of computer program design be considered from the viewpoint of the external file medium used.  This is particularly true in the case of sorting.  In a tape-oriented system, the major sorting problem is that of minimizing merge time despite the limited orders of merge possible. In contrast, sorting in a random access-oriented system encourages the selection of the optimum order of merge from many possible orders.  The latter problem is discussed in this paper, along with criteria developed for determining the optimum order of merge according to the various properties of random access storage devices.  Attention is also given to the problem of key sorting versus record sorting and the possibly serious disadvantage of key sorting on a random access system."}
{"DOCID": "855", "TEXT": "Organization and Structure of Dataon Disk File Memory Systems for Efficient Sorting and Other Data Processing Programs: An approach to the organization and structure of data on Bryant Disc File Memory Systems for sorting and performing other data processing functions is presented.  The following areas are covered: characteristics of Bryant Disc File Systems on the Bendix G-20 and RCA 301; two proposed \"chaining\" structures for data; and functions of a Disk File Executive Routine. The concepts for sorting and performing file maintenance processing using the proposed structure and executive routine are discussed.  Additionally, it is shown that sorting can be accomplished without the use of disk storage work areas."}
{"DOCID": "856", "TEXT": "Sorting with Large Volume, Random Access, Drum Storage: An approach to sorting records is described using random access drum memory.  The Sort program described is designed to be a generalized, self-generating sort, applicable to a variety of record statements. This description is divided into three parts.  The first part presents the operating environment; the second defines the general solution; the third part describes the internal sort-merge technique."}
{"DOCID": "857", "TEXT": "Sorting Nonredundant Files-Techniques Used in the FACT Compiler: Some typical file structures, including some called \"non-redundant,\" are examined,and the methods used in FACT to sort such files are discussed."}
{"DOCID": "858", "TEXT": "A Tape File Merge Pattern Generator: A routine is presented which specifies the sequence of merge cycles to effect the merging of sorted tape files.  The routine is designed to minimize elapsed computer time by varying the power of the merge cycles, so as to use all the available tape drives, with its characteristic of assigning one drive to a single-reel file and two drives to each multiple-reel file."}
{"DOCID": "859", "TEXT": "Computer Planned Collates"}
{"DOCID": "860", "TEXT": "A Comparison Between the Polyphase and Oscillating Sort Techniques: A comparison between the Oscillating and Polyphase Sort techniques is developed for computer systems having from four to ten tape drives.  The basis for the comparison is the total reading and writing required for various number of input strings and tape drives for the two techniques."}
{"DOCID": "861", "TEXT": "Read-Backward Polyphase Sorting: Read-backward Polyphase sorting provides more efficient use of the tapes available to a sort than most other sorting techniques.  Backward Polyphase produces a continuous merging process from n-1 tapes where n is the total number of tapes being used in the sorting process.  Any of the available presorting techniques may be used in conjunction with the Polyphase merge sort provided that the presort has the capability of producing both ascending and descending strings and distributing the strings on the various tapes as required by the Polyphase Merge."}
{"DOCID": "862", "TEXT": "String Distribution for the Polyphase Sort"}
{"DOCID": "863", "TEXT": "Multiphase Sorting"}
{"DOCID": "864", "TEXT": "An Empirical Study of Minimal Storage Sorting"}
{"DOCID": "865", "TEXT": "Internal and Tape Sorting Using the Replacement-Selection Technique: A general technique for sequencing unsorted records is presented.  The technique is shown to be applicable for the first stage of a generalized sort program (the formation of initial strings) as well as for sorting records within a memory storage (an internal sort).  It is shown that given N records in memory storage, records are sequenced using 1+log2 N tests per record, that initial string lengths will average 2N for random input records, and that reading, writing and processing can be accomplished simultaneously if the computer permits such overlap."}
{"DOCID": "866", "TEXT": "Sorting on Computers"}
{"DOCID": "867", "TEXT": "Least Squares Fitting of Planes to Surfaces Using Dynamic Programming: Dynamic programming has recently been used by Stone, by Bellman and by Gluss to determine the closet fit of broken line segments to a curve in an interval under the constraint that the number of segments is fixed.  In the present paper successive models are developed to extend the method to the fitting of broken plane segments to surfaces z=g(x,y) defined over certain types of subareas of the (x,y)-space. The first model considers a rectangular area, with the constraint that the plane segments are defined over a grid in the (x,y)-space.  It is then shown how this model may be incorporated into an algorithm that provides successive approximations to optimal fits for any type of closed area.  Finally, applications are briefly described."}
{"DOCID": "868", "TEXT": "A Suggested Method of Making Fuller Use of Strings in ALGOL 60"}
{"DOCID": "869", "TEXT": "Term of Magic Square (Algorithm 148)"}
{"DOCID": "870", "TEXT": "Term of Magic Square (Algorithm 148)"}
{"DOCID": "871", "TEXT": "PSIF (Algorithm 147)"}
{"DOCID": "872", "TEXT": "Adaptive Numerical Integration by Simpson's Rule (Algorithm 145)"}
{"DOCID": "873", "TEXT": "Random (Algorithm 133)"}
{"DOCID": "874", "TEXT": "Chebyshev Curvefit (Algorithm 91)"}
{"DOCID": "875", "TEXT": "Incomplete Elliptic Integrals (Algorithm 73)"}
{"DOCID": "876", "TEXT": "Complete Elliptic Integral (Algorithm 149)"}
{"DOCID": "877", "TEXT": "Complete Elliptic Integral of the First Kind (Algorithm 55)"}
{"DOCID": "878", "TEXT": "Reduction of a Matrix Containing Polynomial Elements (Algorithm 170)"}
{"DOCID": "879", "TEXT": "Newton Interpolation with Forward Divided Differences (Algorithm 169)"}
{"DOCID": "880", "TEXT": "Newton Interpolation with Backward Divided Differences"}
{"DOCID": "881", "TEXT": "Calculation of Confluent Divided Differences (Algorithm 167)"}
{"DOCID": "882", "TEXT": "Monte Carlo (Algorithm 166)"}
{"DOCID": "883", "TEXT": "Complete Elliptic Integrals (Algorithm 165)"}
{"DOCID": "884", "TEXT": "Orthogonal Polynomial Least Squares Surface Fit (Algorithm 164)"}
{"DOCID": "885", "TEXT": "Modified Hankel Function (Algorithm 163)"}
{"DOCID": "886", "TEXT": "XY-move Plotting (Algorithm 162)"}
{"DOCID": "887", "TEXT": "Combinatorial of M Things Taken One at a Time, Two at a Time, Up to N at a Time (Algorithm 161)"}
{"DOCID": "888", "TEXT": "Algorithm 160 Combinatorial of M Things Taken N at A Time"}
{"DOCID": "889", "TEXT": "Official Actions and Responses to ALGOL As a Programming Language"}
{"DOCID": "890", "TEXT": "Selected Definitions: A selection of the definitions prepared by the ACM Standards Committee's Subcommittee on Programming Terminology is presented for review by the ACM membership."}
{"DOCID": "891", "TEXT": "Everyman's Information Retrieval System: The information retrieval problem whose solution is presented here was posed by a technical library with limited bubget and personnel.  The solution, however, is quite general and is applicable to many different types of retrieval problems.  Further,the method of solution makes it possible for many groups who have previously dismissed an information retrieval program as expensive and difficult (from a programming stand-point) to reconsider their position, for the present solution makes it possible to install an information retrieval program in less than three months, and with relatively little equipment."}
{"DOCID": "892", "TEXT": "RECOL-A Retrieval Command Language: An interrogation scheme is described for the retrieval and manipulation of data file records. The language of the interrogation scheme allows for selecting file records with the are of logical condition statements, defining record classes, associating file records, editing printed output, and summarizing the results of the above operations.  Some examples of a typical file application and the more significant features of a particular machine implementation are given."}
{"DOCID": "893", "TEXT": "Significance Arithmetic on a Digital Computer: The 7090 at NYU has been modified to include a \"Significance Mode\" of operation which is intended to facilitate the identification of significant bits in the results of floating-point arithmetic operations. The manner in which floating-point arithmetic is handled in this mode is discussed.  Several numerical experiments using this mode are described and comparisons are made with the ordinary \"normalized mode.\" Examples include power series evaluation, linear equations solution, determinant evaluation and matrix inversion."}
{"DOCID": "894", "TEXT": "An Iterative Factorization Technique for Polynomials: An iterative technique is displayed whereby factors of arbitrary degree can be found for polynomials in one variable.  Convergence is shown to occur always if a certain Jacobian does not vanish and if the initial approximation to a factor is near enough to an actual factor.  The process is simply programmed, and preliminary results indicate it to be well adapted to use with digital computers.  For factors of degree two, the technique is similar to that of Bairstow, the present method being somewhat simpler."}
{"DOCID": "895", "TEXT": "A Computational Extension of the Variate Difference Method: Presented here is a computational extension of the variate difference method as developed by G. Tintner [1]."}
{"DOCID": "896", "TEXT": "Characteristic Values and Vectors of Defective Matrices"}
{"DOCID": "897", "TEXT": "Note on the Proof of the Non-existence of a Phrase Structure Grammar for ALGOL 60"}
{"DOCID": "898", "TEXT": "Random (Algorithm 133)"}
{"DOCID": "899", "TEXT": "Magic Square (Algorithm 117 & 118)"}
{"DOCID": "900", "TEXT": "Ancestor (Algorithm 79)"}
{"DOCID": "901", "TEXT": "Difference Expression Coefficients (Algorithm 79)"}
{"DOCID": "902", "TEXT": "Determinant (Algorithm 159)"}
{"DOCID": "903", "TEXT": "Exponentiation of Series (Algorithm 134 )"}
{"DOCID": "904", "TEXT": "Fourier Series Approximation (Algorithm 157)"}
{"DOCID": "905", "TEXT": "Algebra of Sets (Algorithm 156)"}
{"DOCID": "906", "TEXT": "Combination in any Order (Algorithm 155)"}
{"DOCID": "907", "TEXT": "Combination in Lexicographical Order (Algorithm 154)"}
{"DOCID": "908", "TEXT": "Test Matrix for Inversion"}
{"DOCID": "909", "TEXT": "Arithmetizing Declarations (Corrigendum)"}
{"DOCID": "910", "TEXT": "Selective Instruction Trap for the 7090"}
{"DOCID": "911", "TEXT": "A Variant Method of File Searching"}
{"DOCID": "912", "TEXT": "Addressing an Array Yi in k-Dimensions by Fortran for Analysis of Variance"}
{"DOCID": "913", "TEXT": "Neliac"}
{"DOCID": "914", "TEXT": "Jovial and Its Documentation"}
{"DOCID": "915", "TEXT": "Documentation of IPL-V"}
{"DOCID": "916", "TEXT": "FORTRAN"}
{"DOCID": "917", "TEXT": "COMIT"}
{"DOCID": "918", "TEXT": "COBOL"}
{"DOCID": "919", "TEXT": "Documentation Problems: ALGOL 60"}
{"DOCID": "920", "TEXT": "Toward Better Documentation of Programming Languages"}
{"DOCID": "921", "TEXT": "Incomplete Elliptic Integrals (Algorithm 73)"}
{"DOCID": "922", "TEXT": "Multint (Algorithm 32)"}
{"DOCID": "923", "TEXT": "Gomory (Algorithm 153)"}
{"DOCID": "924", "TEXT": "Nexcom (Algorithm 152)"}
{"DOCID": "925", "TEXT": "Location of a Vector in a Lexicographically Ordered ListAlgorithm 151)"}
{"DOCID": "926", "TEXT": "Syminv2 (Algorithm 150)"}
{"DOCID": "927", "TEXT": "Linear Programming Applied to Ultraviolet Absorption Spectroscopy"}
{"DOCID": "928", "TEXT": "Character Manipulation in FORTRAN"}
{"DOCID": "929", "TEXT": "Glossary Construction"}
{"DOCID": "930", "TEXT": "Decimal-to-Binary Conversion of Short Fields"}
{"DOCID": "931", "TEXT": "Systematic Mistake Analysis of Digital Computer Programs"}
{"DOCID": "932", "TEXT": "Matrix Inversion by Gauss-Jordan Inversion II (Algorithm 120)"}
{"DOCID": "933", "TEXT": "Magic Squares (Algorithm 117 & 118)"}
{"DOCID": "934", "TEXT": "Gauss's Method (Algorithm 107)"}
{"DOCID": "935", "TEXT": "Calculating Primes by Means of GPS (Algorithm)"}
{"DOCID": "936", "TEXT": "A Set of Test Matrices (Algorithm 52)"}
{"DOCID": "937", "TEXT": "Inverse of a Finite Segment of the Hilbert Matrix (Algorithm 50)"}
{"DOCID": "938", "TEXT": "Invert (Algorithm 42)"}
{"DOCID": "939", "TEXT": "Gamma Function (Algorithm 31)"}
{"DOCID": "940", "TEXT": "Generating Discrete Random Variables in a Computer: This note is concerned with details of how to instruct a computer to choose one from many things with assigned probabilities.  The method uses a uniform variable to direct the computer to a memory location; if this is done by a sequence of appropriately chosen conditional probabilities, efficient use of memory space and quite fast programs will result."}
{"DOCID": "941", "TEXT": "A Recursive Program for the General n-Dimensional Integral: A general program is outlined for n-dimensional integration with variable limits.  The program is of a recursive nature and uses Simpson's rule combined with repeated bisection to attain the required accuracy.  It was developed in the Ferranti Mercury Autocode Scheme."}
{"DOCID": "942", "TEXT": "FORTRAN Subroutines for Time Series Analysis: The authors have recently been concerned in a time-series study that constituted a fairly typical piece of applied statistical research, involving extensive computations on a moderately large quantity of data.  Wehave found that the many different numerical processes that were required could be built up almost completely from a small number of basic operations, and a set of FORTRAN subroutines has been written to perform these.  The main purpose of this note is to describe these subroutines, but since the question of general statistical programs is topical [1], we include some general remarks."}
{"DOCID": "943", "TEXT": "Terms Frequently Combined in Problem Description"}
{"DOCID": "944", "TEXT": "Storage and Search Properties of a Tree-Organized Memory System: A memory with list properties [1] may be used to construct numeric, alphabetic or alphanumeric trees.  Such trees have information storage and retrieval properties applicable to problems involving large quantities of data or to problems where the quantity, word length and distribution of stored information is not known a priori, or changes rapidly during the processing. The purpose of this paper is to examine the storage and search properties of a tree-organized storage system assuming that a memory possessing certain list properties is available.  Of prime interest is the application where a symbol table, dictionary or similar file is to be stored and searched."}
{"DOCID": "945", "TEXT": "Arithmetizing Declarations: An Application to COBOL"}
{"DOCID": "946", "TEXT": "Suggestions on ALGOL 60 (ROME) Issues - A Report by the American Standards Association Subcommittee X3.4.2"}
{"DOCID": "947", "TEXT": "Supplement to the ALGOL 60 Report"}
{"DOCID": "948", "TEXT": "Note on the Use of Procedures"}
{"DOCID": "949", "TEXT": "Integer and Signed Constants in ALGOL: A few remarks are given on the relations between syntax and semantics in the programming languages. The aim is to point out that, if it is true that the grammar of a context-free language should be conceived not only as a strings-generating device but also as a method for expressing a meaning, then the grammar of ALGOL is open to some criticism."}
{"DOCID": "950", "TEXT": "Parallel Methods for Integrating Ordinary Differential Equations: This paper is dedicated to the proposition that, in order to take full advantage for real-time computations of highly parallel computers as can be expected to be available in the near future, much of numerical analysis will have to be recast in a more \"parallel\" form.  By this is meant that serial algorithms ought to be replaced by algorithm which consist of several subtasks which can be computed without knowledge of the results of the other subtasks. As an example, a method is proposed for \"parallelizing\" the numerical integration of an ordinary differential equation, which process, by all standard methods, is entirely serial."}
{"DOCID": "951", "TEXT": "Rational Chebyshev Approximations to the Bessel Function Integrals Kis(x): The second Remes algorithm is used to approximate the integrals Kis by rational functions. The related coefficients for the approximations of Ki1, Ki2, Ki3 are given for different precisions."}
{"DOCID": "952", "TEXT": "Another use of FORTRAN II Chaining"}
{"DOCID": "953", "TEXT": "Scanning Text with a 1401"}
{"DOCID": "954", "TEXT": "A Note on the Calculation of Probabilities in an F-Distribution"}
{"DOCID": "955", "TEXT": "A Class of Matrices to Test Inversion Procedures"}
{"DOCID": "956", "TEXT": "A Family of Test Matrices"}
{"DOCID": "957", "TEXT": "Method for Partial Rewriting of Magnetic Tape"}
{"DOCID": "958", "TEXT": "A Case of too Much Precision"}
{"DOCID": "959", "TEXT": "Mark Sense and Port-A-Punch Programming Inputs"}
{"DOCID": "960", "TEXT": "Curve Fitting with Format Fortran"}
{"DOCID": "961", "TEXT": "Limited Bit Manipulation Using FORTRAN II: Techniques are developed for manipulating bits using only FORTRAN II.  These techniques allow individual bits to be tested, certain fields to be shifted, and numbers coded in BCD to be converted to Binary."}
{"DOCID": "962", "TEXT": "Double-Precision Squares Root for The CDC-3600: In January of 1960, the late Hans J. Maehly completed a summary of approximations to the elementary functions for the CDC-1604 computer.  The approximations and techniques suggested by Maehly are equally applicable to the second large computer in the CDC line, the 3600.  Unlike the 1604, however, the 3600 has built-in double-precision floating-point arithmetic. The present work, largely inspired by the successes of Maehly and his associates, concerns the extension of one of Maehly's ideas to a double-precision subroutine for the 3600."}
{"DOCID": "963", "TEXT": "Relative Effects of Central Processor and Input-Output Speeds Upon Throughput on the Large Computer: Presented in this paper is a technique for determining the relative effects of the internal speed of the computer and the speed of the input-output units upon the overall speed of the system. Equations are derived which permit the determination of these effects from hardware usage measurements."}
{"DOCID": "964", "TEXT": "Mechanization of Tedious Algebra-the e Coefficients of Theoretical Chemistry: A table of formulas for certain integrals involving Legendre functions has been constructed mechanically by a program which performed algebraic operations. The formulas are all rational algebraic expressions in a single variable and were constructed by a recurrence procedure.  They are of interest in molecular quantum chemistry.  Trivial coding techniques were used to write the relevant programs in FORTRAN.  The results were photo composed on a Photon S-560 system, that was controlled by tapes which were punched directly from the computer output, so avoiding manual keyboarding, transcription errors and keyboarded correction."}
{"DOCID": "965", "TEXT": "Greatest Common Divisor (Algorithm 237 [A1])"}
{"DOCID": "966", "TEXT": "Evaluation of Determinant (Algorithm 224 [F3])"}
{"DOCID": "967", "TEXT": "Complementary Error Function (Algorithm 181 [S15])"}
{"DOCID": "968", "TEXT": "Radical-Inverse Quasi-Random Point Sequence (Algorithm 247 [G5])"}
{"DOCID": "969", "TEXT": "Graycode (Algorithm 246 [Z])"}
{"DOCID": "970", "TEXT": "Treesort 3 (Algorithm [M1])"}
{"DOCID": "971", "TEXT": "Time Sharing in a Traffic Control Program: The Toronto traffic signal control system consists of a variety of logically distinct computer programs, all competing for machine time.  To satisfy these demands, a time-sharing program has been written whose purpose is to execute, in the order of a predefined priority, the various subprograms within the real-time system.  In this paper the more interesting aspects of the time-sharing program are outlined."}
{"DOCID": "972", "TEXT": "An Executive System Implemented as a Finite-State Automaton: The 473L command and control system used by the Air Force permits many operators to access large data files through the use of a computer.  The man-machine interface is satisfied by several communication consoles from which operators may enter queries and view replies.  A data link permits remote stations to send messages, status reports and inventories directly to the computer.  The information received over the on-line data link is used to update the data files which are stored on disk.  The 473L programming system is divided into an Executive Control Program and five components with different processing priorities. These priorities permit the system to be most sensitive to the console inputs and permit the operators at all the consoles to time share the central processor. The Executive Control Program provides for the orderly transitions of control among the programming system components. The major emphasis of the paper is on the technique of using the definition of a finite-state automaton for organizing the Executive Control Program."}
{"DOCID": "973", "TEXT": "Estimation of Heart Parameters Using Skin Potential Measurements: A fundamental problem of vector cardiography is the estimation of the state of the heart on the basis of skin potential measurements.  A mathematical model relating ventricular dipoles to surface potentials is sketched.  Then it is shown that the inverse problem-that of determining electrical heart parameters on the basis of skin potential measurements-may be viewed as a nonlinear multipoint boundary value problem.  A feasible solution, employing quasilinearization and high-speed digital computers, is given."}
{"DOCID": "974", "TEXT": "A Technique for Reading Gapless Tapes Makes Electrocardiograph Analysis Feasible on the IBM 7090: To study arrhythmias and higher frequency components of the electrocardiogram, long series of patient heart cycles must be examined before valid comparison of different heart beats can be made. A technique is presented for the automatic analysis of long series heart cycles via a digital computer."}
{"DOCID": "975", "TEXT": "The New Program of Work for the International Standard Vocabulary in Computers and Information Processing"}
{"DOCID": "976", "TEXT": "Fresnel Integrals (Algorithm 213 [S20])"}
{"DOCID": "977", "TEXT": "Conversions Between Calendar Date and Julian Day Number (Algorithm 199 [Z])"}
{"DOCID": "978", "TEXT": "Fresnel Integrals (Algorithm 244 [S20])"}
{"DOCID": "979", "TEXT": "Logarithm of a Complex Number (Algorithm 243 [B3])"}
{"DOCID": "980", "TEXT": "Multiple-Precision Arithmetic and the Exact Calculation of the 3-j, 6-j and 9-j Symbols: Described in this paper is a system of general-purpose multiple-precision fixed-point routines and their use in subroutines which calculate exactly the quantum-mechanical 3-j, 6-j and 9-j symbols of large arguments."}
{"DOCID": "981", "TEXT": "Rounding Problems in Commercial Data Processing: A common requirement in commercial data processing is that the sum of a set of numbers, rounded in a generally understood manner, be equal to the sum of the numbers rounded individually.  Four rounding procedures are described to accomplish this.  The particular procedure that is appropriate depends upon whether the numbers being accumulated can vary in sign, whether their sum can vary in sign, and whether the last number being summed can be recognized as such prior to its rounding."}
{"DOCID": "982", "TEXT": "An Inductive Approach to Language Translation: The possibility of natural language translation by means of fixed operations on example translations is considered.  The conception of sentence translation which motivates the work is informally presented, and the measurement of physical similarity in pairs of strings is discussed, a notion which plays a central role in the proposed type of translator.  Experimental evidence is presented in support of the premise upon which this conception is based."}
{"DOCID": "983", "TEXT": "Take-up reels for One-Inch Perforated Tape for Information Interchange (Proposed American Standard)"}
{"DOCID": "984", "TEXT": "Report on Input-Output Procedures for ALGOL 60 (IFIP)"}
{"DOCID": "985", "TEXT": "Report on SUBSET ALGOL 60 (IFIP)"}
{"DOCID": "986", "TEXT": "Proposed Amendment to Proposed American Standard on Specification for General-Purpose Paper Cards for Information Processing"}
{"DOCID": "987", "TEXT": "FORTRAN vs. Basic FORTRAN (A Programming Language for Information Processing on Automatic Data Processing Systems)"}
{"DOCID": "988", "TEXT": "History and Summary of FORTRAN Standardization Development for the ASA"}
{"DOCID": "989", "TEXT": "A Method of Syntax Specification"}
{"DOCID": "990", "TEXT": "Constraint-Type Statements in Programming Languages: A proposal is made for including in a programming language statements which imply relations between variables but which are not explicit assignment statements.  The compiler sets up a Newtonian iteration making use for the purpose of a routine for formal differentiation."}
{"DOCID": "991", "TEXT": "Gamma Function with Controller Accuracy (Algorithm 225 [S14])"}
{"DOCID": "992", "TEXT": "Gamma Function (Algorithm 221 [S14])"}
{"DOCID": "993", "TEXT": "Kutta Merson (Algorithm 218 [D2])"}
{"DOCID": "994", "TEXT": "Stringsort (Algorithm 207 [M1])"}
{"DOCID": "995", "TEXT": "Steep1 (Algorithm 203 [E4])"}
{"DOCID": "996", "TEXT": "Permutations of a Set with Repetitions (Algorithm 242 [G6])"}
{"DOCID": "997", "TEXT": "Patent Protection of Computer Programs"}
{"DOCID": "998", "TEXT": "Computer Programs are Patentable"}
{"DOCID": "999", "TEXT": "Joint Inventorship of Computers"}
{"DOCID": "1000", "TEXT": "Computer Patent Disclosures"}
{"DOCID": "1001", "TEXT": "Copyright Aspects of Computer Usage: This paper is concerned with the question of what constitutes infringement of a copyright on a book or other nondramatic literary work when the work is fed into a computer and is indexed, analyzed, partially reprinted, or otherwise utilized by the computer to produce eye-readable output.  The question of copyrightability of programs and infringement of copyrights on programs is also discussed.  The paper is directed primarily to a discussion of the present law.  Some aspects of the proposed new copyright law are also included.  General recommendations are made with respect to the proposed revision of the copyright law."}
{"DOCID": "1002", "TEXT": "A Rapid Method for Digital Filtering: Since much of the computer time spent in time-series analysis is used for multiplications, a minimum multiplication method was devised for digital filtering, with the expectation that it would be useful in the on line, real-time analysis of biological data.  The filters are constructed from a succession of readily analyzable components in a manner that facilitates cascading.  The repertoire of frequency response curves includes relatively good low-pass and band-pass designs.  Programs are available for implementing both the synthesis of these filters, and their application on computers whose assemblers allow the definition of recursive macros."}
{"DOCID": "1003", "TEXT": "A Computer Analysis Method For Thermal Diffusion in Biochemical Systems: In the thermal detection of rapid biochemical reactions it is necessary to correct the temperature data for transient heat conduction losses in a cylindrical calorimeter.  To handle the complexities arising from varying thermal-relaxation times of concentric insulating layers, a computer program was developed which gives the temperature distribution of the system as a function of radius and time.  This distribution is corrected at each step by a subroutine which calculates the instantaneous chemical state of there action, as well as the heat produced by this reaction. The program is based on a direct statement of Fourier's law of heat conduction and the chemical rate equation to provide a \"bookkeeping law\" to follow the reactants and the flow of heat packets, in such a way that the computer continually stores the heat distribution.  A computer analysis method is here regarded as one in which the physical laws of a process are used explicitly in the program. Usually this results in by passing much of the mathematical procedures conventionally used.  The program was tested against some known exact solutions of the heat equation and gave identical results, and compared well with experimental data of a known biochemical reaction. The construction of computer programs based on the direct statement of the physical laws is a principle of general applicability which has been applied to several other physical phenomena."}
{"DOCID": "1004", "TEXT": "Arctangent (Algorithm [B1])"}
{"DOCID": "1005", "TEXT": "Coordinates on an Ellipsoid (Algorithm 240 [Z])"}
{"DOCID": "1006", "TEXT": "A Storage Allocation and Reference Structure: A method is proposed and discussed which allows a subscripted-variable capability (in the FORTRAN sense) to be added to AUTOCODER-Type assembly systems."}
{"DOCID": "1007", "TEXT": "Extension of Existing Compilers By Sophisticated Use of Macros: A description is presented of an application in which macros and string concatenation were employed to add a new facility to BELFAP."}
{"DOCID": "1008", "TEXT": "Scheduling Meetings with a Computer: Computer scheduling of papers as it was developed for the 1960 meeting of the Federation of American Societies for Experimental Biology (FASEB) is described. The FASEB meeting is the largest scientific meeting held in the United States each year.  The technique developed for FASEB can be applied to schedule any meeting with parallel sessions."}
{"DOCID": "1009", "TEXT": "Solution of Combinatorial Problems Using Generating Functions on a Variable-Field Computer: The utility of generating functions in solving combinatorial problems is discussed.  Particular implementation results are presented and evaluated."}
{"DOCID": "1010", "TEXT": "A Multiuser Computation Facility for Education and Research: Present-day computing facilities are limited in their value for scientific research by inability to interact strongly with users.  The full power of a research computing instrument should be available at many terminals that give each user the ability to generate, correct and operate any procedure he wishes, either simple or complex.  Implementation is described for a small-scale multiuser computer system that permits several users to work independently with the machine, and to obtain satisfactory response using typewriter communication."}
{"DOCID": "1011", "TEXT": "Logarithm of a Complex Number (Algorithm 48 [B3])"}
{"DOCID": "1012", "TEXT": "Formal Parsing Systems: Automatic syntactic analysis has recently become important for both natural language data processing and syntax-directed compilers.  A formal parsing system G = (V,u,T,R) consists of two finite disjoint vocabularies, V and T, a many-many map, u, from V onto T, and a recursive set R of strings in T called syntactic sentence classes.  Every program for automatic syntactic analysis determines a formal parsing system.  A directed production analyzer (I,T,X,p) is a nondeterministic pushdown-store machine with internal vocabulary I, input vocabulary T, and all productions of p in the form:  (Z,a) -> aY1 ... Ym where  Z, Yi are elements of the set I and a is an element of the set T.  Every context-free language can be analyzed by a directed production analyzer.  The Kuno-Oettinger multiple-path syntactic analyzer for English is a concrete example of a directed production analyzer and of a working parsing algorithm.  The connection between structures assigned by the analyzer and those of a conventional phrase structure grammar is examined in this paper."}
{"DOCID": "1013", "TEXT": "Final Examination Scheduling: A method for scheduling final examinations to yield a minimal number of student conflicts is described.  The \"minimization\" is achieved by repetitively evaluating a nonlinear set of equations. Imbeded in the process is a random or Monte Carlo selection of assignments.  As in such heuristic techniques, the solution may not be optimum and many solutions may be found which yield locally minimal results. Computer programs are described and empirical results given."}
{"DOCID": "1014", "TEXT": "Machine Controls for Analysis of Variance: A major problem in using the analysis of variance, as the number of factors increases, is the exponential rise in the number of interactions.  Even though the experimenter may not be interested in these interactions it is impossible to ignore them in most experimental designs because of the problem of getting error terms.  It is natural therefore to look to the computer to handle the bulk of work involved in computing the interactions.  A program device to get the computer to do this is described."}
{"DOCID": "1015", "TEXT": "Near-Minimax Polynomial Approximations and Partitioning of Intervals: A method of near-minimax polynomial approximation is described.  As a by-product, this method provides a formula for an estimate of the maximum error associated with a given degree of approximation. Using this formula, a partitioning algorithm is obtained for dividing a basic interval into subintervals for which approximations of equal degree give equal maximum error."}
{"DOCID": "1016", "TEXT": "Interchangcable Perforated Tape Variable Block Formats for Positioning and Straight Cut (RS-273) and Contouring and Contouring/Positioning (RS-274) Numerically Controlled Machine Tools (Proposed American Standards)"}
{"DOCID": "1017", "TEXT": "Comments on Bit-Sequencing of the ASCII in Serial-by-Bit Data Transmission"}
{"DOCID": "1018", "TEXT": "Gauss (Algorithm 209 [S15])"}
{"DOCID": "1019", "TEXT": "XY move Plotting (Algorithm 162 [J6])"}
{"DOCID": "1020", "TEXT": "Free Field Read (Algorithm 239 [I5])"}
{"DOCID": "1021", "TEXT": "Conjugate Gradient Method (Algorithm 238 [F4])"}
{"DOCID": "1022", "TEXT": "Greatest Common Divisor (Algorithm 237 [A1])"}
{"DOCID": "1023", "TEXT": "Bessel Functions of the First Kind (Algorithm 236 [S17])"}
{"DOCID": "1024", "TEXT": "A Note on the Formation of Free List"}
{"DOCID": "1025", "TEXT": "A Method of Syntax-Checking ALGOL 60: A syntax checker was designed based on the syntax of ALGOL as described in the ALGOL 60 Report [Communications of the ACM, May, 1960].  Since the definition of the elements of the language is recursive it seemed most desirable to design the syntax checker as a set of mutually recursive processors tied together by subroutines which perform certain bookkeeping functions.  Because of the recursive nature of the language and of the syntax checker the problem of recovery after an error required much attention. A method was devised which permits most programs to be checked completely despite errors."}
{"DOCID": "1026", "TEXT": "Divide-and-Correct Methods for Multiple Precision Division: A division problem is defined and notation to relate it to the problem of multiple precision operation in a digital computer is introduced.  A basic divide-and-correct method for multiple precision division is formulated and its known properties briefly reviewed.  Of particular interest is the fact that the method produces at each step a set of precisely three estimates for the desired result, one of which is exact."}
{"DOCID": "1027", "TEXT": "An Alternate Checksum Method"}
{"DOCID": "1028", "TEXT": "Investigation of a New Analytical Method for Numerical Derivative Evaluation: A recently proposed analytical approach to numerical derivative evaluation is discussed.  The technique is shown to be both accurate and easy to apply, though certain indicated modifications are required.  Its use should greatly facilitate the writing and debugging of programs requiring derivatives of highly complex functions."}
{"DOCID": "1029", "TEXT": "A Simple Automatic Derivative Evaluation Program: A procedure for automatic evaluation of total/partial derivatives of arbitrary algebraic functions is presented.  The technique permits computation of numerical values of derivatives without developing analytical expressions for the derivatives.  The key to the method is the decomposition of the given function, by introduction of intermediate variables, into a series of elementary functional steps.  A library of elementary function subroutines is provided for the automatic evaluation and differentiation of these new variables.  The final step in this process produces the desired function's derivative. The main feature of this approach is its simplicity. It can be used as a quick-reaction tool where the derivation of analytical derivatives is laborious and also as a debugging tool for programs which contain derivatives."}
{"DOCID": "1030", "TEXT": "Techniques for the Simulation of Computer Logic: The simulation of a digital computer is an integral part of most computer design automation systems.  The evaluation of the Boolean functions which characterize the computer being simulated constitutes one major portion of a simulation system.  Four general procedural classes for evaluating these functions are defined.  Toward greatly increased efficiency of a simulation system, methods are presented for simultaneously evaluating many functions for one set of values of the variables,and for evaluating simultaneously one function for many sets of values for the variables."}
{"DOCID": "1031", "TEXT": "A Note on Starting the Newton-Raphson Method: Determination of a suitable initial estimate for a root of an equation f(x) = 0 by means of computing the roots of a sequence of related equations is described."}
{"DOCID": "1032", "TEXT": "Theoretical Considerations in Information Retrieval Systems: Information storage and retrieval systems are composed of three major components: (a) identification of information and tagging it for effective retrieval, (b) searching strategy, how to enter the file to circumvent the scanning of nonrelevant material, and (c) file organization to make access to information efficient.  For identification of information the paper suggests that a metalanguage (recently discussed in a paper by Goffman, Verhoeff and Belzer) associated with an object language be used.  For searching strategy, a linear model for an evaluation function of relevancy is developed which rewards the system for retrieving relevant documents and not retrieving the nonrelevant, and penalizes the system for the escaped relevant documents and false drops.  The inadequacies of a linear model are indicated.  Two approaches to file organization are discussed.  One is self-organization of the file based on its history and past performance, and the second is a self-generating subset of the file with a high probability of being relevant."}
{"DOCID": "1033", "TEXT": "Experimental Personalized Array Translator System: A system designed for intimate man-machine interaction in a general-purpose problem-solving environment is experimentally operational.  The system utilizers an array-oriented symbolic source language containing powerful statement types.  These include numeric, Boolean, relational and selection operators on operands which can be entire arrays.  The system also permits simple specification of test and argument arrays in single statements.  The completely symbolic operating system includes display and entry of program and data.  Sequence control is aided by an interrupt switch which allows the user to interact with the program during execution. In addition to normal stored program sequencing, the system provides trace options and the ability to enter any statement for immediate execution.  Present implementation of the system is with an interpretive translator on an IBM 1620 computer."}
{"DOCID": "1034", "TEXT": "Autosate: An automated data system analysis technique is described. The technique is designed to alleviate some of the principal problems that beset current analysis-large data workloads, long span of time between project inception and system operational date, the lack of explicit directions for conducting data system analysis and using the results, and the lack of a technique to control data system changes throughout its lifetime. The analysis is geared to determining workload, relationships and storage characteristics of documents in the information network automatically."}
{"DOCID": "1035", "TEXT": "Characteristics of the FORTRAN CEP Language: The FORTRAN CEP languages differs from FORTRAN II mainly because: (1) it extends the variety of the modes for real quantities; (2) it allows suitable mixtures, in an input/output list or in an expression, of quantities that occur under different modes; (3) it makes it possible to address a greater number of input/output equipment; and (4) it removes the restrictions on the complexity of the list of quantities to be transmitted between the magnetic core memory and the drum or the magnetic tape units."}
{"DOCID": "1036", "TEXT": "Remark on Further Generalization of ALGOL"}
{"DOCID": "1037", "TEXT": "Reduction of a Matrix Containing Polynomial Elements (Algorithm 170 [F3])"}
{"DOCID": "1038", "TEXT": "Crout with Equilibration and Iteration (Algorithm 135 [F4])"}
{"DOCID": "1039", "TEXT": "Summation of Fourier Series (Algorithm 128 [C6])"}
{"DOCID": "1040", "TEXT": "Romberg Integration(Algorithm 60 [D1])"}
{"DOCID": "1041", "TEXT": "Random Permutation (Algorithm 235 [G6])"}
{"DOCID": "1042", "TEXT": "Poisson-Charlier Polynomials (Algorithm 234 [S23])"}
{"DOCID": "1043", "TEXT": "Talk-A High-Level Source Language Debugging Technique With Real-Time Data Extraction: TALK, meaning Take A Look, is a debugging technique which aids substantially in debugging complex real-time programming systems by interrupting the users program at desired points to extract previously specified data.  The extracted data is later edited, listing the associated data with its high-level source language identification."}
{"DOCID": "1044", "TEXT": "An Automatic Loader for Subroutine Nests: A method for automatic loading of library subroutines, which can be adapted to operate in conjunction with any conventional two-pass assembler is described. The method is specifically designed to cope with a nested library structure."}
{"DOCID": "1045", "TEXT": "Programming Analysis of Variance by Sequences of Operators and Isomorphic Mappings: A special operator calculus developed by Hartley in 1956 together with a new mapping scheme has been found to be efficient in programming analysis of variance for multifactor experiments. The operator calculus and the mapping scheme are described in detail."}
{"DOCID": "1046", "TEXT": "A Compiler-Building System Developed by Brooker and Morris: In a number of articles published during the past two years, R. A. Brooker and D. Morris (joined by J.S. Rohl in their most recent paper have presented a very interesting programming system that they have developed for the Ferranti Atlas computer.  The present paper describes some of the major features of their system. it expands on some points that the original authors cover briefly, and treats only very lightly some topics to which they devote considerable space. The purpose of this paper is purely expository. Except in some very small details, and in some comments, it does not intentionally depart from or add to the material published in the listed references."}
{"DOCID": "1047", "TEXT": "Generation of Test Matrices by Similarity Transformations: A method for obtaining test matrices with a prescribed distribution of characteristic roots is given.  The process consists of using particularly simple similarity transformations to generate full matrices from canonical forms.  The matrices generated also have known characteristic vectors, inverses and determinants."}
{"DOCID": "1048", "TEXT": "Approximate Solution of Axially Symmetric Problems: A variety of physical problems in such diverse fields as electrostatic  field theory, heat and ideal fluid flow, and stress concentration theory reduce, under the assumption of axial symmetry, to the study of an elliptic partial differential equation. Dirichlet-type problems associated with this equation are studied on regions whose boundaries include a nondegenerate portion of the x-axis and exceedingly accurate numerical methods are given for approximating solutions."}
{"DOCID": "1049", "TEXT": "Numerical Solution of Nonlinear Two-Point Boundary Problems by Finite Difference Methods: Solution of nonlinear two-point boundary-value problems is often an extremely difficult task. Quite apart from questions of reality and uniqueness, there is no established numerical technique for this problem.  At present, shooting techniques are the easiest method of attacking these problems.  When these fail, the more difficult method of finite differences can often be used to obtain a solution. This paper gives examples and discusses the finite difference method for non-linear two-point boundary-value problems."}
{"DOCID": "1050", "TEXT": "A Parts Breakdown Technique Using List Structures: List structured parts breakdown is proposed and discussed.  Implementation facts are presented on operating program using these techniques."}
{"DOCID": "1051", "TEXT": "Multiword List Items: The list concept as originally proposed by Newell, Simon and Shaw specified single computer words as elements of a list.  This report describes the use of two or more consecutive words as one element. Such use results in a considerable saving in both the space required to hold a given amount of data, and in the execution time required to perform a given process on the data.  Following a brief description of standard list structures with single-word items, the multiword items are introduced.  Then variable-length items are described, along with the corresponding space-utilization problems.  Finally, several examples are given to illustrate the use of multiword lists. This paper attempts to draw together various recent papers which have applied some of these concepts in different ways, and indicate how they relate to the more general problems."}
{"DOCID": "1052", "TEXT": "Reducing Truncation Errors by Programming: In accumulating a sum such as in a numerical integration with a large number of intervals, the sum itself becomes much larger than the individual addends.  This may produce a less accurate sum as the number of intervals is increased.  Separate variables can be established as accumulators to hold partial sums within various distinct intervals.  Thus, the extensive successive truncations are eliminated."}
{"DOCID": "1053", "TEXT": "Design and Implementation of a General-Purpose Input Routine: A general-purpose input routine is discussed and advocated for FORTRAN.  The philosophy of such programs is examined and exemplified."}
{"DOCID": "1054", "TEXT": "Gauss-Seidel (Algorithm 220 )"}
{"DOCID": "1055", "TEXT": "q-Bessel Functions In(t) (Algorithm 214)"}
{"DOCID": "1056", "TEXT": "Shellsort (Algorithm 201)"}
{"DOCID": "1057", "TEXT": "Critical Path Scheduling (Algorithm 40)"}
{"DOCID": "1058", "TEXT": "Simpson's Rule for Multiple Integration (Algorithm 233)"}
{"DOCID": "1059", "TEXT": "Heapsort (Algorithm 232)"}
{"DOCID": "1060", "TEXT": "Matrix Inversion (Algorithm 231)"}
{"DOCID": "1061", "TEXT": "Matrix Permutation (Algorithm 230)"}
{"DOCID": "1062", "TEXT": "Symbol Manipulation in FORTRAN-SASP I Subroutines: A set of subroutines for use in FORTRAN are described whose purpose is to synthesize output strings from (i) input strings which have been analyzed by the SHADOW general syntactic analysis subroutine reported earlier, and/or (ii) packed BCD strings formed in any way.  Function-type subroutines are included for intermediate manipulations, which are performed on the strings which are stored in an abbreviated internal representation.  The automatic way in which an internal representation for each newly created substring is stored sequentially in a block of common storage, and the manner in which a storage block is dynamically allocated for that purpose, are discussed."}
{"DOCID": "1063", "TEXT": "One-Inch Perforated Paper Tape for Information Interchange (Proposed American Standard)"}
{"DOCID": "1064", "TEXT": "Perforated Tape Code for Information Interchange (Proposed American Standard)"}
{"DOCID": "1065", "TEXT": "Bit Sequencing of the American Standard Code for Information Interchange (ASCII) in Serial-by-Bit Data Transmission (Proposed American Standard)"}
{"DOCID": "1066", "TEXT": "Growing Applications of Linear Programming: Use of linear programming models has grown so extensively in recent years that the whole concept for organizing a computer code has undergone a radical change.  It no longer is adequate merely to reduce a mathematical algorithm (i.e. the simplex method) to a computer code.  An advanced code must cope with such a variety of situations that the respective computer subprograms must be organized into an integrated system.  Emphasis in this paper is devoted to the underlying principles upon which future linear programming systems must be based.  These viewpoints are influenced by the new demands that applications within the petroleum industry are placing on such systems.  Some of the components of such a system are: translation of problem statement in terms of basic data to linear programming matrix coefficients, data transmission for direct computer entry, data file at the computer center, data processing and editing prior to solving the simplex algorithm, an efficient and reliable code for solving the above-mentioned algorithm, and flexible means for summarizing the results."}
{"DOCID": "1067", "TEXT": "Picture Generation With a Standard Line Printer: A method is described for producing gray-toned pictures on a line printer by utilizing the different degrees of blackness of standard print characters. Gray scales with 17, 32 and 64 levels have been devised.  Scanned images of blood cells are used to display the technique."}
{"DOCID": "1068", "TEXT": "A FORTRAN II Load-Time-Saver"}
{"DOCID": "1069", "TEXT": "A Method for Comparing the Internal Operating Speeds of Computers"}
{"DOCID": "1070", "TEXT": "Expand, A System for Input Card Replication"}
{"DOCID": "1071", "TEXT": "Computer-Usage Accounting for Generalized Time-Sharing Systems: The current development of general time-sharing systems requires a revision of accounting procedures for computer usage. Since time-sharing system users operate concurrently, it is necessary to be more precise as to the amount of computer time and storage space that a user actually utilizes.  The various cost factors which should be considered for computer usage accounting in generalized time-sharing systems are discussed."}
{"DOCID": "1072", "TEXT": "An Improved Equivalence Algorithm: An algorithm for assigning storage on the basis of EQUIVALENCE, DIMENSION and COMMON declarations is presented.  The algorithm is based on a tree structure, and has reduced computation time by 40 percent over a previously published algorithm by identifying all equivalence classes with one scan of the EQUIVALENCE declarations.  The method is applicable in any problem in which it is necessary to identify equivalence classes, given the element pairs defining the equivalence relation."}
{"DOCID": "1073", "TEXT": "A Fast Procedure for Generating Exponential Random Variables: A very fast method for generating exponential random variables in a digital computer is outlined."}
{"DOCID": "1074", "TEXT": "Shanks (Algorithm 215)"}
{"DOCID": "1075", "TEXT": "Shuttlesort (Algorithm 175)"}
{"DOCID": "1076", "TEXT": "Multiple Integration (Algorithm 146)"}
{"DOCID": "1077", "TEXT": "Chebyshev Curve Fit (Algorithm 91)"}
{"DOCID": "1078", "TEXT": "Elementary Functions by Continued Fractions (Algorithm 229)"}
{"DOCID": "1079", "TEXT": "Q-Bessel Functions (Algorithm 228)"}
{"DOCID": "1080", "TEXT": "Chebyshev Polynomial Coefficients (Algorithm 227)"}
{"DOCID": "1081", "TEXT": "Normal Distribution Function (Algorithm 226)"}
{"DOCID": "1082", "TEXT": "Gamma Function with Controlled Accuracy (Algorithm 225)"}
{"DOCID": "1083", "TEXT": "An Experiment in a User-Oriented Computer System: A version of a software-hardware system for the purpose of facilitating the programming and analysis of well-formulated problems is described.  A modified Flexowriter is used to generate computer-acceptable input when equations or computable requests are typed in much the same manner as they would appear in conventional mathematical texts.  The typing and language rules are quite flexible and unrestrictive. While the compiler part is efficient, the system as a whole has much broader aspects as a tool for the study of problem solving and self-teaching systems."}
{"DOCID": "1084", "TEXT": "On Declaring Arbitrarily Coded Alphabets: The inability of existing programming languages to handle character strings from more than one or two alphabets is mentioned and a scheme for declaring additional alphabets is proposed.  The scheme provides for: many-to-one encodings, right or left justification, collating sequences different from numeric sequence, variations in character size (number of bits.) from alphabet to alphabet, and arbitrary source-language character representation."}
{"DOCID": "1085", "TEXT": "Specification for General-Purpose Paper Cards for Information Processing (Proposed American Standard)"}
{"DOCID": "1086", "TEXT": "A Proposal for Input-Output Conventions in ALGOL 60-A Report of the Subcommittee on ALGOL of the ACM Programming Language Committee"}
{"DOCID": "1087", "TEXT": "Problems in Automatic Abstracting: A variety of problems concerning the design and operation of an automatic abstracting system are discussed.  The purpose is to a general view of several major problem areas.  No attempt is made to discuss details or to indicate preferences among alternative solutions."}
{"DOCID": "1088", "TEXT": "Menu Planning by Computer: A computer code has been developed which plans menus by finding minimum cost combinations of menu items such that the daily dietary, gastronomic and production requirements can be satisfied for a sequence of days.  A fast, special integer programming algorithm is described which approximates the theoretical solution to the problem.  If necessary, any menu can be changed on-line and then post-optimized. Up to 30 percent saving on food cost is possible. A FORTRAN program for the IBM 1410 is available on request.  A considerable amount of data processing must precede the implementation of the system."}
{"DOCID": "1089", "TEXT": "Designing a Computer Center"}
{"DOCID": "1090", "TEXT": "Incomplete Beta Function Ratios (Algorithm 222)"}
{"DOCID": "1091", "TEXT": "Hypergeometric and Confluent Hypergeometric (Algorithm 191 & 192)"}
{"DOCID": "1092", "TEXT": "Nonrecursive Adaptive Integration (Algorithm 182)"}
{"DOCID": "1093", "TEXT": "Evaluation of Determinant (Algorithm 224)"}
{"DOCID": "1094", "TEXT": "Prime Twins (Algorithm 223)"}
{"DOCID": "1095", "TEXT": "Decimal Tables of Binary Coded Tables"}
{"DOCID": "1096", "TEXT": "On Avoiding Matrix Reversals Between 7090 FORTRAN II and 7090 FORTRAN IV"}
{"DOCID": "1097", "TEXT": "An Algorithm for Converting Integers from Base A to Base B: A little known, simple algorithm for integer conversion between number systems is presented and proved."}
{"DOCID": "1098", "TEXT": "A Comparison of List-Processing Computer Languages (Including a Detailed Comparison of COMIT, IPL-V, LISP 1.5, and SLIP): A detailed comparison is presented of COMIT, IPL-V, LISP 1.5 and SLIP - four well-known computer programming languages which, among them, exhibit all the principal characteristics of existing list-processing languages.  Important common features of list-processing languages are reviewed: forms of data structures which are manipulated, necessity for dynamic allocation of storage, use of pushdown stores, and use of recursive operations.  Principal differences between the four languages under consideration are detailed: representations of data, both by the programmer and within the machine; methods for storage allocation; programming formalisms and special processes available, including arithmetic facilities; and usability in terms of availability, documentation, learning aids and debugging facilities.  A rough comparison shows that all the languages discussed have approximately the same speed.  Finally, the authors give some heuristics to aid in the selection of one of these languages for use in particular problem applications, concluding that no one of the languages considered is distinctly superior in all possible list-processing applications."}
{"DOCID": "1099", "TEXT": "Professional Computer Work for the Blind: Developments in computer technology have opened new professional opportunities for the intelligent blind.  Since there are few if any occupations in which the blind can participate without serious disadvantage, the opportunities offered them to gain entrance into various occupations through computer use including that of programmer, is important for future rehabilitation planning. Also of immediate interest is the fact that the blind may be especially suited for programming work.  Because of intense training in and constant experience with locating objects in the unseen environment and also because of superbly trained memory, the blind brings to the work of programming skills which the sighted has had little need to acquire. These qualifications should result in fewer debugging problems and make the blind a valuable addition to any systems group.  Before the blind could become a serious professional, a number of aids and techniques had to be developed that can mediate between machines and programmer.  This paper describes the techniques and aids which were designed by the staff of the Medical Computing Center of the University of Cincinnati College of Medicine."}
{"DOCID": "1100", "TEXT": "Status of Computer Sciences Curricula in Colleges and Universities"}
{"DOCID": "1101", "TEXT": "The Place of Logical Design and Switching Theory In The Computer Curriculum"}
{"DOCID": "1102", "TEXT": "Mechanical Languages: A Course Specification"}
{"DOCID": "1103", "TEXT": "Logic for the Computer Sciences"}
{"DOCID": "1104", "TEXT": "An Undergraduate Curriculum in Numerical Analysis"}
{"DOCID": "1105", "TEXT": "On Introducing Digital Computing"}
{"DOCID": "1106", "TEXT": "Programming of Digital Computers"}
{"DOCID": "1107", "TEXT": "Computers and Education"}
{"DOCID": "1108", "TEXT": "Digital Data Processor for Tracking the Partially Illuminated Moon*: A study of lunar tracking techniques and fabrication of a breadboard to assess the feasibility of the best technique selected was conducted to define a tracking system for observation of the sight line to the center of a partially illuminated moon.  The data processing portion of the system is presented in detail and then described in general are the operation of the tracker head assembly for data readout, the operation of the entire system and the effect data processing considerations have on the design of the tracker system.  The system basically consists of an optical sensor, digital computer and tracker drive mechanism.  The three system units, connected in cascade, comprise the control loop.  For this application, an optical telescope with a radial mechanical scanning mechanism was used that read out lunar sight line measurement information.  This information is sequentially read into a special purpose digital computer that extracts the measurements and computes the error signals that drive the tracker to the appropriate attitude."}
{"DOCID": "1109", "TEXT": "Conversion of a Power to a Series of Chebyshev Polynomials*: Even slowly convergent power series can be rearranged as series in Chebyshev polynomials if appropriate sequence transformations are used in evaluating the coefficients.  The method is illustrated by computing the coefficients for the expansion of the logarithm and dilogarithm."}
{"DOCID": "1110", "TEXT": "A Fourier Series Method for the Numerical Solution of a Class of Parabolic Partial Differential Equations*: A Fourier series method is described which, when applied to a certain class of parabolic partial differential equations, reduces the problem to a system of ordinary differential equations.  An application is given for which the method shows a considerable advantage over conventional finite difference methods."}
{"DOCID": "1111", "TEXT": "A Class of Iterative Techniques For the Factorization of Polynomials*: A method of iteration is developed in terms of a function of somewhat arbitrary character. Sufficient conditions are given for convergence of the process, yielding factors of arbitrary degree for polynomials in one variable.  Both Lin's method and Newton's method occur as special cases."}
{"DOCID": "1112", "TEXT": "A Technique for Computer Detection and Correction of Spelling Errors*: The method described assumes that a word which cannot be found in a dictionary has at most one error, which might be a wrong, missing or extra letter or a single transposition.  The unidentified input word is compared to the dictionary again, testing each time to see if the words match-assuming one of these errors occurred.  During a test run on garbled text, correct identifications were made for over 95 percent of these error types."}
{"DOCID": "1113", "TEXT": "Computer-Made Perspective Movies as a Scientific and Communication Tool*: It is easy to program the basic transformation required for a perspective drawing.  This fact plus the advent of high speed microfilm printers such as the General Dynamics Electronics S-C 4020 makes possible perspective movies as the direct output from a computer.  The programming of such a movie is briefly described for studying the angular motions of a satellite containing an attitude control system. In the movie, a domino-shaped box represents the satellite and a sphere with circles of latitude and longitude represents the earth.  The cost was approximately three to eight minutes of IBM 7090 time per one minute of movie."}
{"DOCID": "1114", "TEXT": "Generating a Canonical Prefix Encoding*: Computer programs for generating a minimum-redundancy exhaustive prefix encoding are described. One program generates a Huffman frequency tree, another determines the structure functions of an encoding, and a third program assigns codes."}
{"DOCID": "1115", "TEXT": "Randomized Binary Searching With Tree Structure: A more efficient method of using tree structures is proposed, which utilizers both plus and minus branches in the search path.  Very significant gains result when the search key includes alphabetic characters."}
{"DOCID": "1116", "TEXT": "Tests on a Computer Method for Constructing School Timetables*: A previously proposed computer method for constructing timetables, based on an iteration involving Boolean matrices, is described.  In limited tests the method has successfully produced timetables on every trial.  References are given which relate the timetable problem to theorems on matrices of zeros and ones, and to theorems on bipartite graphs.  Some problems of applying the method to constructing timetables in real situations are noted."}
{"DOCID": "1117", "TEXT": "Polyphase Sorting With Overlapped Rewind*: A variation of the polyphase merge technique of sorting is described which permits one tape at a time to be rewound while the merge is continued on the remaining tapes.  The result is the overlapping of a major portion of the rewind time.  The technique should be considered whenever a sort is written to operate on five or more tapes that cannot be read backwards. The savings of the overlap method appear to increase as the number of available tapes is increased."}
{"DOCID": "1118", "TEXT": "FORTRAN Subroutines for Time Series Data Reduction*"}
{"DOCID": "1119", "TEXT": "An Open Letter to X3.4.3 (FORTRAN Standards -- American Association)"}
{"DOCID": "1120", "TEXT": "\"ALCOR Group Representations of ALGOL Symbols,\" Comm. ACM 6 (1963), 597-599. (Corrigenda)"}
{"DOCID": "1121", "TEXT": "Comments on \"A Continued Operation Notation\"*: This note is intended to clarify and correct several points in a recent paper describing some notations for symbol manipulation by M.P. Barnett [Comm. ACM 6(August, 1963)]."}
{"DOCID": "1122", "TEXT": "A Note on Some Compiling Algorithms: Two compiling generators for arithmetic expressions are discussed: one presently in use in an experimental compiler, and an improvement suggested by K. Speierman of Burroughs."}
{"DOCID": "1123", "TEXT": "Gauss (Algorithm 209)"}
{"DOCID": "1124", "TEXT": "Matrix Division (Algorithm 197)"}
{"DOCID": "1125", "TEXT": "Syminv2 (Algorithm 150)"}
{"DOCID": "1126", "TEXT": "ERF (Algorithm 123)"}
{"DOCID": "1127", "TEXT": "Tridiagonal Matrix (Algorithm 122)"}
{"DOCID": "1128", "TEXT": "Evaluation of Determinant (Algorithm 41)"}
{"DOCID": "1129", "TEXT": "Incomplete Beta Function Ratios (Algorithm 222)"}
{"DOCID": "1130", "TEXT": "Gamma Function (Althm 221)"}
{"DOCID": "1131", "TEXT": "On Context and Ambiguity in Parsing*"}
{"DOCID": "1132", "TEXT": "An Extension to ALGOL for Manipulating Formulae*"}
{"DOCID": "1133", "TEXT": "A Programming Package for Some General Modes of Arithmetic*"}
{"DOCID": "1134", "TEXT": "Some Effects of the 6600 Computer on Language Structures*: The problem of compiling efficient 6600 codes prompted the development of an intermediate language reflecting the structure of the machine, that is more easily manipulated in improving object program efficiency.  The subject of this paper is the intermediate language and methods of manipulating it. Compilations of a series of arithmetic statements are discussed.  It is assumed that all functions and exponentials have been removed from these statements, and replaced by simple variables.  For purposes of simplicity the treatment of subscripts is ignored. A simplified 6600 structure is presented to illustrate the compiling method.  Several assumptions are made for purposes of simplification, although there are cases in which the assumptions are violated in the actual machine."}
{"DOCID": "1135", "TEXT": "A General Business-Oriented Language Based on Decision Expressions*: The structure of a digital compute programming language which covers a wide class of business and file processing applications is presented.  Such a structure, based on identifying and incorporating into a compiler the aspects common to all processes of such class, permits writing extremely compact programs, even for comparatively complex applications, in terms of tables of control expressions which express only information characteristic of the particular application.  Furthermore,local changes of a process (e.g. changes affecting only one of the output files involved) can be effected by local modifications in the program (e.g. modification of only one entry of the tables).  This structure also allows for inexpensive preparation of loading-speed compilers which translate the source programs into efficient machine codes. The approach adopted here departs from conventional mechanical language design philosophies.  It stresses the structural analysis of the class of processes to be represented in the languages, as opposed to emphasizing formal (i.e., contents-independent) syntactical definitions. It relies exclusively on nonprocedural representation of process as sets (tables) of relations between data and results (there are no control statements such as GO TO, etc.), instead of using procedure descriptions (which are one-to-one translations of flowcharts).  Here an invariant pattern of procedure is identified as characteristic of the class of all batch file processes.  This new philosophy has the potential to overcome well-known deficiencies of other business-oriented languages and fully meets the requirements set by CODASYL for such languages, including machine-independence."}
{"DOCID": "1136", "TEXT": "Beginnings of a Theory of Information Handling*"}
{"DOCID": "1137", "TEXT": "A Format Language*"}
{"DOCID": "1138", "TEXT": "Formalism in Programming Languages*"}
{"DOCID": "1139", "TEXT": "FORTRAN IV as a Syntax Language*"}
{"DOCID": "1140", "TEXT": "\"Structural Connections\" in Formal Language*"}
{"DOCID": "1141", "TEXT": "Bounded Context Syntactic Analysis"}
{"DOCID": "1142", "TEXT": "An Extension of ALGOL-Like Languages"}
{"DOCID": "1143", "TEXT": "Analysis of Decay-Type Data*: A comparative study has been made of a variety of numerical techniques for fitting experimental data of the decay type by forms involving the sums of exponentials.  Statistical errors of the fitted parameters are also calculated.  These methods have been applied to artificially-generated sets of data as well as to the results of experiments with radioactive tracers on both human and animal subjects. Results show that the values of the fitted parameters are very sensitive to variations in the fitting procedure.  Therefore great care very sensitive to variations in the fitting procedure.  Therefore great care must be exercised in identifying such values with physical constants.  Although the values of functions derived from these fitted parameters which can definitely be associated with physical entities are generally more stable under variations in the fitting techniques, error bounds can be so large that no great confidence can be placed even in them.  It would therefore appear best to select a uniform technique both for running the experiments and for analyzing the data, and then to consider as significant only relative results between one subject and the next."}
{"DOCID": "1144", "TEXT": "Digital Computer Determination of Alpha Source Activity: A technique is described for determining the activity and homogeneity of an alpha source. It is believed that the technique, using a digital computer, has many uses and applications in the field of nuclear physics. The technique involves computer manipulation of the digital image of the nuclear source.  Experimental details are given."}
{"DOCID": "1145", "TEXT": "GIT-A Heuristic Program for Testing Pairs of Directed Line Graphs for Isomorphism*: Given a pair of directed line graphs, the problem of ascertaining whether or not they are isomorphic is one for which no efficient algorithmic solution is known. Since a straightforward enumerative algorithm might require 40 years of running time on a very high speed computer in order to compare two 15-node graphs, a more sophisticated approach seems called for.  The situation is similar to that prevailing in areas such as game-playing and theorem-proving, where practical algorithms are unknown (for the interesting cases), but where various practical though only partially successful techniques are available.  Git-Graph Isomorphism Tester-incorporates a variety of processes that attempt to narrow down the search for an isomorphism, or to demonstrate that none exists.  No one scheme is relied upon exclusively for a solution, and the program is designed to avoid excessive computation along fruitless lines.  GIT has been written in the COMIT language and successfully tested on the IBM 7090."}
{"DOCID": "1146", "TEXT": "An Efficient Composite Formula for Multidimensional Quadrature: A (2s+1)-point second-degree quadrature formula for integration over an s-dimensional hyper-rectangle is presented.  All but one of the points lie on the surface with weights of opposite sign attached to points on opposite faces.  When a large volume is subdivided into congruent rectangular subdivisions, only one point is required in each interior subdivision to achieve second-degree accuracy."}
{"DOCID": "1147", "TEXT": "On the Numerical Solution of Boundary Value Problems for Linear Ordinary Differential Equations*: A numerical method is presented for the solution of boundary value problems involving linear ordinary differential equations.  The method described is noniterative and makes use of any one-step numerical integration scheme to reduce the problem from one of boundary values to one of initial values. Comments are made concerning some numerical results of applying the method to a specific problem.  In addition an extension of the algorithm described to more general problems is discussed."}
{"DOCID": "1148", "TEXT": "An Example in \"Significant-Digit\" Arithmetic*: Different methods of handling the summing process for the geometric series are shown to give results indicating widely differing significances when carried out in a machine incorporating \"significant-digit\" arithmetic."}
{"DOCID": "1149", "TEXT": "GARGOYLE , A Language for Compiler Writing*"}
{"DOCID": "1150", "TEXT": "A Fortran Post-Mortem Procedure"}
{"DOCID": "1151", "TEXT": "A Note on Multiplying Boolean Matrices II"}
{"DOCID": "1152", "TEXT": "Floating-Point Arithmetic with 84-Bit Numbers: A classic and straightforward technique is presented which is not limited to the size or type of number representation used or multiple precision arithmetic."}
{"DOCID": "1153", "TEXT": "A Fast Procedure for Generating Normal Random Variables*: A technique for generating normally distributed random numbers is described.  It is faster than those currently in general use and is readily applicable to both binary and decimal computers."}
{"DOCID": "1154", "TEXT": "Multi-Tape and Infinite-State Automata -- A Survey: A survey of machines which are more powerful than finite automata and less powerful than general Turing machines is presented.  It is felt that the machines in this category are as closely related to digital computers as either the finite automata or the unrestricted Turing machines.  Intermediate machines can be created by adjoining on infinite-state memory to a finite-state machine and then performing any or all of the following: (1) restrict the manner in which the unbounded portion of the memory can be accessed, (2) bound the number of steps allowed for a computation by some increasing recursive function of the length of the input, (3) restrict the total amount of memory available in the same manner.  Examples from all three classes and their properties are discussed."}
{"DOCID": "1155", "TEXT": "Experiments with a Deductive Question-Answering Program: As an investigation in artificial intelligence, computer experiments on deductive question-answering were run with a LISP program called DEDUCOM, an acronym for DEDUctive COMmunicator. When given 68 facts, DEDUCOM answered 10 questions answerable from the facts. A fact tells DEDUCOM either some specific information or a method of answering a general kind of question. Some conclusions drawn in the article are: (1) DEDUCOM can answer a wide variety of questions.  (2) A human can increase the deductive power of DEDUCOM by telling it more facts.  (3) DEDUCOM can write very simple programs (it is hoped that this ability is the forerunner of an ability to self-program, which is a way to learn).  (4)DEDUCOM's search procedure at present has two bad defects: some questions answerable from the given facts cannot be answered and some other answerable questions can be answered only if the relevant facts are given in the \"right\" order. (6) At present, DEDUCOM's method of making logical deductions in predicate calculus has two bad defects: some facts have to be changed to logically equivalent ones before being given to DEDUCOM, and some redundant facts have to be given to DEDUCOM."}
{"DOCID": "1156", "TEXT": "Hankel Function (Algorithm 124 [S17])"}
{"DOCID": "1157", "TEXT": "Procedure for the Normal Distribution Functions (Algorithm 272 [S15])"}
{"DOCID": "1158", "TEXT": "Program Structures for Parallel Processing: Constructs for organizing and explicating parallel program segments are discussed as extensions to ALGOL 60.  The constructs serve as meta-commands and are motivated by equipment having multiprocessing capability."}
{"DOCID": "1159", "TEXT": "Machine Independence: Its Technology and Economics: A survey is offered of techniques for transferring programs, and especially compilers, from one computer to another.  Of the methods examined, the \"bootstrap\" technique is singled out for detailed discussion, with emphasis on its economics. The considerations that determine the applicability of bootstrapping in any specific case are discussed, and an attempt is made to assign appropriate qualitative weights to them.  Finally, reasons are given for believing that the machine-independence problem is being substantially diminished by current trends in computer design, and that it is this process of convergence in hardware design rather than any foreseeable software developments that will lead to its satisfactory resolution."}
{"DOCID": "1160", "TEXT": "CAT: A 7090-3600 Computer-Aided Translation: A semi-automatic translation system has been implemented which converts 7090 FAP language programs into 3600 assembly language.  The input to the system is a FAP program deck which has been specially prepared for translation by the user.  The output consists of the translated COMPASS language program together with a comprehensive diagnostic listing which the user must analyze in order to verify any questionable areas of the translation.  The translation processor consists of three distinct phases: an assembly of the FAP program, a comprehensive analysis of the assembled code with particular regard to the actions of instructions upon other instructions and upon data, and finally the output pass which generates the COMPASS program in the form of macro instructions."}
{"DOCID": "1161", "TEXT": "1401 Compatibility Feature on the IBM System/360 Model 30: The \"second generation\" of stored-program computers, of which IBM 1400 series was a part, brought EDP into the mass market for the first time on a large scale.  As this era unfolded, rapid changes in technology led to rapid obsolescence of data processing equipment.  Program written for a particular system required tedious conversion as incompatible new machines came into use.  The IBM System/360 has been designed with the conversion problem specifically in mind.  One of the conversion aids available on the Model 30 is the 1401 compatibility feature.  This feature, in conjunction with other aids, permits a smooth and inexpensive transition to optimum use of the new system."}
{"DOCID": "1162", "TEXT": "An Assembly Language for Reprogramming: Complete reprogramming of compiler language programs is seldom necessary.  It is assembly language programs which present the greatest difficulty.  Assembly languages generally provide a one-for-one translation from a symbolic to a numeric version of a program, that is, from assembly language to machine language. The meta-language presented here can be used to specify the mapping of any language which conforms to a canonical list form into an arbitrary stream of bits. This bit stream may be treated as a machine language program, a character stream, or whatever else the user might desire.  Thus, this meta-language can be used to map from one assembly language into another or from the assembly language for one machine into the machine language of another."}
{"DOCID": "1163", "TEXT": "Philco/IBM Translation at Problem-Oriented, Symbolic and Binary Levels: A translation system has been developed to eliminate most of the effort formerly required to reprogram Philco 2000 series codes for IBM 7094 operation. Experience with this system is limited but highly successful encouraging application of the techniques to other source and object languages."}
{"DOCID": "1164", "TEXT": "Emulation of Large Systems: The conversion problem and a new technique called emulation are discussed.  The technique of emulation is developed and includes sections on both the Central Processing Unit (CPU) and the Input/Output unit (I/O).  This general treatment is followed by three sections that describe in greater detail the implementation of compatibility features using the emulation techniques for the IBM 7074, 7080 and 7090 systems on IBM System/360."}
{"DOCID": "1165", "TEXT": "The Spectra 70/45 Emulator for the RCA 301: The RCA 301 Emulator System is supplied with the Spectra 70/45 as a reprogramming aid.  It allows an RCA 301 object program to be run on the Spectra 70/45 without necessitating changes in the RCA 301 object code.  Execution rates are considerably better than traditional simulation.  The Emulator provides an increase in throughput capacity for the 301 user on the Spectra 70/45.  The Emulator makes use of both hardware micro-program routines and software routines to accomplish its function."}
{"DOCID": "1166", "TEXT": "A Use of Macros in Translation of Symbolic Assembly Language of One Computer to Another: A set of macro-operations has been prepared to assist in translating IBM 7090 symbolic assembly language programs to IBM 7040 machine language programs. This set, inserted at the beginning of the 7090 symbolic deck, treats incompatible instruction mnemonics as macro-instructions to produce equivalent 7040 instruction sets.  Incompatible instructions are categorized into basic operational classes which can be expressed by a single basic skeleton.  Several levels of macro calls are required to supply arguments to the basic skeleton for each particular instruction. Modification at execution time of the address or tag of an incompatible instruction requires incorporation of an address-tag equivalent.  I/O is handled by generating calls to I/O simulation subroutines."}
{"DOCID": "1167", "TEXT": "On the Translation of Machine Language Programs: Automatic translation of machine language programs is becoming a highly desirable goal with the advent of new large-scale computers.  The pitfalls that make it difficult to achieve completely automatic translations are analyzed, and it is shown that these are primarily of a semantic nature.  A semi-automatic procedure for resolving semantic problems is suggested."}
{"DOCID": "1168", "TEXT": "Across Machine Lines in COBOL: The production of a large, file-maintenance-and-retrieval program system written in COBOL is described.  The COBOL language was used specifically to enable the system to operate on three IBM computers."}
{"DOCID": "1169", "TEXT": "An Algorithm for Minimizing Backboard Wiring Functions: A partially exhaustive algorithm is presented for solving the following problem arising from automatic layout of a computer.  Given an ordered set E1, E2,..., EN of N computer components, for each permutation of the elements E1, E2.., EN, there is attached a value of an integer function F.  The algorithm finds a local minimum of F by evaluating the set {Delta F} of the increments corresponding to a certain set of exchanges of two elements.Then the exchange corresponding to the least negative increment of {Delta F} is performed.  The process is iterated and stopped when the set of the increments is a positive or empty set, which, it is proved, corresponds to a minimum.  The procedure is similar to the Downhill Method for finding the minimum of a real function F(P), and can be applied to other placement problems. Experimental results are presented with backboards formed by many elements and different initial placements."}
{"DOCID": "1170", "TEXT": "Analyzing English Syntax with a Pattern-Learning Parser: A dependency analysis system based on pattern recognition and learning logic was developed to infer word classes and rules of syntactic combination from experience with text which had been analyzed. The characteristics used to form word classes are the depth in the dependency tree of each word, the direction of its governor and the same features for each of its immediate neighbors. Syntactic rules of combination show the relation of a word to its governor in the depth pattern of the sentence.  The system was tested on 400 elementary basic English sentences including 300 used earlier by Knowlton in a different learning parser of all 400 sentences.  After experience with 300 sentences it was able to generalize with 77 percent accuracy to the next 100. In accumulative learning trials after the first 200 sentences it averaged a probability of .9 for accurately parsing each new sentence it encountered. It was concluded that the system is adequate for learning to parse the bulk of basic English but that further development is required before conclusions about its application to ordinary English can be stored. The system is operational and available on the ARPA/SDC time-shared computing system."}
{"DOCID": "1171", "TEXT": "A Comparison of the Primal-Simplex and Primal-Dual Algorithms for Linear Programming: A statistical comparison of the primal-dual and the more commonly used primal-simplex algorithm for solving linear programming problems has been made under the assumption of starting with a full artificial basis.  Under these conditions the primal-dual method shows a statistically significant superiority on randomly generated problems.  It has also been found, via a regression analysis, that the relevant parameters in determining the difference in the number of iterations between the algorithms is not only the number of constraints and the number of variables but also the ratio of the latter to the former."}
{"DOCID": "1172", "TEXT": "Conversion of Limited-Entry Decision Tables to Computer Programs: Decision tables are useful for describing a set of complex decision rules based on given sets of conditions.  Algorithms that can efficiently convert the tables into computer programs will extend the usefulness of decision tables to computer users. Two such algorithms, based on work done by M. S. Montalbano, are described and extended here to handle dashes and ELSE-decision rules.  The first algorithm minimizes the computer storage space required for the resultant program, the second minimizes computer running time. During the conversion process, both pinpoint any contradictions or redundancies among the rules in a table."}
{"DOCID": "1173", "TEXT": "The Performance of a System for Automatic Segmentation of Programs Within an ALGOL Compiler (GIER ALGOL): The GIER ALGOL compiler makes use of an automatic system for handling the transfers of program segments from the drum store to the core store at program execution time.  The logic of this system is described. The performance of the system is discussed, primarily on the basis of execution times related to two specific programs.  This discussion concludes with an assessment of the potential gains of various ways of improving the system."}
{"DOCID": "1174", "TEXT": "Inverse Permutation (Algorithm 250 [G6])"}
{"DOCID": "1175", "TEXT": "Quickersort (Algorithm 271 [M1])"}
{"DOCID": "1176", "TEXT": "Finding Eigenvectors by Gaussian Elimination (Algorithm 270 [F2])"}
{"DOCID": "1177", "TEXT": "Determinant Evaluation (Algorithm 269 [F3])"}
{"DOCID": "1178", "TEXT": "ALGOL 60 Reference Language Editor (Algorithm 268 [R2])"}
{"DOCID": "1179", "TEXT": "PUFFT-The Purdue University Fast FORTRAN Translator: A core resident, compile-and-go system designed for the IBM 7090/7094 computer is described. In little more than half of the 32k word core memory PUFFT provides a monitor for job sequencing, a translator for the full FORTRAN IV language, the FORTRAN subroutine library, an input--output system for use at compile time and at execute time, and a rather elaborate diagnostic message writing routine. Batches of small- and medium-sized FORTRAN IV source language programs are processed at very high speeds. Language compatibility has been maintained so that programs may be debugged in the PUFFT system and then recompiled and run in the IBJOB-IBFTC system supplied by the manufacturer."}
{"DOCID": "1180", "TEXT": "AXLE: An Axiomatic Language for String Transformations: AXLE is a language designed for data manipulation. Data arranged in a linear form in a workspace is transformed according to a table of axioms, called imperatives. A transformation consists of a matching procedure, which decides where an imperative is applicable, and a replacement procedure that modifies that part of the workspace.  Imperatives are applied in accordance with definitions of symbolic terms, presented systematically in an assertion table.  The process of definition includes the special case of recursive assertions.  Several complete programs of imperatives are given to show a few applications of the language."}
{"DOCID": "1181", "TEXT": "A Simple Data Transmission System Using the Office Telephone: A method has evolved for transmitting data of a type originating in many laboratory situation direct to a central computer.  The method requires almost no specialized equipment and uses any ordinary telephone on a \"callup\" basis.  Present applications include cardiac-output calculations, radio-activity tracer studies and neurophysiology time-sequence studies of nerve impulses."}
{"DOCID": "1182", "TEXT": "Contextual Correlates of Synonymy: Experimental corroboration was obtained for the hypothesis that the proportion of words common to the contexts of word A and to the contexts of word B is a function of the degree to which A and B are similar in meaning.  The shapes of the functions, however, indicate that similarity of context is reliable as criterion only for detecting pairs of words that are very similar in meaning."}
{"DOCID": "1183", "TEXT": "A Note on the Use of a Digital Computer for Doing Tedious Algebra and Programming: A special purpose compiler was written with FORTRAN II language and made possible the writing of very long programs by the computer.  The procedure is based on a straight-forward use of FORMAT statements for generating machine-written programs."}
{"DOCID": "1184", "TEXT": "A Fast Storage Allocator: A fast storage bookkeeping method is described which is particularly appropriate for list-structure operations and other situations involving many sizes of blocks that are fixed in size and location. This scheme, used in the LLLLLL or L6 (Bell Telephone Laboratories Low-Level List Language), makes available blocks of computer registers in several different sizes: the smaller blocks are obtained by successively splitting larger ones in half, and the larger blocks are reconstituted if and when their parts are simultaneously free."}
{"DOCID": "1185", "TEXT": "A program to Solve the Pentomino Problem by the Recursive Use of Macros: A coding technique is described in which certain macro-instructions are given lists as arguments and are thereby used recursively.  The discussion covers primarily an example in which the technique is used to solve the pentomino problem-the problem of fitting 12 pentominos without overlapping into a plane area formed of 60 elemental squares."}
{"DOCID": "1186", "TEXT": "Recursive Solution of a Class Of Combinatorial Problems: An Example: Combinatorial problems requiring the selection of n elements from a set of m elements may be solved by a recursion process analogous to that for computing binomial coefficients.  Several specific problems are analyzed, the general technique is exposed, and an ALGOL program is developed for one of the problems."}
{"DOCID": "1187", "TEXT": "Note on an ASCII-Octal Code Table (Standards)"}
{"DOCID": "1188", "TEXT": "An ALGOL-like Computer Design Language: The idea of constructing a computer design language by making use of an ALGOL-like programming language is presented.  A computer designer can benefit from using a design language at a higher level just as a computer user can benefit from a higher level programming language.  The purposes and requirements of the design language are enumerated.  To achieve most of the purposes a translator is required to translate a design of computer logic into a set of Boolean equations. The design language is presented in terms of vocabulary, statements, sequences and microprogram. Included are examples of identifiers, expressions with both unary and binary operators, declaration statements, transfer statements, terminal statements, exchange statements, if statements, do statements, go to statements, several sequences and a microprogram."}
{"DOCID": "1189", "TEXT": "Random Normal Deviate (Algorithm 267 [G5])"}
{"DOCID": "1190", "TEXT": "Pseudo-Random Numbers (Algorithm 266 [G5])"}
{"DOCID": "1191", "TEXT": "Find Precedence Functions (Algorithm 265 [L2])"}
{"DOCID": "1192", "TEXT": "Interpolation in a Table (Algorithm 264 [E1])"}
{"DOCID": "1193", "TEXT": "Gomory 1 (Algorithm 263 [H])"}
{"DOCID": "1194", "TEXT": "Establishment of the ACM Repository and Principles of the IR System Applied to its Operation: The history of the establishment of the ACM Repository at the Moore School, University of Pennsylvania, is reviewed briefly.  Two principles are presented as paramount in the provision of information services: (1) easy accessibility to the information files by users unfamiliar with file organization, and (2) value of service exceeding user costs.  These principles serve as guides in mechanizing the ACM Repository. The main features of the information system are direct user access via on-line teletypewriter console, direct user access to all details of the system organization, unrestricted and expandable search vocabulary, user access through many facets of document indexing, and stochastic search through linked index terms and other file relationships.  The first contribution to the ACM Repository consisted of 315 documents, relating primarily to early research on compilers.  These documents have been cataloged and indexed and the catalog is scheduled to appear in Computing Reviews. The indexing system is described in detail. The Main Catalog is used to describe the documents, and inverted lists are provided by the Repository system for retrieval by concept coordination."}
{"DOCID": "1195", "TEXT": "UPLIFTS-University of Pittsburgh Linear File Tandem System: A series of computer programs has been developed and is now operational for processing the National Aeronautics and Space Administration linear file system on an IBM 1401-7090 combined data processing system.  The program are note-worthy in that they create fixed length logical records and fixed length blocks from variable length source data, and format the output for optimization of processing on the IBM 7090 system.  The programs are completely self-checking and test for both validity and accuracy of the input materials as provided by the National Aeronautics and Space Administration."}
{"DOCID": "1196", "TEXT": "Applications of Differential Equations in General Problem Solving: A large class of problems leading to digital computer processing can be formulated in terms of the numerical solution of systems of ordinary differential equations.  Powerful methods are in existence for the solution of such systems.  A good general purpose routine for the solution of such systems furnishes a powerful tool for processing many problems.  This is true from the point of view of ease of programming, ease of debugging, and minimization of computer time. A number of examples are discussed in detail."}
{"DOCID": "1197", "TEXT": "Finding Zeros of a Polynomial by the Q-D Algorithm: A method which finds simultaneously all the zeros of a polynomial, developed by H. Rutishauser, has been tested on a number of polynomials with real coefficients.  This slowly converging method (the Quotient-Difference (Q-D) algorithm) provides starting values for a Newton or a Bairstow algorithm for more rapid convergence.  Necessary and sufficient conditions for the existence of the Q-D scheme are not completely known; however, failure may occur when zeros have equal, or nearly equal magnitudes. Success was achieved, in most of the cases tried, with the failures usually traceable to the equal magnitude difficulty.  In some cases, computer roundoff may result in errors which spoil the scheme.  Even if the Q-D algorithm does not give all the zeros, it will usually find a majority of them."}
{"DOCID": "1198", "TEXT": "Solution of a Problem in Concurrent Programming Control: A number of mainly independent sequential-cyclic processes with restricted means of communication with each other can be made in such a way that at any moment one and only one of them is engaged in the \"critical section\" of its cycle."}
{"DOCID": "1199", "TEXT": "A Computer Center Simulation Project: Today's computation centers are based on rapidly changing technologies of hardware and software systems.  It is difficult, therefore, to base decisions on experience; in most instances, the benefits of comparable experience for a given problem situation are not available.  In this paper, a mathematical model of the Lockheed Central Computer Center is formulated that describes the operation of a computation center in terms of information nets, decision processes, and control functions.  Experiments performed with this model, the results of the experiments, and the application of the results are discussed."}
{"DOCID": "1200", "TEXT": "On Reversible Subroutines and Computers that Run Backwards: A computer design is describe which permits subroutines to be executed backward as well as forward, either with their instructions unchanged or replaced with conjugate instructions.  It is shown that using this concept a number of new subroutine types can be developed with rather unusual properties. Since these properties are analogous to certain matrix operations, a parallel nomenclature is suggested for their classification."}
{"DOCID": "1201", "TEXT": "Generation of Permutations in Lexico-Graphical Order (Algorithm 202 [G6])"}
{"DOCID": "1202", "TEXT": "Normal Random (Algorithm 200 [G5])"}
{"DOCID": "1203", "TEXT": "Normdey (Algorithm 121 [G5])"}
{"DOCID": "1204", "TEXT": "Character Structure and Character Parity Sense for Serial-by-Bit Data Communication in the American Standard Code for Information Interchange (Proposed American Standard)"}
{"DOCID": "1205", "TEXT": "An Undergraduate Program in Computer Science-Preliminary Recommendations"}
{"DOCID": "1206", "TEXT": "The Self-Judgment Method of Curve Fitting: A computer-oriented method for processing and communicating numerical data is described.  The Instrument Reliability Factors (IRF), which exactly define the limits of reliability of each measured item of information, are used to compute the Maximum Permitted Error (MPE) associated with each values of each ordinate.  The Self-Judgment Principle (SJP) is used to discard wrong information and to compute mean values of the parameters and their MPE's in terms of the IRF.  Data compatibility tests with any number of different equations can be made quickly. Otherwise intractable problems are easily solved, and the design of many experiments is greatly simplified. The computational and mathematical techniques used to reduce bias in the SJP are discussed.  Inadequacies in the statistical and graphical methods of curve fitting are noted."}
{"DOCID": "1207", "TEXT": "Remarks on Simulation of Boolean Functions"}
{"DOCID": "1208", "TEXT": "Simulation of Computer Logic by Fortran Arithmetic"}
{"DOCID": "1209", "TEXT": "Negative and Zero Subscripts in Fortran II Programming for the IBM 1620"}
{"DOCID": "1210", "TEXT": "File-Handling Within FORTRAN: This note describes some FORTRAN subroutines to facilitate handling of tape files.  They allow symbolic naming of information files, without violating the casual scientific programmer's idea of simplicity. Some comments on two years use of these subroutines are given."}
{"DOCID": "1211", "TEXT": "A Note on Storage of Strings: A method for storing strings is described which uses blocks of indefinite size, and is therefore completely dynamic.  Its relation to similar schemes is discussed."}
{"DOCID": "1212", "TEXT": "Non-linear Extrapolation and Two-Point Boundary Value Problems: It is suggested that the convergence properties of the usual Picard successive approximation scheme may be improved through use of non-linrar extrapolation techniques.  A numerical example is provided."}
{"DOCID": "1213", "TEXT": "Dynamic Format Specifications: The use and implementation of two new FORTRAN format conversions are discussed.  These format types give the FORTRAN programmer control of input/output specifications at execution time."}
{"DOCID": "1214", "TEXT": "Some Experiments in Algebraic Manipulation by Computer: A set of subroutines to allow algebraic manipulations on the IBM 7094 computer has been written a List Processor, SLIP.  A series of four problems of increasing difficulty were solved using these routines."}
{"DOCID": "1215", "TEXT": "Some Techniques Used in the ALCOR ILLINOIS 7090: An ALGOL compiler has been written by the ALCOR group for the IBM 7090.  Some little known but significant techniques in compiler writing, together with organizational details of this compiler, are described.  Timing estimates and an indication of compiler requirements are also given."}
{"DOCID": "1216", "TEXT": "Symbolic Derivatives Without List Processing, Subroutines, or Recursion: A routine has been developed which computes and prints out the symbolic derivative of an absolutely continuous elementary function of one or several variables. No use is made of list-processing languages. The chain rule is applied and the result is edited to produce results as elegant and efficient as those obtained by hand computation.  A subset may be imbeded in a formula translator to introduce a differentiation operator into an \"algebraic\" programming language."}
{"DOCID": "1217", "TEXT": "Map of Partitions into Integers (Algorithm 264 [A1])"}
{"DOCID": "1218", "TEXT": "Partition Generator (Algorithm 263 [A1])"}
{"DOCID": "1219", "TEXT": "Number of Restricted Partitions of N (Algorithm 262 [A1])"}
{"DOCID": "1220", "TEXT": "9-J Symbols (Algorithm 261 [Z])"}
{"DOCID": "1221", "TEXT": "6-J Symbols (Algorithm 260 [Z])"}
{"DOCID": "1222", "TEXT": "Legendre Functions for Arguments Larger Than One (Algorithm 259 [S16])"}
{"DOCID": "1223", "TEXT": "High Speed Compilation of Efficient Object Code: A three-pass compiler with the following properties is briefly described:  The last two passes scan an intermediate language produced by the preceding pass in essentially the reverse of the order in which it was generated, so that the first pass is the only one which hasto read the bulky problem-oriented input.  The double scan, one in either direction, performed by the first two passes, allows the compiler to remove locally constant expressions and recursively calculable expressions from loops and to do the important part of common subexpression recognition. Optimization such as the effective use of index registers, although as important, is not discussed since the object code which would be most efficient is highly machine dependent.  The discussion is in terms of a FORTRAN-like language, although the technique is applicable to most algebraic languages."}
{"DOCID": "1224", "TEXT": "Determining a Computing Center Environment: An investigation is described in which several generally unavailable parameters descriptive of a computing center environment are obtained.  The actual data collection and reduction is described, and the results of one month of this collection are tabulated and summarized."}
{"DOCID": "1225", "TEXT": "The Predictive Analyzer and a Path Elimination Technique: Some of the characteristic features of a predictive analyzer, a system of syntactic analysis now operational at Harvard on and IBM 7094, are delineated. The advantages and disadvantages of the system are discussed in comparison to those of an immediate constituent analyzer, developed at the RAND Corporation with Robinson's English grammar.  In addition, a new technique is described for repetitive path elimination for a predictive analyzer, which can now claim efficiency both in processing time and core storage requirement."}
{"DOCID": "1226", "TEXT": "The Organization of Structured Files: A data file is an integral part of a data processing system.  In many systems, the selection of an organization for the data within the file can be critical to the system's operating efficiency. This paper provides the systems designer with an information source which describes ten techniques that may be employed for organizing structured data.  The characteristics of the organizations described are application independent, thus providing the designer with a reference which allows him to limit the number of file organizations he must consider for his system."}
{"DOCID": "1227", "TEXT": "Transport (Algorithm 258 [H])"}
{"DOCID": "1228", "TEXT": "Treesort 3 (Algorithm 245 [M1])"}
{"DOCID": "1229", "TEXT": "Random Permutation (Algorithm 235 [G6])"}
{"DOCID": "1230", "TEXT": "Method for Hyphenating at the End of a Printed Line: A description of a method of hyphenation is presented as a result of application of several general rules.  The character sets considered by the routine and the method are briefly outlined."}
{"DOCID": "1231", "TEXT": "Peephole Optimization: Redundant instructions may be discarded during the final stage of compilation by using a simple optimizing technique called peephole optimization. The method is described and examplesare given."}
{"DOCID": "1232", "TEXT": "Representation of the Standard ECMA 7-Bit Code in Punched Cards (ECMA Standard)"}
{"DOCID": "1233", "TEXT": "Conventions for the Use of Symbols in the Preparation of Flowcharts for Information Processing Systems (A Standard Working Paper): This paper is intended as an outline of the various conventions which are being considered for the use of flowcharts for information processing systems.  The conventions are applied to the use of the symbols appearing in the proposed American Standard Flowchart Symbols and not with the symbols per se."}
{"DOCID": "1234", "TEXT": "The Structure of Yet Another ALGOL Compiler: A high-speed \"top down\" method of syntax analysis which completely eliminates \"back-up\" of the source string has been implemented in a convenient macro-language.  A technique of  simulation at compile time of the use of a conventional run-time stack enables the generation of code for expressions which minimizes stores, fetches and stack-pointer motion at run time, while properly trating recursion and side effects of procedures.  Block structure and recursion are handled without need for interpretive methods at run times.  The \"context problem\" in the transmission to recursive procedures of parameters \"called by name\" is solved in a manner which permits the handling of the common cases of simple expressions and array identifiers with particular efficiency."}
{"DOCID": "1235", "TEXT": "A Stochastic Approach to the Grammatical Coding of English: A computer program is described which will assign each word in an English text to its form class or part of speech.  The program operates at relatively high speed in only a limited storage space. About half of the word-events in a corpus are identified through the use of a small dictionary of function words and frequently occurring lexical words.  Some suffix tests and logical-decision rules are employed to code additional words.  Finally, the remaining words are assigned to one class or another on the basis of the most probable form classes to occur within the already identified contexts.  The conditional probabilities used as a basis for this coding were empirically derived from a separate hand-coded corpus.On preliminary trials, the accuracy of the coder was 91% to 93%, with obvious ways of improving the algorithm being suggested by an analysis of the results."}
{"DOCID": "1236", "TEXT": "The SMART Automatic Document Retrieval System-An Illustration: A fully automatic document retrieval system operating on the IBM 7094 is described.  The system is characterized by the fact that several hundred different methods are available to analyze documents and search requests.  This feature is used in the retrieval process by leaving the exact sequence of operations initially unspecified, and adapting the search strategy to the needs of individual users. The system is used not only to simulate an actual operating environment, but also to test the effectiveness of the various available processing methods.  Results obtained so far seem to indicate that some combination of analysis procedures can in general be relied upon to retrieve the wanted information.  A typical search request is used as an example in the present report to illustrate systems operations and evaluation procedures."}
{"DOCID": "1237", "TEXT": "Conversion of Decision Tables To Computer Programs: Several translation procedures for the conversion of decision tables to programs are presented and then evaluated in terms of storage requirements, execution time and compile time.  The procedures are valuable as hand-coding guides or as algorithms for a compiler.  Both limited-entry and extended-entry tables are analyzed.  In addition to table analysis, the nature of table-oriented programming languages and features is discussed.  It is presumed that the reader is familiar with the nature of decision tables and conventional definitions."}
{"DOCID": "1238", "TEXT": "A Technique for Integrated Reports from a Multi-run System: The requirements of a requisition accounting system for the San Francisco Overseas Supply Agency (OSA) included exception reporting to OSA itself.  The simultaneous satisfaction of the reporting requirement and the accounting requirements posed definite problems in system design, particularly the handling of the reporting function.  A practical and satisfactory solution was developed by expanding the basic system with two tailored service runs for report production. These two runs permitted a final system that was easier to debug, easy to maintain, efficient in production and responsive to the changing requirements of OSA."}
{"DOCID": "1239", "TEXT": "Graycode (Algorithm 246 [Z])"}
{"DOCID": "1240", "TEXT": "Transport (Algorithm 258 [H])"}
{"DOCID": "1241", "TEXT": "Havie Integrator (Algorithm 257 [D1])"}
{"DOCID": "1242", "TEXT": "Modified Graeffe Method (Algorithm 256 [C2])"}
{"DOCID": "1243", "TEXT": "Testing the Understanding of the Difference Between Call by Name and Call by Value in ALGOL 60"}
{"DOCID": "1244", "TEXT": "Bit Manipulation in Fortran Language"}
{"DOCID": "1245", "TEXT": "A Fortran n-Ary Counter"}
{"DOCID": "1246", "TEXT": "Deeply Nested Iterations"}
{"DOCID": "1247", "TEXT": "An Operating Environment for Dynamic-Recursive Computer Programming Systems: Presented in this paper is a brief nontechnical introduction to OEDIPUS, a computer programming system which can serve as an operating environment for dynamic and/or recursive programs and programming systems.  The available services include dynamic allocation of storage for contiguous blocks of arbitrary size, input and output for a hierarchy of data types, a public pushdown list for automatic recursive programming, a rudimentary compiler for subroutine communication and bookkeeping, and debugging aids."}
{"DOCID": "1248", "TEXT": "On the Automatic Simplification of Computer Programs: Presented in this paper is the problem of writing a program which would examine any other program and perform such simplifications on it as can be detected from the argument-program's form alone, without having any knowledge of what it is supposed to do."}
{"DOCID": "1249", "TEXT": "Recorded Magnetic Tape for Information Interchange (200 CPI, NRZI) (On the Revised Proposed American Standard)"}
{"DOCID": "1250", "TEXT": "Graphic Symbols for Problem Definition and Analysis-A Standards Working Paper"}
{"DOCID": "1251", "TEXT": "American Standard and IFIP/ICC Vocabularies compared: The \"Proposed American Standard Vocabulary of Information Processing\" and the \"IFIP/ICC Vocabulary of Terms Used in Information Processing\" are analyzed and compared."}
{"DOCID": "1252", "TEXT": "Symbolic Notations for Statistical Tables and an Approach Towards Automatic System Design: The preparation of statistical tables is an important function of the data processing systems of some organizations, and a symbolic notation for the description of tables has been shown to be a useful aid to documentation.  Such a notation also provides the first step towards making automatic a tedious and time-consuming part of system design and programming in many computer applications.  One notation is described and suggestions are made for the implementation of the larger goal."}
{"DOCID": "1253", "TEXT": "QUIKSCRIPT-A SIMSCRIPT- Like Language for the G-20: QUIKSCRIPT is a simulation language based on SIMSCRIPT and programmed entirely in an algebraic language, 20-GATE.  The QUIKSCRIPT language, its internal implementation, and major differences between QUIKSCRIPT and SIMSCRIPT are presented.  This paper is not a programming guide to the language, but rather an attempt to present its flavor.  A brief description of SIMSCRIPT is included, as is a sufficient description of 20-GATE to render this material understandable to the reader familiar with algebraic languages."}
{"DOCID": "1254", "TEXT": "The Iteration Element: A recent addition to the MAD language has made the iteration structure of the MAD THROUGH statement (corresponding to the ALGOL for statement and the FORTRAN DO statement) available within expressions."}
{"DOCID": "1255", "TEXT": "A Method of Data List Processing With Application to EEG Analysis: A set of subroutines is discussed, which is designed to aid in the programming of computations on indexed lists of numbers using machine language or a symbolic assembly system.  The most commonly performed list operations are outlined, and logically arranged into five groups.  As an example, the computation of power spectral density from the autocovariance function is discussed for a class of EEG signals."}
{"DOCID": "1256", "TEXT": "Dynamic Variable Formatting"}
{"DOCID": "1257", "TEXT": "DEBUG-An Extension to Current On-Line Debugging Techniques: A method of on-linr assembly-language debugging which greatly simplifies several of the bookkeeping tasks characteristically associated with that process has been developed and implemented in a program for the UNIVAC M-460 computer at Air Force Cambridge Research Laboratories.  With this program, an online user may insert or delete (in symbolic assembly language) any number of lines at any point of his previously assembled program in core, with the remainder of the program being relocated appropriately."}
{"DOCID": "1258", "TEXT": "An Extended Arithmetic Package: In many fields, for example algebraic number theory, arithmetic must be carried out to a degree of precision which exceeds the normal hardware capacity of most machines.  In such cases, an extended arithmetic package provides a comprehensive and easy-to-use way of performing such arithmetic.  Such a package was coded for the IBM 7090.  In discussing the general problems associated with the design of an extended arithmetic package, specific reference is made to this program."}
{"DOCID": "1259", "TEXT": "Applications of Binary Numbers In Computer Routines: A binary number can be thought of as an alternate form of expression for either a set of letters or a decimal number.  There are then three equivalent expressions, easily translatable to one another, each having different characteristics.  Four examples are given in which the form of an expression is changed to an equivalent expression to save space or gain power."}
{"DOCID": "1260", "TEXT": "Least-Squares Analysis of Resonance Spectra on Small Computers: The problem of analyzing data from a Mossbauer effect experiment is discussed.  By using the cut step procedure for convergence and by imposing physical constraints on the functional form of the calculation it is possible to make the analysis on a small computer.  The analysis has been carried out on an IBM 1410 computer with a 40,000 BCD core memory."}
{"DOCID": "1261", "TEXT": "Modeling and Simulation of Digital Networks: The simulation of digital networks on a digital computer provides the engineer with an effective means of analyzing time-quantized logical behavior.  The digital network is modeled as a set of time-dependent or time-independent Boolean transformations; each transformation describing the input-output relationship of a model element comprising the network mode.  The simplicity of utilizing the FORTRAN IV Programming System as a digital Network Simulator is discussed an illustrated.  This simplicity is derived from a common modeling technique applicable to combinational and sequential digital networks and a systematic programming approach."}
{"DOCID": "1262", "TEXT": "Procedure-Oriented Language Statements to Facilitate Parallel Processing: Two statements are suggested which allow a programmer writing in a procedure-oriented language to indicate sections of program which are to be executed in parallel.  The statements are DO TOGETHER and HOLD.  These serve partly as brackets in establishing a range of parallel operation and partly to define each parallel path within this range.  DO TOGETHERs may be nested.  The statements should be particularly effective for use with computing devices capable of attaining some degree of compute-compute overlap."}
{"DOCID": "1263", "TEXT": "Metalanguage and Syntax Specification: Two metalanguages are described, one sufficient for the table specification of the ALGOL syntax, the other with additional metaoperators adequate and used for the formal table description of Basic FORTRAN."}
{"DOCID": "1264", "TEXT": "BLNSYS-A 1401 Operating System with Braille Capabilities: BLNSYS is an operating system designed for a 4K 1401 with common optional features and two attached tape drives.  Printed output of this system or of executing programs may be in either English or braille.  Even though this system was written for a small machine with minimal peripheral equipment, jobs may be batched, so that card handling and lost processing time is at a minimum.  This system will perform any or all of the following users specified functions: assemble SPS source decks, post list, produce condensed or uncondensed object decks, execute user's program, list card input to a program, list punched output, provide a storage dump, execute a program submitted for execution as an uncondensed object deck under debugging trace control, card-to-braille conversion, brailled listings of 7040 IBSYS batch output, and update or duplicate the system tape itself.  Input-ouput subroutines are also included in the system."}
{"DOCID": "1265", "TEXT": "On the Relative Efficiencies of Context-Free Grammar Recognizers: A number of diverse recognition procedures that have been proposed for parsing sentences with respect to a context-free grammar are described in this paper by means of a common device.  Each procedure is defined by giving an algorithm for obtaining a nondeterministic Turing Machine recognizer that is equivalent to a given context-free grammar.  The formalization of the Turing Machine has been chosen to make possible particularly simple description of the parsing procedures considered.  An attempt has been made to compare recognition efficiencies for the procedures defined.  For a few simple grammars and sentences a formal comparison has been made.  Empirical comparison of the recognition of more realistic programming languages such as LISP and ALGOL has been made by means of a program which simulates the Turing Machine on the Univac M-460 computer.  Several algorithms for producing grammars equivalent to a given context-free grammar have been considered, and the increase in recognition efficiency they afford has been empirically investigated."}
{"DOCID": "1266", "TEXT": "Considerations Relating to Purpose of FORTRAN Standardization (Appendixes to ASA FORTRAN Standard)"}
{"DOCID": "1267", "TEXT": "Performance of Systems Used for Data Transmission Transfer Rate of Information Bits -An ASA Tutorial Standard: Information thruput as a characteristic of systems performance is discussed.  This discussion includes the pertinent aspects of information transfer, of determination of transfer rate of information bits (TRIB), of residual errors, and of standard measurement conditions.  The paper also presents an orderly arrangement of characteristics and parameters that affect information thruput, and some examples on procedures for determining a thruput rate in terms of TRIB.  It concludes that a performance characteristic involving information rate can best be expressed as the TRIB in conjunction with the Residual Error Rate."}
{"DOCID": "1268", "TEXT": "Logarithm of a Complex Number (Algorithm 243 [B3])"}
{"DOCID": "1269", "TEXT": "Computation of Fourier Coefficients (Algorithm [C6])"}
{"DOCID": "1270", "TEXT": "On ALGOL Education: Automatic Grading Programs: Two ALGOL grader programs are presented for the computer evaluation of student ALGOL programs. One is for a beginner's program; it furnishes random data and checks answers. The other provides a searching test of the reliability and efficiency of an integration procedure.  There is a statement of the essential properties of a computer system, in order that grader programs can be effectively used."}
{"DOCID": "1271", "TEXT": "Secondary Key Retrieval Using an IBM 7090-1301 System: The secondary key retrieval method involves the preparation of secondary storage lists from primary data records. Search requests are satisfied by logical operations on appropriate lists, producing a complete set of addresses of primary records relevant to the request.  Experimental results are presented and a comparative analysis is given."}
{"DOCID": "1272", "TEXT": "Expanding the Editing Function In Language Data Processing: In automatic abstracting, citation indexing, mechanical translation and other such procedures, editing is required whenever the automatic method leaves something to be desired.  This paper discusses the economy of editing as a function of the amount of condensation of text in language processing operations, and then contends that editing can be regarded as an opportunity rather than as an unwelcome necessity. \"Heavy editing,\" which goes beyond mere correction and improvement of computer output, is exemplified by the use of a concordance in preparing a survey article or lecture.  Other opportunities for heavy editing are described, chief among them being interpretation and expansion of computer output in such processes as factor analysis.  Applications are described, such processes as factor analysis.  Applications are described, such as the quick, unbiased evaluation of a large volume of incoming mail or telegrams, yielding summary reports not possible for either humans or computers to produce alone."}
{"DOCID": "1273", "TEXT": "Remark on Romberg Quadrature: A modified form of Romberg quadrature is described, which is less sensitive to the accumulation of rounding errors than the customary one."}
{"DOCID": "1274", "TEXT": "On the Numerical Solution of an N-Point Boundary Value Problem for Linear Ordinary Differential Equations: A method for the numerical solution of then-point boundary value problem for homogeneous linear ordinary differential equations is developed.  The method requires two Runge-Kutta integrations over the interval under consideration and the solution of a linear system of equations with n-1 unknowns."}
{"DOCID": "1275", "TEXT": "Code Structures for Protection and Manipulation of Variable Length Items (Corrigendum)"}
{"DOCID": "1276", "TEXT": "Still Another Use for FORTRAN II Chaining"}
{"DOCID": "1277", "TEXT": "The Use of Cobol Subroutines in Fortran Main Programs"}
{"DOCID": "1278", "TEXT": "Wengert's Numerical Method for Partial Derivatives, Orbit Determination and Quasilinearization: In a recent article in the Communications of the ACM, R. Wengert suggested a technique for machine evaluation of the partial derivatives of a function given in analytical form.  In solving non-linear boundary-value problems using quasilinearization many partial derivatives must be formed analytically and then evaluated numerically.  Wengert's method appears very attractive from the programming viewpoint equations which might not otherwise be undertaken."}
{"DOCID": "1279", "TEXT": "Use of a Conditional Base Number System for Encoding Sequences of Correlated Characters: A procedure is described for the relatively efficient encoding of sequences of characters which have predecessor-successor selection rules.  The procedure is shown to assign a unique integer to each sequence and to generate a reasonably compact set of values."}
{"DOCID": "1280", "TEXT": "Numerical Integration of a Differential-Difference Equation with a Decreasing Time-Lag: Systems in which variable time-lags are present are of common occurrence in biology.  Variable flow rates are a common cause of these variable lags. At present no extensive body of knowledge exists concerning the effects which these variable lags can cause.  Shown here is a method of reducing some differential-difference equations to ordinary differential equations which can then be studied numerically with ease.  Subsequent study will deal with situations in which multiple-lags and lags dependent on the solution itself are present."}
{"DOCID": "1281", "TEXT": "Data Input by Question and Answer: A data input scheme for a time-sharing computer is described in this paper.  Instead of using format statements to determine the input, the computer asks the user for the required values one at a time.  The computer converses with the user during the input process, checks for errors, provides standard data, and allows editing of values input."}
{"DOCID": "1282", "TEXT": "The Use of FORTRAN in Subroutines with COBOL Main Programs: By using the proper COBOL coding techniques and accounting for differences in storage allocation and library routines between the two languages, it is possible to write FORTRAN IV subroutines that may be called from COBOL main programs.  Such a technique enables the programmer to take advantage of the most useful properties of each language while minimizing their respective disadvantages."}
{"DOCID": "1283", "TEXT": "Matrix Inversion (Algorithm 231 [F1])"}
{"DOCID": "1284", "TEXT": "Bessel Function for a Set of Integer Orders"}
{"DOCID": "1285", "TEXT": "Eigenvalues and Eigenvectors of a Real Symmetric Matrix by the QR Method (Algorithm 254 [F2])"}
{"DOCID": "1286", "TEXT": "Eigenvalues of a Real Symmetric Matrix by the QR Method (Algorithm 253 [F2])"}
{"DOCID": "1287", "TEXT": "Vector Coupling or Clebsch-Gordan Coefficients (Algorithm 252 [Z])"}
{"DOCID": "1288", "TEXT": "CLP-The Cornell List Processor: Presented in this paper are the highlights of CLP, a teaching language which has been employed at Cornell University and was constructed to serve as a means of introducing simulation and other list-processing concepts.  The various advantages of CLP are discussed and examples are given."}
{"DOCID": "1289", "TEXT": "Proposed Revised American Standard Code for Information Interchange"}
{"DOCID": "1290", "TEXT": "Transparent-Mode Control Procedures for Data Communication, Using the American Standard Code for Information Interchange -A Tutorial: This paper gives the considerations of Task Group X3.3.4 in the area of transparent-mode data communication control philosophy.  The appearance of this paper was forecast (underthe name of \"second-level control\") in the earlier tutorial paper, \"Control Procedures for Data Communications,\" Task Group document X3.3.4.44, dated May 1964.  The present paper elaborates upon solutions to the problems of transparency to the basic ASCII communication control characters as outlined in the previous paper mentioned above. Moreover, it goes on to cover the additional control problems of handling material such as off line encrypted data or non-ASCII codes by means of systems providing complete character transparency.  It does not cover concepts of transparency in which the normal character structure or modulation rate of a system may be abandoned.  In conjunction with the earlier tutorial paper, this paper is expected to lead to a proposal for stand ardizationof data communication control procedures using the American Stand ard Code for Information Interchange."}
{"DOCID": "1291", "TEXT": "Tabular Input of Data"}
{"DOCID": "1292", "TEXT": "On a Divide-and-Correct Method For Variable Precision Division: Described in this paper is a divide-and-correct method for variable precision division in digital computers.  Unlike the earlier methods of Stein and Pope, the present method uses a suitably rounded form of the normalized divisor for getting an estimate of the quotient characters.  This results in a correction of at most plus or minus one to the estimate, to obtain the exact quotient character.  It is believed that this method will be widely applicable for division operations in variable word-length character-oriented machines."}
{"DOCID": "1293", "TEXT": "Method is Randomness: Certain nonrandom properties of a commonly used random number generator are described and analyzed."}
{"DOCID": "1294", "TEXT": "Note on Triple-Precision Floating-Point Arithmetic with 132-Bit Numbers: In a recent paper, Gregory and Raney described a technique for double-precision floating-point arithmetic.  A similar technique can be developed for triple-precision floating-point arithmetic and it is  the purpose of this note to describe this technique. Only the multiplication and the division algorithms are described, since the addition-subtraction algorithm can be obtained by a trivial modification of the algorithm in Gregory's and Raney's paper."}
{"DOCID": "1295", "TEXT": "PERT Time Calculations Without Topological Ordering: A simplified technique is presented for PERT Time calculations without topological ordering. Each event is assigned a unique memory location.  An activity is represented by a link.  A link is defined as a memory location containing the address of another memory location.  The time information for an activity is carried with its link.  For a typical net, the majority of activities can be described by one 36-bit cell each.  The remainder use two 36-bit cells each.  The links are unidirectional; forward during the T(E) calculation (expected completion time for an activity);backward during the T(L) calculation (time latest allowable for completion of an activity). The calculations progress through the net topologically even though the net is not represented topologically in core."}
{"DOCID": "1296", "TEXT": "Ative (Algorithm 205 [E4])"}
{"DOCID": "1297", "TEXT": "Steep1 (Algorithm 203 [E4])"}
{"DOCID": "1298", "TEXT": "Adaptive Numerical Integration by Simpson's Rule (Algorithm 145 [D1])"}
{"DOCID": "1299", "TEXT": "Solutions of the Diophantine Equation (Algorithm 139 [A1])"}
{"DOCID": "1300", "TEXT": "Function Minimization (Algorithm 251[E4])"}
{"DOCID": "1301", "TEXT": "On ALGOL I/O Conventions"}
{"DOCID": "1302", "TEXT": "Parallel Signaling Speeds for Data Transmission (Proposed American Stand ard)"}
{"DOCID": "1303", "TEXT": "A Correspondence Between ALGOL 60 and Church's Lambda-Notation: Part II*"}
{"DOCID": "1304", "TEXT": "A Rapid Turnaround Multi-Programming System: In this paper, basic features, system characteristics and the control algorithm for a multi-programming system with rapid turnaround time are described."}
{"DOCID": "1305", "TEXT": "The Internal Structure of the FORTRAN CEP Translator: The FORTRAN CEP translator converts a source program written in the FORTRAN CEP language into an object program written in the language of the CEP computer.  In this paper, after an outline of the CEP computer, the internal structure of the translator is described.  Emphasis is on the compilation of expressions, of input/output lists, and of subscripted variables."}
{"DOCID": "1306", "TEXT": "A Class of Unambiguous Computer Languages: Discussed in this paper is the concept of a fully nested computer language which may be one means of designing computer languages which would be completely free of ambiguities.  Several suggestions are also given here for the redefinition of ALGOL as a fully nested language."}
{"DOCID": "1307", "TEXT": "A Lightpen-Controlled Program For On-Line Data Analysis: This paper describes a technique designed to ease the use of a data processing system by a person, in particular, a scientist, who is intimately and primarily concerned with interpreting the significance of data handled by the system.  Since such a person is often unable to spend the time necessary to master a programming language, it is essential that he be aided in composing commands to the computer.  In the system described, the user is not required to learn or remember the vocabulary of the language because the vocabulary is displayed before him on\"menus\" by means of a computer-drive scope.  He selects the various vocabulary elements required by pointing with the light pen.  By use of a small unordered set of rewriting rules applied as a result of light pen selections, the user generates only syntactically correct commands to the system.  He does not have to learn or remember the grammar.  The program restricts the user severely in the particular language he can use, but the method for communicating with the program makes these restrictions seem quite natural and unconstraining. The program has been used successfully for over ten months."}
{"DOCID": "1308", "TEXT": "A Mathematical Model for Mechanical part Description: The flexibility of a mathematical model takes advantage of the common information requirements of computer-aided engineering drawing, numerical control tape generation, and physical characteristic computation.  By judicious control of man-machine communication requirements, improved results over conventional engineering design processes are possible.  An English-like input language, tailored for use by draftsmen and designers, will describe the part and specify the output desired.  One approach to the mathematical model consists of a group of surface-defining quadric equations, which are created by a system of modular subprogram.  Other subprograms will convert the mathematical model into instructions for driving automatic drafting machines and numerical controlled machine tools. Physical part characteristics, such as center of gravity, can be computed by subprograms and used in dynamic analysis work.  The proposed overall system is presented and experiments and demonstrations are discussed."}
{"DOCID": "1309", "TEXT": "A Computer User-Oriented System: A computer language system has been developed which makes possible fast preparation of management reports, regardless of computational complexity or format variety.  Costs are sufficiently low so that individually tailored reports can be prepared for every manager.  The system requires initial preparation of large data banks containing data in elementary form. Use of two special languages, EXTRACT and MATRAN, permits selective extraction of any data subset, efficient processing through any computational sequence, and flexible presentation of results in either tabular or graphical form.  Matrix algebra is used as a fundamental vehicle for accomplishing both manipulation and computation."}
{"DOCID": "1310", "TEXT": "A Rapid Braille Transliteration Technique for Certain IBM Machines"}
{"DOCID": "1311", "TEXT": "Efficient Autocorrelation"}
{"DOCID": "1312", "TEXT": "Recursion and Iteration"}
{"DOCID": "1313", "TEXT": "Construction of Nonlinear Programming Test Problems"}
{"DOCID": "1314", "TEXT": "The Organization of Symbol Tables: An efficient symbol table organization is an important feature in the design of any compiler. During the construction of the Virginia ALGOL 60 compiler for the Burroughs B205, the primary consideration in the symbol table design was that the recognition of identifiers and reserved words should be as rapid as possible.  The general features of the technique are described."}
{"DOCID": "1315", "TEXT": "Automation of the Radioisotope Accountability System: The Radioisotope Service of the Veterans Administration Hospital, Omaha, Nebraska, used a manual system of radioisotope accountability for three years.  The procedure which was satisfactory but time-consuming was converted from manual to a fully automated computer system in January, 1963.  The program for purchased radioisotopes is written in FORMAT FORTRAN for the IBM 1620 Computer.  A second program for maintaining accountability for reactor-created radioisotopes is written in the FORCOM programming language.  A minimum amount of bookkeeping is required by the reactor operating staff.  The United States Atomic Energy Commission regulations specify that records be kept.  This system provides detailed records for each container of radioactive material purchased and/or created in the Triga reactor indicating the amounts received, used, and/or transferred to the health physicist for disposal. Consolidated records contain total amounts received, used, and/or disposed of for any specified period of time.  Purchased radioisotopes are reported in millicuries; reactor-created radioisotopes in microcuries."}
{"DOCID": "1316", "TEXT": "Bessel Functions of the First Kind (Algorithm 236 [S17])"}
{"DOCID": "1317", "TEXT": "Poisson-Charlier Polynomials (Algorithm 234 [S23])"}
{"DOCID": "1318", "TEXT": "Arccossin (Algorithm 206 [B1])"}
{"DOCID": "1319", "TEXT": "Crout with Equilibration and Iteration (Algorithm 135 [F4])"}
{"DOCID": "1320", "TEXT": "Inverse Permutation (Algorithm 250 [G6])"}
{"DOCID": "1321", "TEXT": "Outreal N (Algorithm [I5])"}
{"DOCID": "1322", "TEXT": "Netflow (Algorithm 248 [H])"}
{"DOCID": "1323", "TEXT": "A Correspondence Between ALGOL 60 and Church's Lambda-Notation: Part I*: This paper describes how some of the semantics of ALGOL 60 can be formalized by establishing a correspondence between expressions of ALGOL 60 and expressions in a modified form of Church's L-notation. First a model for computer languages and compute behavior is described, based on the notions of functional application and functional abstraction, but also having analogues for imperative language features. Then this model is used as an \"abstract object language\" into which ALGOL 60 is mapped.  Many of ALGOL 60's features emerge as particular arrangements of a small number of structural rules, suggesting new classifications and generalizations.  The correspondence is first described informally, mainly by illustrations. The second part of the paper gives a formal description, i.e. an \"abstract compiler\" into the \"abstract objct language.\"  This is itself presented in a \"purely functional\" notation, that is one using only application and abstraction."}
{"DOCID": "1324", "TEXT": "Answering English questions by Computer: A Survey: Fifteen experimental English language question-answering systems which are programmed and operating are described and reviewed.  The systems range from a conversation machine to programs which make sentences about pictures and systems which translate from English into logical calculi.  Systems are classified as list-structured data-based, graphic data-based, text-based and inferential.  Principles and methods of operations are detailed and discussed.  It is concluded that the data-base question-answer has passed from initial research into the early developmental phase.  The most difficult and important research questions for the advancement of general-purpose language processors are seen to be concerned with measuring, dealing with ambiguities, translating into formal languages and searching large tree structures."}
{"DOCID": "1325", "TEXT": "Remote, On-Line, Real-time Computer Diagnosis of the Clinical Electrocardiogram: Presented in this paper is a brief report on the hardware, software, system configuration and function of a system for the remote, online, real-time digital computer diagnosis of clinical electrocardiograms. It seems likely that efforts of this sort will lead to a satisfactory solution to the problem of the automatic diagnosis of electrocardiograms.  Current attempts by the authors to extend the diagnostic capabilities of the present system are particularly concerned with increasing the fidelity of the adaptive matched filters, the development of three dimensional pattern analysis, the analysis of parallel electrocardiographer-computer diagnostic interaction, and a study of the possibility of introducing major, tree-like branching decisions early in the diagnostic process."}
{"DOCID": "1326", "TEXT": "Boundary Networks: A feasible computer procedure is described for determining the total or partial inclusion of arbitrarily given points and lines with respect to a set of general polygonal domains which partition a plane bounded region.  A scheme for the computer representation of the boundaries of the domains and an algorithm, based on this  scheme, for evaluating the inclusion relations are specified in detail. The method employs several levels of selection criteria for the purpose of reducing the number of accesses to auxiliary storage devices and the amount of boundary data for which processing is required."}
{"DOCID": "1327", "TEXT": "Use of Decision Tables in Computer Programming: A decision table is a tabular form for displaying decision logic.  Decision tables have many inherent advantages.  The technique to be illustrated puts these advantages to use in that it enables one to program directly from a decision table.  The technique is based on the creation of a binary image of a limited entry decision table in computer memory. A binary image of a given set of input conditions can also be created.  This data image is used to scan the decision table image to arrive at the proper course of action.  There are several advantages gained from the programming point view: (1) amount of computer memory used is drastically reduced, (2) programming is simplified, and (3) documentation is brief and clear."}
{"DOCID": "1328", "TEXT": "Further Remarks on Reducing Truncation Errors"}
{"DOCID": "1329", "TEXT": "Simulation of Boolean Functions in a Decimal computer"}
{"DOCID": "1330", "TEXT": "Automated Plotting Flow-Charts on a Small Computer"}
{"DOCID": "1331", "TEXT": "Code Structures for Protection and Manipulation of Variable-Length Items: When items are made up of a variable number of characters, each containing the same number of bits, certain control information (partition symbols) is inserted to mark their separations.  Since errors in identification of these control characters can lead to serious trouble, methods of protecting these symbols are indicated.  A 6-bit code assignment of alphanumeric characters for fixed word-length computers is given and its suitability for error detection and variable-length item manipulation is shown. Also indicated is its flexibility during certain arithmetic operations."}
{"DOCID": "1332", "TEXT": "Subroutine Assembly: A description is given of an assembly system, which requires only one pass and does not maintain a table of information about the subroutine library."}
{"DOCID": "1333", "TEXT": "Reducing Truncation Errors Using Cascading Accumulators: When accumulating a large number of quantities as in numerical integration, the sum itself may become much larger than the individual addends. This results in truncation error.  Much of this error can be eliminated using cascading accumulators as noted in a recent article by Wolfe.  A simpler and slightly more flexible algorithm is presented which deals also with the case of negative addends."}
{"DOCID": "1334", "TEXT": "Mechanization of Tedious Algebra: The Newcomb Operators of Planetary Theory: A computer program has been written to generate tables of formulas for the Newcomb operators of planetary theory.  The Newcomb operators are expressed as polynomials in two variables, one of which stands for a simple differential operator, and the other for an arbitrary integer.  The polynomials are generated by a recurrence scheme.  The program is coded in FORTRAN, using simple array manipulation techniques to perform the algebraic operations.  Formulas for over 100 Newcomb operators have been produced by the program and typeset photographically on an S-560 Photon system."}
{"DOCID": "1335", "TEXT": "Character Set for Optical Character Recognition (Proposed American Stand ard)"}
{"DOCID": "1336", "TEXT": "NPL: Highlights of A New Programming Language"}
{"DOCID": "1337", "TEXT": "EULER: A Generalization of ALGOL, and its Formal Definition"}
{"DOCID": "1338", "TEXT": "Additional Comments on a Problem in Concurrent Progamming Control"}
{"DOCID": "1339", "TEXT": "A Contribution to the Development of ALGOL"}
{"DOCID": "1340", "TEXT": "Multiplexing of Slow Peripherals: The philosophy of a monitor which allows slow output devices to be multiplexed is presented."}
{"DOCID": "1341", "TEXT": "Levels of Computer Systems: In building current computer systems, we tend to break them down into \"levels\" of control, command and communication; in using the system, we break our problems down correspondingly.  The continued use of such a structure raises questions about its effects on the usefulness of future systems, particularly with regard to such trends as time sharing, parallel programming, and, eventually, systems which learn. In this essay some of these questions are posed, and the general attitude we must take in pursuing the problem further is discussed."}
{"DOCID": "1342", "TEXT": "Transportation Problem (Algorithms 293 [H])"}
{"DOCID": "1343", "TEXT": "Havie Integrator (Algorithm 257 [D1])"}
{"DOCID": "1344", "TEXT": "Statistical Computations Based Upon Algebraically Specified Models: Based upon a machine-readable statistical model and related symbolic specifications, an efficient method of performing calculations for statistical models of a balanced complete nature is presented. Fixes, mixed, and random analysis of variance models are considered.  A procedure for obtaining variance components and calculated F statistics for the model terms is included."}
{"DOCID": "1345", "TEXT": "Tensor Calculations on the Computer: A FORMAC program has been written which is capable of calculating various quantities of interest in tensor calculus.  Using this code, Christoffel symbols have been calculated for 12 basic orthogonal coordinate systems."}
{"DOCID": "1346", "TEXT": "On the Application of the Process of Equalization of Maxima to Obtain Rational Approximation to Certain Modified Bessel Functions: The second Remes algorithm as originally established for polynomials, may converge or not when the approximating functions are rational.  However, the few results known in this domain show how efficient the algorithm can be to obtain approximations with a small error, much more than in the polynomial case, in which the best approximation can be very nearly approached directly by a series development.  The aim of this paper is to investigate the limitations of the applicability of certain extensions of the algorithm to the case where the approximations are rational as well as to present some numerical results."}
{"DOCID": "1347", "TEXT": "General Time-Varying Systems Error Sensitivities Program: The evaluation, by the propagation of variance technique, of the sensitivity of time-varying systems to initial condition and parameter errors, involves the determination of several system-dependent partial derivative matrices.  This requirement has led to separate programs for each system under investigation. A new program, through utilization of the Wengert differentiation technique, automatically determines the required matrices from specific system equations supplied in subroutine form at execution time, eliminating the need for individualized programs, and presaging the further development of extremely general computer programs."}
{"DOCID": "1348", "TEXT": "FLOWTRACE, A Computer Program for Flowcharting Programs: The FLOWTRACE system produces flowcharts of programs written in \"almost any\" programming language. One most describe the syntax of the control statements in his language; for this purpose a metalanguage is available.  The resultant object deck is used to flowchart any programs in the language described. Several examples of FAP and SNOBOL flowcharts are given. However, it is not necessary to confine one's scope to existing languages.  One may define his own language in any \"well-structured\" manner.  This feature is particularly useful when it is desirable to chart only comments within a program.  Such an approach permits the documentation of descriptive remarks and avoids the inclusion of coding details."}
{"DOCID": "1349", "TEXT": "Computing Capabilities at Western European Universities: This report on the author's trip to universities in Western Europe in the summer of 1966 gives brief descriptions of computing activities at each institution visited.  Present equipment capabilities vary from moderate to large scale; however, many institutions plan to acquire complex time-shared systems in the near future.  In the author's opinion, the state of the art lags behind that on this continent. This lag is attributed to four principal factors: (a) the handicapping organization of academic procedures; (b) the university-government financial relationship; (c) the subordinated organization of the computing facility; (d) the paucity of professional interchange of knowledge.  The effects of these constraints are explicated."}
{"DOCID": "1350", "TEXT": "The Augmented Predictive Analyzer for Context-Free Languages-Its Relative Efficiency: It has been proven by Greibach that for a given context-free grammar G, a standard-form grammar Gs can be constructed, which generates the same languages as is generated by G and whose rules are all of the form Z --> cY(1) ... Y(m), (m >= O) where Z and Y(i) are intermediate symbols and c a terminal symbol.  Since the predictive analyzer at Harvard uses a standard-form grammar, it can accept the language of any context-free Grammar G, given an equivalent standard-form grammar Gs.  The structural descriptions SD(Gs,X) assigned to a given sentence X by the predictive analyzer, however, are usually different from the structural descriptions SD(G,X) assigned to the same sentence by the original context-free grammar G from which Gs is derived.  In Section 1, an algorithm, originally due to Abbott is described standard-form grammar each of whose rules is in standard form, supplemented by additional information describing its derivation from the original context-free grammar. A technique for performing the SD(Gs,X) to SD(G,X) transformation effectively is also described.  In section 2, the augmented predictive analyzer as a parsing algorithm for arbitrary context-free languages is compared with two other parsing algorithms: a selective top-to-bottom algorithm similar to Irons' \"error correcting parse algorithm\" and an immediate constituent analyzer which is an extension of Sakai-Cocke's algorithm for normal grammars.  The comparison is based upon several criteria of efficiency, covering core-storage requirements, complexities of the programs and processing time."}
{"DOCID": "1351", "TEXT": "Automatic Error Bounds on Real Zeros of Rational Functions: A procedure for implementing an interval arithmetic version of the Newton-Raphson method is proposed.  The procedure require only a starting interval over which the zeros of a given rational function are to be located.  The method automatically provides bounds for roundoff error."}
{"DOCID": "1352", "TEXT": "Automatic Integration of a Function with a Parameter: Two efficient methods for automatic numerical integration are Romberg integration and adaptive Simpson integration.  For integrands of the form f(x)g(x,a) where a is a parameter, it is shown that Romberg's method is more efficient.  A FORTRAN program shows how to achieve this greater efficiency."}
{"DOCID": "1353", "TEXT": "Techniques for Automatic Tolerance Control in Linear Programming: In this technical note, the numerical steps for the simplex method of linear programming are reviewed and the tolerances needed in the numerical procedure are defined.  Objective criteria are given for accomplishing the numerical steps of the method and the calculation of necessary tolerances."}
{"DOCID": "1354", "TEXT": "Conversion of Decision Tables to Computer Programs by Rule Mark Techniques: The rule mask technique is one method of converting limited entry decision tables to computer programs.  Recent discussion suggest that in many circumstances it is to be preferred to the technique of constructing networks or trees.  A drawback of the technique as hitherto presented is its liability to produce object programs of longer run time than necessary. In this paper a modification of the technique is discussed which takes into account both rule frequencies and the relative times for evaluating conditions. This can materially improve object program run time."}
{"DOCID": "1355", "TEXT": "Regular Coulomb Wave Functions (Algorithm 292 )"}
{"DOCID": "1356", "TEXT": "Havie Integrator (Algorithm 257 [D1])"}
{"DOCID": "1357", "TEXT": "Examination Scheduling (Algorithm 286 [H])"}
{"DOCID": "1358", "TEXT": "Syntax Macros and Extended Translation: A translation approach is described which allows one to extended the syntax and semantics of a given high-level base language by the use of a new formalism called a syntax-macro.  Syntax-macros define string transformations based on syntactic elements of the base language.  Two types of macros are discussed, and examples are given of their use.  The conditional generation of macros based on options and alternatives recognized by the scan are also described."}
{"DOCID": "1359", "TEXT": "Data Filtering Applied to Information Storage and Retrieval Applications: Manipulation of data strings is the most complex processing function in information storage and retrieval applications.  Data string manipulation is discussed within the context of an interpretive processing environment controlled by the use of procedural directives.  The sequence of procedural directives is derived from a job assumed to be expressed in a user-oriented source language.  Each data string with the structured data environment (data bank) is explicitly or implicitly related to a format declaration residing in a format library.  The processing mechanics associated with data string manipulation is developed in accordance with a generalized data filtering concept. This results in the implementation of a two-part data filter module that satisfies internal processing functions by filtering data strings through format declarations associated with its input and output ports."}
{"DOCID": "1360", "TEXT": "Description of Systems Used for Data Transmission* (An ASA Tutorial)"}
{"DOCID": "1361", "TEXT": "Rectangular Holes in Twelve-Row Punched Cards* (Proposed American Standard)"}
{"DOCID": "1362", "TEXT": "Code Extension in ASCII* (An ASA Tutorial): The American Standard Code for Information Interchange (ASCII) contains a number of control characters associated with the principle of code extension, that is, with the representation of information which cannot be directly represented by means of the characters in the Code.  The manner of use of these characters has not previously been completely described. This paper presents a set of mutually consistent philosophies regarding code extension applications, and suggests a corollary set of doctrines for the application of the code extension characters.  Distinctions are drawn between code extension and such other concepts as \"graphic substitution\" or \"syntactic representation\" which are often used to meet similar requirements.  Also covered are certain topics which are not truly concerned with code extension but which are often linked with it in discussion on code applications. The material in this paper is equally applicable in principle to the (proposed) ISO international 7-bit code for information interchange."}
{"DOCID": "1363", "TEXT": "A General Method of Systematic Interval Computation for Numerical Integration of Initial Value Problems: A procedure is given for continuously computing and monitoring the step size to be used by a self-starting, p-th order numerical integration method to solve an initial value problem.  The procedure uses an estimate of the truncation error to calculate the step size."}
{"DOCID": "1364", "TEXT": "Mathematical Experimentation in Time-Lag Modulation: Equations of the form du/dt = g(u(t),u(h(t))) arise in a number of scientific contexts.  The authors point out some interesting properties of the solution u'(t) = -u(t-1-k*sin(wt))+sin(at).  These properties were obtained by means of numerical solution."}
{"DOCID": "1365", "TEXT": "Eliminating Monotonous Mathematics with FORMAC: The FORMAC (FORmula MAnipulation Compiler) programming system provides a powerful tool for performing mathematical analysis.  It is an extension of FORTRAN IV which permits the use of the computer to perform the tedious algebraic computations that arise in many different fields.  Among the areas in which it has been successfully used are: differentiation of complicated expressions, expansion of truncated power series, solution of simultaneous equations with literal coefficients, nonlinear maximum likelihood estimation, tensor analysis, and generation of the coefficients of equations in Keplerian motion.  These types of analysis-which arose in the solution of specific practical problems in physics, engineering, astronomy, statistics and astronautics-are discussed in the paper.  In addition to its usage for specific problem solutions, FORMAC can also be used to automate the analysis phase in certain production programming. Several such applications are presented."}
{"DOCID": "1366", "TEXT": "Computer Simulation-Discussion of the Technique and Comparison of Languages: The purpose of this paper is to present a comparison of some computer simulation languages and of some of the involved in comparing software packages for digital computers are discussed in Part I.  The issue is obvious: users of digital computers must choose from available languages or write their own.  Substantial costs can occur, particularly in training, implementation and computer time if an inappropriate language is chosen.More and more computer simulation languages are being developed: comparisons and evaluations of existing languages are useful for designers and implementers as well as users.  The second part is devoted to computer simulation and simulation languages.  The computational characteristics of simulation are discussed with  special attention being paid to a distinction between continuous and discrete change models.  Part III presents a detailed comparison of six simulation languages and packages: SIMSCRIPT, CLP, CSL, GASP, CPSS and SOL.  The characteristics of each are summarized in a series of tables.  The implications of this analysis for designers of languages, for users, and for implementers are developed. The conclusion of the paper is that the packages now available for computer simulation offer features which none of the more general-purpose packages do and that analysis of strengths and weaknesses of each suggests ways in which both current and future simulation languages and packages can be improved."}
{"DOCID": "1367", "TEXT": "Character Structure and Character Parity Sense for Parallel-by-Bit Data Communication in ASCII* (Proposed American Standard)"}
{"DOCID": "1368", "TEXT": "Systematic Generation of Hamiltonian Circuits: For a combinatorial matrix which may specify both directed and nondirected arcs, the paper describes a computer program which generates systematically and exhaustively all the Hamiltonian circuits. Specific application is made to the \"traveling salesman\" problem."}
{"DOCID": "1369", "TEXT": "Half Rotations in N-Dimensional Euclidean Space: An iterative procedure is described for determining half rotations in n-dimensional Euclidean space. The method is a variant of the cyclic Jacobi procedure and utilizers elementary plane rotations to obtain the half rotation matrix.  Numerical examples are given."}
{"DOCID": "1370", "TEXT": "Linear Equations, Exact Solutions (Algorithm 290 [F4])"}
{"DOCID": "1371", "TEXT": "Logarithm of Gamma Function (Algorithm 291 [S14])"}
{"DOCID": "1372", "TEXT": "Direct Search (Algorithm 178 [E4])"}
{"DOCID": "1373", "TEXT": "Gamma Function; Gamma Function for Range 1 to 2; Reciprocal Gamma Function to Real Argument; Gamma Function; Logarithm of Gamma Function (Algorithms 34[S14]; 54[S14]; 80[S14]; 221[S14]; 291[S14])"}
{"DOCID": "1374", "TEXT": "Evaluation of Determinant; Determinant Evaluation (Algorithms 41[F3]; 269[F3])"}
{"DOCID": "1375", "TEXT": "Function Minimization (Algorithm 251 [E4])"}
{"DOCID": "1376", "TEXT": "Modified Graeffee Method (Algorithm 256 [C2])"}
{"DOCID": "1377", "TEXT": "Pseudo-Random Numbers (Algorithm 266 [G5])"}
{"DOCID": "1378", "TEXT": "Pseudo-Random Numbers (Algorithm 266 [G5])"}
{"DOCID": "1379", "TEXT": "A Final Solution to the Dangling Else of ALGOL 60 and Related Languages: The dangling else problem consists of a class of potential ambiguities in ALGOL-like conditional statements whose basic form is \"if B1 then if B2 then S1 else S2\" where B1 and B2 are Boolean expressions and S1 and S2 are basic statements.  The difficulty lies in whether to attach the else to the first if or to the second one.  Existing solutions to the problem are either ambiguous or unnecessarily restrictive. Let Sand S1 be statements.  We define S to be closed if \"S else S1\" is not a statement, and to be open if \"S else S1\" is a statement.  Thus an unconditional statement is  a closed statement.  Open and closed conditional statements are defined by syntax equations in such a way as to preserve openness and closure. In each case, an else must always be preceded by a closed statement.  It is shown that the syntax equations are unambiguous, and that may change in the statement types required within the syntax equations would lead to either ambiguity or unnecessary restriction."}
{"DOCID": "1380", "TEXT": "SIMULA-an ALGOL-Based Simulation Language: This paper is an introduction to SIMULA, a programming language designed to provide a systems analyst with unified concepts which facilitate the concise description of discrete event systems.  A system description also serves as a source language simulation program.  SIMULA is an extension of ALGOL 60 in which the most important new concepts is that of quasi-parallel processing."}
{"DOCID": "1381", "TEXT": "Impact of Computers on the Undergraduate Mathematics Curriculum: The use of computers to permit the widespread application of mathematical ideas requiring computation in science and technology is extremely significant for the understanding of our current society.  Student interest in this development is intense and if properly utilized should yield a much better understanding of mathematical concepts as well as the ideas of programming and logical structure which have been introduced into many fields by the use of computers.  The present paper suggests that that portion of the undergraduate mathematical curriculum which is preparation for the use of mathematics by persons who are not professional mathematicians be modified to include the extensions and clarifications which are possible because of computers.  An early introduction to programming is desirable to permit a continuing use of automatic computation to illustrate and clarify mathematical concepts. Following the calculus equation stage an intensive introduction to numerical analysis should be added to the current curriculum.  In addition to providing competence in the mostly used computing techniques, it would permit a more sophisticated utilization of the advanced mathematical ideas associated with complex variables and transform theories."}
{"DOCID": "1382", "TEXT": "Desired Computer Impact on Undergraduate Mathematics: Three matters relating to the theme of the Symposium are discussed here.  The author examines some projections concerning the supply and demand for mathematicians in the United States through the mid-1970s, comments briefly on some of the factors which may influence the professional activities of applied mathematicians over the next several years, and discusses in broad terms how this information may relate to the undergraduate training of mathematicians."}
{"DOCID": "1383", "TEXT": "Implications of the Digital Computer for Education In the Mathematical Sciences: The digital computer has profoundly altered the definition of what is interesting in mathematics. The importance of applied logic in human affairs is changed by the existence of the \"logical engine.\" The result is that one should no longer think in terms of a single discipline of mathematics but in terms of a complex of mathematical sciences."}
{"DOCID": "1384", "TEXT": "Mathematics for Undergraduate Computer Scientists: The mathematical requirements for an undergraduate program in Computer Science are a subject of debate.  The Association for Computing Machinery's Curriculum Committee, however, believes that these requirements are essentially the same as the mathematical content of physical sciences undergraduate programs.  The Committee believes that these requirements should assure the student of a broad mathematical background and should enable him to take a wide variety of courses in other scientific disciplines. The Committee's concern is to develop a solid scientific  approach to Computer Science."}
{"DOCID": "1385", "TEXT": "Computer Technology in Communist China, 1956-1965: Based on information from translations of Communist Chinese news items and periodical literature for the 1965 period, computer technology in China is reviewed under the following headings: (1) initial planning, organization and educational aspects of computer technology and automation; (2) machine development progress: two major specific machines in 1958-59, with Soviet aid; a vacuum in 1960-64 due to the withdrawal of Soviet aid; then presumably all-Chinese-made machines from 1965 to the present; (3) computer applications; (4) the trend of automation: control of production processes rather than data processing; and (5) the \"Yun Ch'ou Hsueh\" (Science of Operation and Programming) campaign of 1958-60, during which an attempt was made to bring concepts such as linear programming to ordinary Chinese workers and peasants.  Communist China is adjudged to have a marginal computer capability, with most of its machines probably being of a binary nature; however, a turning point may have been reached in mid-1965."}
{"DOCID": "1386", "TEXT": "Symbolic Factoring of Polynomials in Several Variables: An algorithm for finding the symbolic factors of a multi-variate polynomial with integer coefficients is presented.  The algorithm is an extension of a technique used by Kronecker in a proof that the prime factoring of any polynomial may be found in a finite number of steps.  The algorithm consists of factoring single-variable instances of the given polynomial by Kronecker's method and introducing the remaining variables by interpolation.  Techniques for implementing the algorithm and several examples are discussed. The algorithm promises sufficient power to be used efficiently in an online system for symbolic mathematics."}
{"DOCID": "1387", "TEXT": "Solution of Systems of Polynomial Equations By Elimination: The elimination procedure as described by Williams has been coded in LISP and FORMAC and used in solving systems of polynomial equations.  It is found that the method is very effective in the case of small systems, where it yields all solutions without the need for initial estimates. The method, by itself, appears in appropriate, however, in the solution of large systems of equation due to the explosive growth in the intermediate equations and the hazards which arise when the coefficients are truncated. A comparison is made with difficulties found in other problems in non-numerical mathematics such as symbolic integration and simplification."}
{"DOCID": "1388", "TEXT": "AUTOMAST: Automatic Mathematical Analysis and Symbolic Translation: A procedure for numerically solving systems of ordinary differential equation is shown to also generate symbolic solutions.  The procedure is based on a finite Taylor series expansion that includes an estimate of the error in the final result.  A computer program is described that reads in a system of such equations and then generates the expansions for all of the dependent variables. The expansions are determined symbolically, hence any non-numeric parameters in the original equations are carried automatically into the final expansions.  Thus the exact influence of any parameters on the problem solution can be easily displayed."}
{"DOCID": "1389", "TEXT": "A Programmer's Description of L^6: Bell Telephone Laboratories' Low-Linked List Language L^6 (pronounced \"L-six\") is a new programming language for list structure manipulations.  It contains many of the facilities which underlie such list processors as IPL, LISP, COMIT ad SNOBOL, but permits the user to get much closer to machine code in order to write faster-running programs, to use storage more efficiently and to build a wider variety of linked data structures."}
{"DOCID": "1390", "TEXT": "CONVERT: A programming language is described which is applicable to problems conveniently described by transformation rules.  By this is meant that patterns may be prescribed, each being associated with a skeleton, so that a series of such pairs may be searched until a pattern is found which matches an expression to be transformed.  The conditions for a match are governed by a code which also allows subexpressions to be identified and eventually substituted into the corresponding skeleton.  The primitive patterns and primitive skeletons are described, as well as the principle which allow their elaboration in to more complicated patterns and skeletons.  The advantages of the language are that it allows one to apply transformation rules to lists and arrays as easily as strings, that both patterns and skeletons may be defined recursively, and that as a consequence programs may be stated quite concisely."}
{"DOCID": "1391", "TEXT": "Computer Experiments in Finite Algebra: A medium-scale programming system is written in MAD and FAP on the IBM 7094 to manipulate some of the objects of modern algebra: finite groups, maps and sets of maps, subsets and sets of subsets, constant integers and truth-values.  Designed to operate in a time-sharing environment, the system can serve as a teacher's aid to the undergraduate student of modern algebra, as well as for the working scientist or engineer wishing to familiarize himself with the subset."}
{"DOCID": "1392", "TEXT": "Experience with FORMAC Algorithm Design: Various facets of the design and implementation of mathematical expression manipulation algorithms are discussed.  Concrete examples are provided by the FORMAC EXPAND and differentiation algorithms, a basic FORMAC utility routine, and an experiment in the extraction of the skeletal structure of an expression. One recurrent theme is the need to avoid excessive intermediate expression swell in order to minimize core storage requirements. Although many details from the FORMAC implementation are presented, an attempt is made to stress principles and ideas of general relevance in the design of algorithms for manipulating mathematical expressions."}
{"DOCID": "1393", "TEXT": "PM, A System for Polynomial Manipulation: PM is an IBM 7094 program system for formal manipulation of polynomials in any number of variables, with integral coefficients unrestricted in size.  Some of the formal operations which can be performed by the system are sums, differences, products, quotients, derivatives, substitutions and greater common divisors.  PM is based on the REFCO III list processing system, which is described and compared with the LISP and SLIP systems.  The PM subroutines for arithmetic of large integers are described as constituting an independently useful subsystem.  PM is compared with the ALPAK system in several respects, including the choice of canonical forms for polynomials.  A new algorithm for polynomial greatest common divisor calculation is mentioned, and exaples are included to illustrate its superiority."}
{"DOCID": "1394", "TEXT": "Computation of Algebraic Properties of Elementary Particle Reactions Using a Digital Computer: A large number of calculations in high-energy elementary particle physics involve the manipulation of complicated algebraic expressions containing both tensor and noncommutative matrix quantities.  Many of these calculations take several months to complete, although the operations involved follow straightforward rules.  In this paper a program is described, which has been developed in LISP for solving such problems. The manner in which these problems are encountered is outlined, and their representation in the computer discussed.  At present, about six months of human work takes less than fifteen minutes on an IBM 7090. Limitations of the present system and future plans are also outlined."}
{"DOCID": "1395", "TEXT": "On the Implementation of AMBIT, A Language for Symbol Manipulation: A brief description is given of the implementation technique for the replacement rule of the AMBIT programming language.  The algorithm for the \"AMBIT scan\" and an example of its application are given.  The algorithm is applicable to other members of the family of string transformation languages of which AMBIT is a member, and it provides a rationale for the design of the AMBIT language."}
{"DOCID": "1396", "TEXT": "Survey of Formula Manipulation: The field of formula manipulation is surveyed, with particular attention to the specific capabilities of differentiation, integration and the supporting capabilities of simplification, displays and input/output editing, and precision arithmetic.  General systems-both batch and online-are described.  Finally, some programs to solve specific applications are discussed."}
{"DOCID": "1397", "TEXT": "Proceedings of the ACM Symposium on Symbolic and Algebraic Manipulation: The ACM Symposium on Symbolic and Algebraic Manipulation brought together over four hundred people interested in programming languages designed for manipulation of algebraic formulas and symbol strings, in their applications, and in algorithms for their implementation.  Twenty-eight papers were presented, followed by a lively panel discussion of future directions.  Evening meetings were arranged for several interest groups.  The conference was sponsored by the ACM Special Interest Committee on Symbolic and Algebraic Manipulation.  The program committee consisted of Chairman Jean E. Sammet, Paul Abrahams, Thomas E. Cheatham, Max Goldstein, and Douglas Mcllroy. Conference arrangements were made by Lewis C. Clapp, Daniel Bobrow and James H. Griesmer.-Robert W. Floyd, Editor"}
{"DOCID": "1398", "TEXT": "Robot Data Screening: A Solution to Multivariate Type Problems in the Biological and Social Sciences: A new approach is outlined toward the solution of the type of multivariate problem that is found usually in the biological and social sciences as well as in medicine.  This approach uses a \"logical\" rather than a \"statistical\" criterion by which variables are grouped into a deterministic model.  Algorithm are developed by which some variables are kept on for further analysis while others are eliminated. Criteria for the acceptance of a variable as well as the termination of the searching process are derived from information theory."}
{"DOCID": "1399", "TEXT": "On Top-to-Bottom Recognition and Left Recursion: A procedure is given for obtaining structural descriptions in a context-free grammar by performing the recognition according to a strongly equivalent, left-recursion-freegrammar. The effect of allowing null strings in the rewriting rules is discussed."}
{"DOCID": "1400", "TEXT": "Free-Text Inputs to Utility Routines: Through the use of some rather simple techniques, it is frequently possible to produce a program which will accept free-text inputs.  The techniques are discussed and related to a general tape manipulation routine."}
{"DOCID": "1401", "TEXT": "Quasilinearization and the Calculation of Eigenvalues: Several eigenvalue problems for systems of ordinary differential equations are considered. They are resolved computationally using the quasilinerization technique, a quadratically convergent successive approximation scheme related to the Newton-Raphson-Kantorovich method."}
{"DOCID": "1402", "TEXT": "Partial Step Integration: A partial step integration equation is derived for use with the Adams or Adams-Bashforth method of integration of differential equations.  This method of obtaining functional values at points intermediate to the integration points yields accuracy comparable to the integration and does not require storing of additional information as in interpolation methods."}
{"DOCID": "1403", "TEXT": "A Method for Finding the m Smallest Values of a Monotonic Function Defined on Ordered Sets of Positive Integers: The minimum value of a monotonic increasing function defined on a partially ordered set S is assumed on the set of minimal points of S.  This observation is used to devise an efficient method for finding the m smallest functional values of monotonic functions defined on ordered pairs of positive integers.  The method is easily extended to include monotonic functions defined on ordered n-tuples. Included is a FORTRAN program which was written to implement the procedure for a certain important case."}
{"DOCID": "1404", "TEXT": "Computational Aspects of Multiple Covariance Analysis on a Multifactor Structure: The computational procedure for the analysis of multiple covariance in statistics is discussed with reference to the analysis of variance.  A special operator calculus developed by Hartly for programming analysis of variance for multifactor experiments is extended to cover the analysis of covariance.  This extension is accomplished by utilizing the connection between the analysis of covariance and the analysis of variance and by introducing a new operator.  The results are illustrated by a numerical example for analysis of covariance, in which the basic computations are shown to be carried out by an analysis-of-variance program."}
{"DOCID": "1405", "TEXT": "Matrix Triangulation with Integer Arithmetic (Algorithm 287 [F1])"}
{"DOCID": "1406", "TEXT": "Solution of simultaneous Linear Diophantine Equations (Algorithm 288 [F4])"}
{"DOCID": "1407", "TEXT": "Confidence Interval for a Ratio (Algorithm 289 [G1])"}
{"DOCID": "1408", "TEXT": "The Eschenbach Drum Scheme: The prime function of a drum, operating in real time, is to perform accesses quickly.  The usual means for increasing this capacity is to incorporate engineering or hardware improvements.  In this paper the problem is attacked not by changing the drum, but rather by modifying the manner in which it operates.  At the outset, a drum is given a functional definition.  Then a simple design scheme (Eschenbach) is introduced which enormously increases the rate of accessing for drums so defined.  This is shown to enable a system to perform a job by employing fewer or less expensive drums.  It is suggested that although the design scheme has a specific use, the method underlying it has more general applicability.  The question of the efficacy of the drum scheme is then raised.  To deal with this, a standard of efficiency is developed in light of realistic real-time circumstances.  The drum scheme is then modelled in a manner which permits it to be analyzed as a problem in queueing theory. Thus one is enabled to ascertain whether the drum scheme is efficient enough for its application.  Again, whereas the analysis of the drum scheme has a specific use, the methods underlying it have more general applicability."}
{"DOCID": "1409", "TEXT": "NEBULA: A Digital Computer Using a 20 Mc Glass Delay Line Memory: Oregon State University has designed and constructed a medium-speed serial digital computer using glass delay lines circulating at 22 Mc as memory. The design objectives as originally conceived in a special seminar were: (1) to be a research project in computer design; (2) to be usable as an educational machine;and (3) to have easily modifiable hardware for basic research in computer systems design.  An unusual arrangement of information within the 22 Mc memory allows a simple interface with the 340 Kc arithmetic unit, which results in an effective zero latency time and provides possibilities for an associative memory.  The arithmetic unit has a command structure similar to large parallel machines, and uses flip-flop arithmetic and control registers throughout.  All hardware development has been aimed toward the concept of easy modification, elaborate console controls for effective man-machine interaction and low cost."}
{"DOCID": "1410", "TEXT": "Interarrival Statistics for Time Sharing Systems: The optimization of time-shared system performance requires the description of the stochastic processes governing the user inputs and the program activity. This paper provides a statistical description of the user input process in the SDC-ARPA general-purpose Time-Sharing System (TSS).  The input process is assumed to be stationary, and to be defined by the interarrival time distribution.  The data obtained appear to justify satisfactorily the common assumption that the interarrival times are serially independent. The data do not appear to justify, except as a very rough approximation, the usual assumption off an exponential distribution for interarrival time.  A much more satisfactory approximation to the data can be obtained with a biphase or triphase hyperexponential distribution."}
{"DOCID": "1411", "TEXT": "Comparison of Several Algorithms for Computation of Means, Standard Deviations and Correlation Coefficients: Several algorithms for computation of basic statistics are compared by their performance on systematically generated test data.  The statistics calculated were the mean, standard deviation and correlation coefficient.  For each statistic, the algorithm included the usual computing formulas, correction due to an accumulated error term, and a recursive computation of the current value of the statistic. The usual computing formulas were also evaluated in double precision.  Large errors were noted for some calculation using the usual computing formulas.  The most reliable technique was correction of the initial estimate by use of an accumulated error term.  To eliminate the need for making two passes on the data, it was suggested that the initial estimate of the mean be obtained from a subset of the data."}
{"DOCID": "1412", "TEXT": "The Banking Information System Concept: Most large commercial banks have progressed to the  point where their major accounting applications have been automated and more sophisticated usage of data processing equipment is being sought.  This, coupled with the availability of equipment well suited to real-time, direct access processing,has led to development within some banks of the central file of data base approach toward a banking information system.  The banking information system now serves the two-fold purpose of providing real-time responses to inquires about individual account stasus and providing more complex combinations of information for management use.  Both kinds of processing draw upon a common store of data contained in the direct access central file.  This data base includes indexes which facilitate cross referencing of account information so that all relationships between bank and customer may be discerned.  In introducing the banking information system concept, a gradual approach to account cross-referencing and file conversion is most prudent. Generally, this system must interface with other computer applications already existing within the bank."}
{"DOCID": "1413", "TEXT": "A Vision of Technology and Education: Educational technology is currently quite fashionable.  Here, as in many other branches or aspects of technology, changes possible in the next generation or two are now known as ideas, discoveries or inventions.  The unknown is whether the potential will become the actual and, if so, on what time scale.  This ignorance stems largely from ignorance about the social response to potential technological change.  The object of this paper is to present a vision of potential educational technology and to raise questions about the modes of social response and adaptation likely to be evoked by such a vision."}
{"DOCID": "1414", "TEXT": "Twelve-Row Punched-Card Code for Information Interchange* (Proposed American Standard)"}
{"DOCID": "1415", "TEXT": "Automatic Derivation of Microsentences: The decomposition of long complex English sentences into shorter kernel-like constituent sentences (microsentences)has often been suggested as an avenue toward conducting automatic retrieval of natural language messages.  To explore the prospects of such a step, the authors attempted in 1963 to prepare a general program for deriving microsentences from longer sentences that had been syntactically analyzed by the Harvard Multipath Analysis Program.  The basic idea was to extract the subject, verb and object (if any) of each clause and to reassemble these materials into a grammatical microsentence.  A program is described in this paper, which was designed to operate on the tree structure output of the analyzer, and the microsentences that were produced are exhibited. The authors conclude that while microsentences of the quality achieved do not open up immediate prospects for improving the performance of automatic message retrieval systems, they may have practical value in man-machine systems using human monitors to select the preferred syntactic interpretation of a sentence."}
{"DOCID": "1416", "TEXT": "A Fortran Technique for Simplifying Input to Report Generators: Typical report generators allow the production of standard forms when tabulating a magnetic tape file; the extraction of nonstandard sets of information, with suitable annotation, involves troublesome forms design.  A method of information extraction involving the calculation of suitable FORTRAN FORMAT statements, which combats this problem, is described."}
{"DOCID": "1417", "TEXT": "Economies of Scale and the IBM System/360: Cost functions among five System/360 models are analyzed through examinations of instruction times, program kernels and a \"typical\" instruction mix. Comparisons are made between the data developed here and Grosch's Law which seems to be applicable to much of the data.  Sizable economies of scale are unquestionably present in computing equipment."}
{"DOCID": "1418", "TEXT": "Examination Scheduling (Algorithm 286 [ZH])"}
{"DOCID": "1419", "TEXT": "Chebyshev Quadrature (Algorithm 279 [D1])"}
{"DOCID": "1420", "TEXT": "A New Uniform Pseudorandom Number Generator: A new multiplicative congruential pseudorandom number generator is discussed, in which the modulus is the largest prime within accumulator capacity and the multiplier is a primitive root of that prime.  This generator passes the usual statistical tests and in addition the least significant bits appear to be as random as the most significant bits-a property which generators having modulus 2^k do not possess."}
{"DOCID": "1421", "TEXT": "A Contribution to the Development of ALGOL: A programming language similar in many respects to ALGOL 60, but incorporating a large number of improvements based on six years experience with that language, is described in detail.  Part I consists of an introduction to the new language and a summary of the changes made to ALGOL 60, together with a discussion of the motives behind there visions.  Part II is a rigorous definition of the proposed language. Part III describes a set of proposed standard procedures to be used with the language, including facilities for input/output."}
{"DOCID": "1422", "TEXT": "Eleven-Sixteenths Inch Perforated Paper Tape (Proposed American Standard)"}
{"DOCID": "1423", "TEXT": "A Simple Algorithm for Computing the Generalized Inverse of a Matrix: The generalized inverse of a matrix is important in analysis because it provides an extension of the concept of an inverse which applies to all matrices. It also has many applications in numerical analysis, but it is not widely used because the existing algorithms are fairly complicated and require considerable storage space.  A simple extension has been found to the conventional orthogonalization method for inverting non-singular matrices, which gives the generalized inverse with little extra effort and with no additional storage requirements.  The algorithm gives the generalized inverse for any m by n matrix A, including the special case when m+n and A is non-singular and the case when m>n and rank(A) = n.  In the first case the algorithm gives the ordinary inverse of A.  In the second case the algorithm yields the ordinary least squares transformation matrix INV(A'A)A' and has the advantage of avoiding the loss of significance which results in forming the product A'A explicitly."}
{"DOCID": "1424", "TEXT": "Automatic Analysis of Electronic Digital Circuits Using List Processing: A mapping from black diagrams of digital circuits to list structures is described, together with a list processing program written for the Control Data 3600 which uses this mapping to automatically carry out circuit analysis."}
{"DOCID": "1425", "TEXT": "Flow Diagrams, Turing Machines And Languages With Only Two Formation Rules: In the first part of the paper, flow diagrams are introduced to represent inter al. mappings of a set into itself.  Although not every diagram is decomposable into a finite number of given base diagrams, this becomes true at a semantical level due to a suitable extension of the given set and of the basic mappings defined in it.  Two normalization methods of flow diagrams are given.  The first has three base diagrams; the second, only two.  In the second part of the paper, the second method is applied to the theory of Turing machines.  With every Turing machine provided with a two-way half-tape, there is associated a similar machine, doing essentially the same job, but working on a tape obtained from the first one by interspersing alternate blank squares. The new machine belongs to the family, elsewhere introduced, generated by composition and iteration from the two machines L and R.  That family is a proper subfamily of the whole family of Turing machines."}
{"DOCID": "1426", "TEXT": "A Simulation of Hospital Admission Policy: A study is described which simulates different admission policies of a large specialized hospital. The objective is to determine better policies for stabilization of admission and census rates while maintaining a reasonably full hospital.  There types of policies were examined: admission based on percentages of discharge rates, discharge rates plus or minus a constant, and fixed authorizations independent of discharge rates.  The last type policy produced more stable simulated results, and when put into practice, improvements were realized."}
{"DOCID": "1427", "TEXT": "Simulation of Radioisotope Scans by Computer: In radioisotope scanning, a field which is assuming increasing importance in medical diagnosis, the scan is a two-dimensional pattern made up of dots. Areas of increased source activity are represented on the scan by areas of increased dot density.  To study the output of scanners with various characteristics, a program which simulates radioisotope scans has been written  for a PDP-1 computer with auxiliary disk storage and cathode ray tube display.  Past and present research using the output of the simulator has shown the flexibility of the system to be important. The structure of this program can be useful in the simulation of the output of any quantum-limited system."}
{"DOCID": "1428", "TEXT": "SHOCK III, A Computer System As an Aid in the Management of Critically III Patients: SHOCK III, an online digital computer system to assist the physician, nurse and paramedical personnel in monitoring and reporting on critically ill patients, is described."}
{"DOCID": "1429", "TEXT": "Matrix Reduction Using the Hungarian Method For The Generation of School Timetables: The application of Kuhn's Hungarian Method to the problem of matrix reduction as needed in Gotlieb's method for timetable generation is described. The method is suited to both hand and computer calculation.  Devices to improve the efficiency of the basic algorithm are discussed."}
{"DOCID": "1430", "TEXT": "Multiple Precision Floating-Point Conversion from Decimal-to-Binary and Vice Versa: Decimal-to-binary and binary-to-decimal floating-point conversion is often performed by using a table of the powers 10^i, (ia positive integer) for converting from base 10 to base 2, and by using a table of the coefficient of a polynomial approximation of 10^x, (0<=x<1) for converting from base 2 to base 10.  These tables occupy a large storage region in the case of a nonsingle precision conversion. This paper shows that a single small table suffices for a floating-point conversion from decimal to binary, and vice versa, in any useful precision."}
{"DOCID": "1431", "TEXT": "On a Storage Mapping Function For Data Structures: Some basic facts about certain data structures are reviewed and an efficient algorithm is presented for constructing a storage mapping function for a structure from the structure's definition."}
{"DOCID": "1432", "TEXT": "Incorporation of Nonstandard Input/Output Devices into FORTRAN Systems: A FORTRAN system may readily be modified to handle input/output with nonstandard media on the same basis on which it handles the standard media.  This is done by providing a character-handling subroutine suited to the nonstandard medium and arranged to be called by an otherwise unused output statement type or unit number. This method was used to control output of alphanumeric information on a digital graph plotter."}
{"DOCID": "1433", "TEXT": "A Note on Linear Programming Algorithm Design: A Combinatorial Problem: As linear programming models grow bigger and bigger in size, much actual data that must be memorized is often put on magnetic tape or disk, and consequently there is an improportionality fast rise in the consumption of computer time.To cut down this expense, an ever increasing effort is made to design more efficient algorithms.  This paper is meant to support the effort.  It is attempted to find some characteristics of the way a pivot column is found.  The number of repetitions of a certain transfer of data from tape to core memory is considered. After some simplification, the problem is restated in a general way.  The generating function of the probability distribution and the moment generating function of the number of repetitions is found.  Asymptotic formulas are given for the moments using a result from a paper of S. Narumi [1].  The results may be applied to write very efficient routines that search for an extreme value in a table.  Formulas provide a means of calculating the computer timings in this case."}
{"DOCID": "1434", "TEXT": "A Monte Carlo Algorithm for Assigning Students to Classes: A technique of random choice is illustrated by application to the problem of assigning students to a fixed schedule of courses.  Using the technique it is possible to reduce or eliminate difficulties that result when a popular section is filled and closed before all students requesting and requiring it have been scheduled.  The effectiveness of automatic scheduling is retained without loss of the students privilege of picking favorite instructors."}
{"DOCID": "1435", "TEXT": "Design of Computer Simulation Experiments for Industrial Systems: The aim of this paper is to provide background information on the existing literature on experimental design techniques which may be applicable to the design of computer simulation experiments for industrial systems.  Although major emphasis is placed on analysis of variance techniques, three other techniques of data analysis are considered-multiple ranking procedures, sequential sampling and spectral analysis. The paper treats four specific experimental design problems and several techniques for solving them. The four experimental design problems are: (1) the problem of stochastic convergence, (2) the problem of factor selection, (3) the problem of motive and (4) the many response problem."}
{"DOCID": "1436", "TEXT": "Interchange of Two Blocks of Data (Algorithm 284 [K2])"}
{"DOCID": "1437", "TEXT": "The Mutual Primal-Dual Method (Algorithm 285 [H])"}
{"DOCID": "1438", "TEXT": "A Method for Locating Zeros of Complex Functions: A method for computing the index, or winding number, is developed and applied to the problem of finding zeros of functions from the plane into the plane."}
{"DOCID": "1439", "TEXT": "Mechanization of the Curve Fitting Process: DATAN: A process for fitting a curve to approximate data and the problem it creates for the engineer-programmer is defined.  An approach has also been defined and a system has been written for the SRU 1107 to mechanize a major portion of this process.  The techniques developed to accomplish the mechanization are largely empirical, and are dependent for their information only on the actual data points."}
{"DOCID": "1440", "TEXT": "Starting Approximations for Square Root Calculation on IBM System/360: Several starting approximations for square root calculation by Newton's method are presented in a form to facilitate their use in IBM System/360 square root routines.  These approximations include several for the range [1/16, 1], which is the interval of primary interest on IBM System/360."}
{"DOCID": "1441", "TEXT": "Methods of Numerical Integration Applied to a System Having Trivial Function Evaluations: A study has been made to determine which methods of numerical integration require the least computation time for a given amount of truncation error when applied to a particular system of ordinary differential equations where function evaluations are relatively trivial.  Recent methods due to Butcher and Gear are compared with classic Runge-Kutta, Kutta-Nystrom and Adams methods.  Some of the newer one-step methods due to Butcher are found to be slightly superior, but no one method is found to have any great advantage over the others in the application to this particular problem."}
{"DOCID": "1442", "TEXT": "Recorded Magnetic Tape For Information Interchange (800 CPI, NRZI)* (Proposed American Standard)"}
{"DOCID": "1443", "TEXT": "A Method for Finding the Least Squares Estimate of the Intersection Point of Two Helices in Space: When the helical trajectories of two charged particles moving away from a common point in a magnetic field are reconstructed from measurements on the tracks, the reconstructed tracks are perturbed by measurement and other errors and do not, in general, intersect.  A method is given for adjusting the reconstructed tracks in a least squares manner so that they do intersect."}
{"DOCID": "1444", "TEXT": "An Algorithm for Generating Projective Reduction Formulas for Matrix Elements of Many-Electron Wavefunctions: An ALGOL procedure is given for automatically generating formulas for matrix elements arising in the variational solution of the Schrodinger equation for many-electron systems."}
{"DOCID": "1445", "TEXT": "Use of the Computer to Teach Introductory Statistics: It has always been obvious that the aid to calculation offered by the computer forces a change in the curricula of mathematics, statistics, physics, engineering and other courses.  Not so obvious are the many pedagogic aids the computer can offer in teaching the subject matter.  The possibilities of giving the student a better technical as well as conceptual understanding of statistics were explored for a number of years at the College of Medicine of the University of Cincinnati and are reported here."}
{"DOCID": "1446", "TEXT": "Chebyshev Quadrature (Algorithm 279 [D1])"}
{"DOCID": "1447", "TEXT": "Abscissas and Weights for Gregory Quadrature [D1])"}
{"DOCID": "1448", "TEXT": "Abscissas and Weights for Romberg Quadrature (Algorithm 281 [D1])"}
{"DOCID": "1449", "TEXT": "Derivatives (Algorithm 282 [S22])"}
{"DOCID": "1450", "TEXT": "Simultaneous Displacement of Polynomial Roots if Real and Simple (Algorithm 283 [C2])"}
{"DOCID": "1451", "TEXT": "Runge-Kutta Integration (Algorithm 9 [D2])"}
{"DOCID": "1452", "TEXT": "Kutta-Merson (Algorithm 218 [D2]"}
{"DOCID": "1453", "TEXT": "A Nonrecursive Method of Syntax Specification: The use of the Kleene regular expression notation for describing algebraic language syntax, in particular of ALGOL, is described in this paper. A FORTRAN II computer program for carrying out the elimination algorithm of Gorn,similar to Gaussian elimination for linear systems of algebraic equations, is described.  This was applied to numerous smaller languages, including some sublanguage of ALGOL. A hand calculation result of the application of the algorithm to all of ALGOL is given, thus expressing the Revised ALGOL 1960 syntax in completely nonrecursive terms, as far as its context-free portion is concerned.  This description in many ways is far more intuitively understood than the previous recursive description, it is suggested.  The paper also includes results of the machine program, which does not include a simplification algorithm."}
{"DOCID": "1454", "TEXT": "A Simple User-Oriented Compiler Source Language for Programming Automatic Test Equipment: For the nonprogrammer, difficulty in using a language increases rapidly with the number of nonproblem-oriented conventions.  A simple language, even if inelegant, which considers the user's background as part of the problem may be more effective than a source language containing subtle and more powerful capabilities.  The language described in this paper is used to write computer programs which test electronic equipment.  Because this testing process contains few complex ideas, there is little need for the elegance and redundancy of a highly syntax-oriented language. A simple and direct language will suffice for the problem.  The eventual users of this language are military depot personnel who cannot he expected to have computer programming skill or significant programming training.  For this nonprogramming-oriented user, it was essential to create a language using familiar engineering statements; programming-oriented conventions would have unnecessarily complicated his task."}
{"DOCID": "1455", "TEXT": "TRAC, A Procedure-Describing Language for the Reactive Typewriter: A description of the TRAC (Text Reckoning And Compiling) language and processing algorithm is given.  The TRAC language was developed as the basis of a software package for the reactive typewriter. In the TRAC language, one can write procedures for accepting, naming and storing any character string from the typewriter; for modifying any string in any way; for treating any string at any time as an executable procedure, or as a name, or as text; and for printing out any string.  The TRAC language is based upon an extension and generalization to character strings of the programming concept of the \"macro.\"  Through the ability of TRAC to accept and store definitions of procedures, the capabilities of the language can be indefinitely extended, and can deal with character strings, integers and Boolean vector variables."}
{"DOCID": "1456", "TEXT": "Storage and Retrieval of Aspects of Meaning in Directed Graph Structures: An experimental system that uses LISP to make a conceptual dictionary is described.  The dictionary associates with each English word the syntactic information, definitional material, and references to the contexts in which it has been used to define other words. Such relations as class inclusion, possession, and active or passive actions are used as definitional material.  The resulting structure serves as a powerful vehicle for research on the logic of question answering. Examples of methods of inputting information and answering simple English questions are given.  An important conclusion is that, although LISP and other list processing languages are ideally suited for producing complex associative structures, they are inadequate vehicles for language processing on any large scale-at east until they can use auxiliary memory as a continuous extension of core memory."}
{"DOCID": "1457", "TEXT": "Data Manipulation and Programming Problems in Automatic Information Retrieval: Automatic information retrieval programs require the manipulation of a variety of different data structures, including linear text, sparse matrices, and tree or list structures.  The main data manipulations to be performed in automatic information systems are first briefly reviewed.  A variety of data representations which have been used to describe structured information are then examined, and the characteristics of various processing languages are outlined in the light of the procedures requiring implementation.  Advantages of these programming languages for the retrieval application are examined, and suggestions are made for the design of programming facilities to aid in information retrieval."}
{"DOCID": "1458", "TEXT": "Online Programming: When the transition has been made from off line to online programming, there are a number of changes in the working conditions noted.  These changes in the environment make necessary corresponding changes in the processes related to producing and checking out programs.  In the main, it it not the programming language itself which must be changed to provide a facility for the online user; it is the system surrounding the programming language.  In this paper the online environment and its effect on programming are discussed."}
{"DOCID": "1459", "TEXT": "Requirements for Real-Time Languages: Real-time languages have different requirements from other programming languages because of the special nature of their applications, the environment in which their object programs are executed and the environment in which they may be compiled.  It may not be the language extensions that ultimately advance developments in the field.  Progress may be made by attacking the special compiling and executing system problems that must be solved."}
{"DOCID": "1460", "TEXT": "Evolution of the Meta-Assembly Program: A generalized assembler called a \"meta-assembler\" is described.  The meta-assembler is defined and factors which contributed to its evolution are presented. How a meta-assembler is made to function as an assembly program is described. Finally, the implication of meta-assemblers on compiler design is discussed."}
{"DOCID": "1461", "TEXT": "Discussion Summary on Operating Systems"}
{"DOCID": "1462", "TEXT": "Multilevel Operating Systems: The Basic software for all newer computers is built on the well-established need for standard operating systems. This implies that all applications-no matter how large, complex or time consuming-must operate under (or, more precisely, on top of) the standard system.  Large applications require supervisory monitors which handle problems similar to those of the operating systems, but at a different level. Sometimes, still a third or even a fourth such level is required or desirable.  This leads naturally to the concept of multilevel systems-similar vertically, but different horizontally.  Proper division of responsibility between levels leads to greater efficiency and less logical complexity, while actually enhancing capability."}
{"DOCID": "1463", "TEXT": "More on Extensible Machines: One of the most salient characteristics of extensible machines (EM) is the facility for providing system control over program-to-program and program-to-data linkage (e.g., address connection).  It is the intent of this paper to expand and clarify the remarks concerning program-to-program and program-to-data linkage that were embodied in the authors' previous paper on the EM concepts, and to, finally, trace the employment of linkage mechanisms through various levels of programming languages."}
{"DOCID": "1464", "TEXT": "An ALGOL Compiler: Construction and Use in Relation to an Elaborate Operating System: An ALGOL translator has been prepared and integrated into the IBSYS Operating System.  Assembly and \"go\" features of IBSYS permit immediate execution with optional listings, decks and debugging information. Using the chain feature of IBSYS, links written in MAP or FORTRAN as well as ALGOL may be called by the ALGOL main program.  In addition, procedures coded in MAP may be included in any ALGOL program. Although assembly plus loading time exceeds compilation time, the total time is satisfactory and the user gets ease and facility which are fully compensating."}
{"DOCID": "1465", "TEXT": "Program Translation Viewed as a General Data Processing Problem: Efficiency dictates that the overall effectiveness of a compiler be increased by all means available.  For a compiler to have a substantial useful life it needs a clear logical structure, reliability and sound data processing techniques.  A compiler must be based on fixed conventions to preserve efficiency and reliability; empty options and default conventions violate this dictum.  Use of structure to associate various parts of a program and economy of features promote clarity and reliability."}
{"DOCID": "1466", "TEXT": "Discussion Summary on Graphical Languages"}
{"DOCID": "1467", "TEXT": "A Graphical ServiceSystem With Variable Syntax: Man-machine interaction in many fields of endeavor should be greatly facilitated in the near future through the use of interactive graphical languages. To provide a variety of display scope communication procedures, a Graphic Service system which functions as a generalized graphical language translator, is being developed to aid the definition as well as the use of new graphical languages."}
{"DOCID": "1468", "TEXT": "Syntax-Directed Interpretation of Classes of Pictures: A descriptive scheme for classes of pictures based on labeling techniques using parallel processing algorithms was proposed by the author some years ago. Since then much work has been done in applying this to bubble chamber pictures.  The parallel processing simulator, originally written for an IBM 7094 system, has now been rewritten for a CDC 3600 system. This paper descriptive models by considering their specific application to bubble chamber pictures.  How the description generated in this phase can be embedded in a larger \"conversation\" program is explained by means of a certain specific example that has been worked out.  A partial generative grammar for \"handwritten\" English letters is given, as are also a few computer-generated outputs using this grammar and the parallel processing simulator mentioned earlier."}
{"DOCID": "1469", "TEXT": "The Next 700 Programming Languages: A family of unimplemented computing languages is described that is intended to span differences of application area by a unified framework.  This framework dictates the rules about the uses of user-coined names, and the conventions about characterizing functional relationships.  Within this framework the design of a specific language splits into two independent parts.  One is the choice of written appearances of programs (or more generally, their physical representation). The other is the choice of the abstract entities (such as numbers, character-strings, lists of them, functional relations among them) that can be referred to in the language.  The system is biased towards \"expressions\" rather than \"statements.\" It includes a nonprocedural(purely functional) subsystem that aims to expand the class of users' needs that can be met by a single print-instruction, without sacrificing the important properties that make conventional right-hand-side expressions easy to construct and understand."}
{"DOCID": "1470", "TEXT": "The Structure of Programming Languages: The following are identified as major components of every programming language: (1) the elementary program statement, (2) mechanisms for linking elementary statements together, (3) the means by which a program can obtain data inputs.  Several alternative forms of each of these components are described, compared and evaluated.  Many examples, frequently from list processing languages, illustrate the forms described.  Elementary program statements usually take the form of commands, requirements, or implicit specifications.  A command is an imperative statement that commands the action to be taken.  A requirement describes the effect to be achieved without saying anything about the actions to be taken.  An implicit specification is similar to a requirement, but the programmer must understand what actions will be taken to achieve the desired effect.  Subroutines may be entered explicitly, by execute call, or by function composition.  Explicitly called subroutines generally require special linkage conventions.  An execute subroutine call is syntactically indistinguishable from a basic instruction of the programming language. Function composition is a convenient alternative to the explicit call.  The three principal ways of getting inputs for routines are (1) by referring to the data itself, (2) by referring to the data by a \"name\", and (3) by referring to it implicitly by means of variables or functions.  Names are useful entry points into permanent data structures, but can be error-causing distractions in other contexts. The author discusses advantages, disadvantages, and factors influencing the choice of a form of component for a language.   He concludes by suggesting the evolution of programming languages toward one which will permit all the most convenient ways of structuring programs, organizing systems, and referencing data."}
{"DOCID": "1471", "TEXT": "Programming Semantics for Multiprogrammed computations: The semantics are defined for a number of meta-instructions which perform operation essential to the writing of programs in multiprogrammed computer systems.  These meta-instructions relate to parallel processing, protection of separate computations, program debugging, and the sharing among users of memory segments and other computing objects, the names of which are hierarchically structured.  The language sophistication contemplated is midway between an assembly language and an advanced algebraic language."}
{"DOCID": "1472", "TEXT": "Description of a High Capacity, Fast Turnaround University Computing Center: The operating system for the UNIVAC 1107 at Case Institute is reviewed.  The system is of interest because of the low turnaround times achieved, the high throughput achieved and the lack of an operating staff.  Turnaround times below 5 minutes and job volume above 75,000 per quarter year one reported."}
{"DOCID": "1473", "TEXT": "The Stability of the Fourth Order Runge-Kutta Method for the Solution of Systems of Differential Equations: The problem of the region of stability of the fourth order-Runge-Kutta method for the solution of systems of differential equations is studied.  This region can be characterized by means of linear transformation but can not be given in a closed form. In the paper, this region is determined by the electronic digital computer Z22."}
{"DOCID": "1474", "TEXT": "Tests of Probabilistic Models for Propagation of Roundoff Errors: In any prolonged computation it is generally assumed that the accumulated effect of roundoff errors is in some sense statistical.  The purpose of this paper is to give precise descriptions of certain probabilistic models for roundoff error, and then to describe a series of experiments for testing the validity of these models.  It is concluded that the models are in general very good.  Discrepancies are both rare and mild.  The test techniques can also be used to experiment with various types of special arithmetic."}
{"DOCID": "1475", "TEXT": "Dribble Posting a Master File: Many business applications employ sequential magnetic tape rather than random-access storage techniques to process a very small number of transactions against a voluminous master file.  In such situations, it may prove economical to avoid creating a new master file during each updating run by producing instead a dribble ledger containing only those master file accounts which have experienced activity."}
{"DOCID": "1476", "TEXT": "Control Procedures for Data Communication-An ASA Progress Report: Sectional Committee X.3 of the American Standards Association, has charged one of its task groups, X3.3.4, with the responsibility to \"Define and specify functional control requirements and characteristics governing the operation of digital data generating and receiving systems interconnected by communication system.\"  This effort is primarily directed toward systems employing the American Standard Code for Information Interchange (ASCII).  This paper represents a progress report on the work of this group toward a proposal for national and international standardization in the field of control procedures.  It describes both the old and new work of the task group.  The new work is presented in detail, while the work that has been presented in earlier papers [\"Control Procedures for Data Communication,\" Task Group document X3.3.4/44, May 1964: \"Transparent-Mode Control Procedures for Data Communication,\" Task Group document X3.3.4/58, December, 1964: Comm. ACM 8 (Apr. 1965), 203-206; \"Control Procedures for Data Communications,\" Task Group document X3.3.4/60, March, 1965] is retained here in summary form.  Many of the concepts and principles described herein have been submitted to the International Organization for Standardization via earlier papers and are now embodied in working papers of that organization."}
{"DOCID": "1477", "TEXT": "EULER: A Generalization of ALGOL, and its Formal Definition: Part II*"}
{"DOCID": "1478", "TEXT": "Exponential Curve Fit (Algorithm 275 [E2])"}
{"DOCID": "1479", "TEXT": "Constrained Exponential Curve Fit (Algorithm 276 [E2])"}
{"DOCID": "1480", "TEXT": "Computation of Chebyshev Series Coefficients (Algorithm 277[C6])"}
{"DOCID": "1481", "TEXT": "Graph Plotter (Algorithm 278 [J6])"}
{"DOCID": "1482", "TEXT": "BUGSYS: A Programming System for Picture Processing-Not for Debugging: BUGSYS is a picture processing and measuring system that depends upon a pictorial input to the computer's memory.  BUGSYS can be used for many types of applications.  In particular, the authors have used the system for the analysis of linear graphs. The main concept of the system is the use of a collection of programmable pointers, which are visualized as a family of \"bugs.\""}
{"DOCID": "1483", "TEXT": "A Comparison of the FORTRAN Language Implementation for Several Computers: A feature-by-feature comparison is made of five different implementations of FORTRAN IV representing three different manufacturers.  A table is constructed showing, where possible, the use of each feature in each implementation.  Only those items which are different from, or have been added to FORTRAN II are shown."}
{"DOCID": "1484", "TEXT": "A Language for Describing the Functions of Synchronous Systems*: Before the design of a system is started, the exact function desired of it should be specified. It is suggested that a computer-oriented language be used for this purpose.  The inadequacies of the standard programming languages for the description of systems are discussed, and a dialect of ALGOL which is suitable for describing synchronous systems is introduced. These descriptions can be used for simulation and automatic design of the system described, in addition to communicating system specifications."}
{"DOCID": "1485", "TEXT": "The Structure of Programming Languages: In this paper the major components of every programming language are identified as: (1) the elementary program statement, (2) mechanisms for linking elementary statements together, (3) the means by which a program can obtain data inputs.  Several alternative forms of each of these components are also described, compared and evaluated.  Many examples, frequently from list processing languages, illustrate the forms described.  The advantages, disadvantages and factors influencing the choice of a form of component for a language are discussed, and the paper concludes with the suggestion that programming languages evolve toward one which will permit all the most convenient ways of structuring programs, organizing systems and referencing data."}
{"DOCID": "1486", "TEXT": "A Reprogramming Machine: In this paper a description is given of a model programming system which is directed by a programming language and has a library for storing the user's items. Rules are given for transforming programs written in the language and for rearranging the items in the library so that they share their common parts. Some speculations are made about how the mechanical detection of common parts or patterns of library items could help a user to solve his problems, and about the relationships between the behavior of the reprogramming machine and human intelligent behavior."}
{"DOCID": "1487", "TEXT": "ELIZA-A Computer Program For the Study ofNatural Language Communication Between Man And Machine: ELIZA is a program operating within the MAC time-sharing system at MIT which makes certain kinds of natural language conversation between man and computer possible.  Input sentences are analyzed on the basis of decomposition rules which are triggered by key words appearing in the input text.  Responses are generated by reassembly rules associated with selected decomposition rules.  the fundamental technical problems with which ELIZA is concerned are: (1)the identification of key words, (2) the discovery of minimal context, (3) the choice of appropriate transformations, (4) generation of responses in the absence of key words, and (5) the provision of an editing capability for ELIZA \"scripts\".  A discussion of some psychological issues relevant to the ELIZA approach as well as of future developments concludes the paper."}
{"DOCID": "1488", "TEXT": "Programming Decision Tables in FORTRAN, COBOL or ALGOL: A simple broad-based approach for programming decision tables in FORTRAN or COBOL is developed and presented.  With inputs in standard form, as defined in the paper, the programming of any decision table can be done with one or two FORTRAN statements, or with two COBOL statements, if the COMPUTE verb is available in the COBOL processor.  It is  shown that the method is applicable even when there are more than two mutually exclusive states of one, two or more table conditions.  It is further shown that multi-state conditions in decision tables can often simplify the programming.  The method outlined has the further advantage that all possible combinations of conditions are considered.  It is shown that the suggested procedure is easily implemented in ALGOL."}
{"DOCID": "1489", "TEXT": "Data, Documentation and Decision Tables: In business data processing systems, it is necessary to be able to define and document data, files, programs and decision rules in a way that adequately represents both (1) their changing information content, and (2) their continuous interaction.  Tabular description makes this possible, being notably objective, through and economical in cost and time when systems must be analyzed and programs prepared or modified.  To show how quickly tabular techniques make an unfamiliar system manageable, a detailed example and a self-test are provided."}
{"DOCID": "1490", "TEXT": "One Inch Perforated Paper Tape for Information Interchange (Proposed American Standard)"}
{"DOCID": "1491", "TEXT": "EULER: A Generalization ALGOL, and its Formal Definition: Part I*: A method for defining programming languages is developed which introduces a rigorous relationship between structure and meaning.  The structure of a language is defined by a phrase structure syntax, the meaning in terms of the effects which the execution of a sequence of interpretation rules exerts upon a fixed set of variables, called the Environment. There exists a one-to-one correspondence between syntactic rules and interpretation rules is determined by the sequence of corresponding syntactic reductions which constitute a parse.  The individual interpretation rules are explained in terms of an elementary an d obvious algorithmic notation.  A constructive method for evaluating a text is provided, and for certain decidable classes of languages their unambiguity is proved.  As an example, a generalization of ALGOL is described in full detail to demonstrate that concepts like block-structure, procedures, parameters, etc. can be defined adequately and precisely by this method."}
{"DOCID": "1492", "TEXT": "Serrev (Algorithm 273 [C1])"}
{"DOCID": "1493", "TEXT": "Generation of Hilbert Derived Test Matrix (Algorithm 274 [F1])"}
{"DOCID": "1494", "TEXT": "Complete Elliptic Integral of the Second Kind (Algorithm 56 [S21])"}
{"DOCID": "1495", "TEXT": "Solution of Transcendental Equations by Series Reversion: An algorithm is developed for expressing the solution Y, of the equation F(Y) = G(X) as a power series in (X - X0) when f and g are given as power series,and the root Y0, is known at Y=X0.  The algorithm is illustrated for the equation Y^Y = X, i.e., (1+y)*ln(1+y) = ln(1+x)."}
{"DOCID": "1496", "TEXT": "A Formal Semantics for Computer Languages and its Application In a Compiler-Compiler: A semantic meta-language has been developed for representing the meanings of statements in a large class of computer languages.  This meta-language has been the basis for construction of an efficient, functioning compiler-compiler.  An informal discussion of the meta-language based on the example of a complete translator for a small language is presented."}
{"DOCID": "1497", "TEXT": "On the Normalization Requirement of Divisor in Divide- and- Correct Methods: This paper presents an analysis on the normalization requirement of the divisor in a divide-and-correct method.  This analysis is made subject to the condition that not more than one correction is required to obtain the true quotient character, from the trial estimate got from the division of a two-precision segment of every partial remainder by a suitably rounded single-precision divisor.  (This segmented division is denoted here as a (2, 1) precision basic division.) It is found that the normalization requirement could be narrowed down to a smaller range of divisors, provided the magnitude of the character next to the leading character of the divisor is known.  If, however, the normalization is to be eliminated one has to choose proper higher precision segments of operands for the basic division.  Also considered is the possibility of eliminating the normalization by an increase on the number of corrections on the quotient estimate got from a (2, 1) precision basic division. It is shown that such a scheme is economical only for small radices."}
{"DOCID": "1498", "TEXT": "The ALCOR Illinois 7090/7094 Post Mortem Dump: A dump technique for programs written in ALGOL 60 is described.  This technique provides an intelligible analysis of an unsuccessful computation process in terms of the original source program."}
{"DOCID": "1499", "TEXT": "Chebyschev Curve-Fit (revised) (Algorithm 318 [E2])"}
{"DOCID": "1500", "TEXT": "Chebyschev Curve-Fit (Algorithm 91 [E2])"}
{"DOCID": "1501", "TEXT": "Eigenvectors of a 2n x 2n Matrix: It has been known that the eigenvalues of a certain 2n x 2n matrix can be obtained by use of two smaller matrices of order n which can be easily constructed.  An algorithm is given to obtain the eigenvectors of the 2n x 2n matrix by use of the eigenvectors of the smaller matrices."}
{"DOCID": "1502", "TEXT": "An Online Editor: An online, interactive system for test editing is described in detail, with remarks on the theoretical and experimental justification for its form. Emphasis throughout the system is on providing maximum convenience and power for the user.  Notable features are its ability to handle any piece of text, the content-searching facility, and the character-by-character editing operations.  The editor can be programmed to a limited extent."}
{"DOCID": "1503", "TEXT": "A SIMSCRIPT-FORTRAN Case Study: Two programs for a vehicle dispatching model, one written in 7040 SIMSCRIPT and the other in 7040 FORTRAN IV are compared. The comparison is made in terms of basic program design decisions, storage requirements, computer time used, and the ease of making changes.  In the SIMSCRIPT program, the primary design considerations center around the choice of model variables, model changing events, and model testing. In the FORTRAN program, basic design problems relate to the representation of the passage of time, the allocation of storage, and the organization of input data.  The comparison of these differently designed programs shows that the SIMSCRIPT program uses more computer storage and more computer time, but requires fewer program changes to introduce model revisions."}
{"DOCID": "1504", "TEXT": "Algorithms for Finding a Fundamental Set of Cycles for an Undirected Linear Graph: Given the adjacency matrix of the graph, the algorithm presented in this paper finds a spanning tree and then constructs the set of fundamental cycles. Our algorithm is slower than an algorithm presented by Welch by a ratio of N/3 (N is the number of nodes) but requires less storage.  For graphs with a large number of nodes and edges, when storage is limited our algorithm is superior to Welch's; however, when the graphs are small, or machine storage is very large, Welch's algorithm is superior.  Timing estimates and storage requirements for both methods are presented."}
{"DOCID": "1505", "TEXT": "A System Organization for Resource Allocation: This paper introduces a system for resource management using the concepts of \"process,\" facility,\" and \"event.\"  Except for the processor no attempt has been made to give serious suggestions for the policy to be followed for resource allocation.  However, a basic framework is provided in which a system analyst can express solutions to resource management problems. The paper is divided into a tutorial presentation, a description of the system primitives, and a small collection of examples of the use of the primitives."}
{"DOCID": "1506", "TEXT": "The LACONIQ Monitor: Time Sharing for Online Dialogues: The LACONIQ (Laboratory Computer Online Inquiry) Monitor was developed primarily to support non-numerical applications such as retrieval from very large files by means of a \"dialogue\" between a system user and a retrieval application.  The monitor was designed so that it could work with a small computer (an IBM System 360/30).  Therefore techniques for resource allocation were important.  For this reason the use of core storage, computational facilities, and input-output were all scheduled.  An unusual feature of the system is that it is event-driven rather than clock-driven.  The program segments called into execution by the remote CRT consoles are invariably run to completion rather than \"rolled-out\" to be brought back at a later time."}
{"DOCID": "1507", "TEXT": "A Multiprogramming Environment for Online Data Acquis ition and Analysis: An experimental system for acquis ition and analysis of large bodies of data derived from scientific experiments is described.  Its architecture and implementation is largely based on certain objectives and characteristics of a general data analysis scheme. Early applications have been oriented towards the investigation of data obtained in biological research. Some of the problems encountered by the chosen approach are discussed."}
{"DOCID": "1508", "TEXT": "Magnetic Tape Labels for Information Interchange (Proposed USA Standard)"}
{"DOCID": "1509", "TEXT": "Recorded Magnetic Tape for Information Interchange (200 CPI, NRZI) (Proposed USA Standard)"}
{"DOCID": "1510", "TEXT": "Finding a Solution of N Functional Equations in N Unknown (Algorithm 314 [C5])"}
{"DOCID": "1511", "TEXT": "The Damped Taylor's Series Method for Minimizing a Sum of Squares and for Solving Systems of Nonlinear Equations"}
{"DOCID": "1512", "TEXT": "Solution of Simultaneous Non-Linear Equations (Algorithm 316[C5])"}
{"DOCID": "1513", "TEXT": "PERMUTATION (Algorithm 317 [G6])"}
{"DOCID": "1514", "TEXT": "On the Expected Gain From Adjust ing Matched Term Retrieval Systems: A file adjustment procedure based on maximizing the Bayes expected gain proposed for matched term retrieval systems.  The expected gain and its probability distribution are derived as a function of: (1) the prior proportion of omitted terms, and (2) the coefficient of separation between two distributions corresponding to values of an adjustment statistic.  An example evaluates the gain parameters for a typical information retrieval system."}
{"DOCID": "1515", "TEXT": "A Computer System for Inference Execution and Data Retrieval: This paper presents a RAND project concerned with the use of computers as assistants in the logical analysis of large collections of factual data. A system called Relational Data File was developed for this purpose.  The Relational Data File is briefly detailed and problems arising from its implementation are discussed."}
{"DOCID": "1516", "TEXT": "Automatic Data Compression: The \"information explosion\" noted in recent years makes it essential that storage requirements for all information be kept to a minimum.  A fully automatic and rapid three-part compressor which can be used with \"any\" body of information to greatly reduce slow external storage requirements and to increase the rate of information transmission through a computer is described in this paper.  The system will also automatically decode the compressed information on an item-by-item basis when it is required.  The three component compressors, which can be used separately to accomplish their specific tasks, are discussed: NUPAK for the automatic compression of numerical data, ANPAK for the automatic compression of \"any\" information, and IOPAK for further compression of information to be stored on tape or cards."}
{"DOCID": "1517", "TEXT": "Methods for Analyzing Data from Computer Simulation Experiments: This paper addresses itself to the problem of analyzing data generated by computer simulations of economic systems.  We first turn to a hypothetical firm, whose operation is represented by  single-channel, multistation queueing model.  The firm seeks to maximize total expected profit for the coming period by selecting one of five operating plans, where each plan incorporates a certain marketing strategy, an allocation of productive inputs, and a total cost. The results of the simulated activity under each plan are subjected to an F-test, two multiple comparison methods, and a multiple ranking method.  We illustrate, compare, and evaluate these techniques. The paper adopts the position that the particular technique of analysis (possibly not any one of the above) chosen by the experimenter should be an expression of his experimental objective: The F-test tests the homogeneity of the plans; multiple comparison methods quantify their differences; and multiple ranking methods directly identify the one best plan or best plans."}
{"DOCID": "1518", "TEXT": "An Experimental Model of System/360: The problem of predicting the performance of modern computer systems is formidable.  One general technique which can ease this problem is macroscopic simulation. This paper reports on the applicability of that technique to System/360.  The paper describes an experimental model of System/360-its hardware, software, and its environment.  The measures of system performance produced by the model consist of statistics relating to turnaround time, throughput, hardware utilization, software utilization, and queueing processes. The model is mechanized in SIMSCRIPT and consists of some 1750 statements.  An auxiliary programs, the Job Generator, creates automatically the properties of System/360 jobs that get simulated."}
{"DOCID": "1519", "TEXT": "GEORGE 3-A General Purpose Time Sharing and Operating System: An Operating System is described which will run on a wide variety of configurations of the I.C.T. 1900, and can handle a large number of online console users while at the same time running several off line (background) jobs.  The system is not oriented towards either mode and can be either a batch processing system (such as the ATLAS Supervisor, IBSYS, or GECOS), or a multiaccess system (resembling, to the user, CTSS or MULTICS), or both simultaneously, depending on the installation, which can adjust the Schedulers.  Both online users and off line jobs use a common Command Language.  The system includes a Multilevel device-independent File Store."}
{"DOCID": "1520", "TEXT": "Absolute Value and Square Root of a Complex Number (Algorithm 312 [A2])"}
{"DOCID": "1521", "TEXT": "Multi-Dimensional Partition Generator (Algorithm 313 [A1])"}
{"DOCID": "1522", "TEXT": "Chebyschev Quadrature (Algorithm 279 [D1])"}
{"DOCID": "1523", "TEXT": "SHARER, a Time Sharing System for the CDC 6600: A time sharing system embedded within the standard batch processing system for the CDC 6600 is described.  The system is general purpose and file-based, providing facilities for file input, manipulation, editing, compilation, and conversational execution. It uses a simple scheme for system extension for a machine with only one relocation and memory bound register. No attempt was made to use reentrant code, or to simulate segmentation or paging.  Implementation time was approximately six man-years, with the majority of the code being written in FORTRAN."}
{"DOCID": "1524", "TEXT": "A Stopping Criterion for Polynomial Root Finding: When searching for the root of a polynomial, it is generally difficult to know just when to accept a number as an adequate approximation to the root. In this paper an algorithm is presented which allows one to terminate the iteration process on the basis of calculated bounds for the roundoff error which occurs in evaluating the polynomial.  This stopping criterion has been tested on numerous examples and has been found to serve as a satisfactory means for accepting a complex number as a zero of a real polynomial."}
{"DOCID": "1525", "TEXT": "On Computing The Fast Fourier Transform: Cooley and Tukey have proposed a fast algorithm for computing complex Fourier transform and have shown major time savings in using it to compute large transforms on a digital computer.  With n a power of two, computing time for this algorithm is proportional to n log2 n, a major improvement over other methods with computing time proportional to n^2. In this paper, the fast Fourier transform algorithm is briefly reviewed and fast difference equation methods for accurately computing the needed trigonometric function values are given.  The problem of computing a large Fourier transform on a system with virtual memory is considered, and a solution is proposed.  This method has been used to compute complex Fourier transforms of size n = 2^16 on a computer with 2^15 words of core storage; this exceeds by a factor of eight the maximum radix two transform size with fixed allocation of this amount of core storage.  The method has also been used to compute large mixed radix transforms.  A scaling plan for computing the fast Fourier transform with fixed-point arithmetic is also given."}
{"DOCID": "1526", "TEXT": "Multiprogramming under a Page on Demand Strategy: A model of multiprogramming for a particular computer system using a page on demand strategy is developed.  Analysis of this model is used to predict performance (measured by the average usage of the CPU) when user programs are typical of those arising from an interactive time sharing environment. The effect of several hardware modifications is also analyzed.  A parameter, readily calculated from the hardware characteristics and the program statistics, is proposed for gauging the effect of multiprogramming."}
{"DOCID": "1527", "TEXT": "A Grammar Base Question Answering Procedure: The subject of this paper is a procedure for the automatic retrieval of certain segments of stored information, either explicitly or implicitly represented, through questions posed in natural language sentences.  This procedure makes use of a sentence recognition device for the class of grammars which will correctly decide between the grammatical and ungrammatical sentences of a natural language.  It is possible to make use of a recognition device of this sort for the following reason: Much data is fully expressible as a set of sentences in a natural language, a set which can be exhaustively and exclusively generated by a grammar.  Based upon the rules of this grammar, a sentence recognizer will evaluate sentences, questions in the normal situation.  Since the recognition function succeeds just in case the posed question is drawn from the set of sentences expressing the data, or, more correctly, is grammatical in terms of the grammar for this set of sentences, sentence recognition itself is a procedure for retrieving information. When the recognition function succeeds, its value represents the requested information."}
{"DOCID": "1528", "TEXT": "Three Fonts of Computer Drawn Letters: Detailed descriptions are given for three fonts of letters.  Letter shapes are entirely described by numbers.  The basic vectors are in a general form so the fonts may be easily drawn on a variety of computers and cathode-ray tubes.  The fonts include both upper and lower case Roman letters, mathematical signs, and upper and lower case Greek letters.  Design of the fonts is described.  However, the principal contribution of this paper concerns the fonts themselves."}
{"DOCID": "1529", "TEXT": "Decomposition Programming An Analysis of Matrix Substructure: A petroleum blending problem was analyzed in order to compare the primal and primal-dual decomposition algorithms.  In the course of the analysis, a substructure was discovered which has relevance to the relative performance of the two algorithms and to their absolute performance as compared with a standard primal-Simplex solution without decomposition."}
{"DOCID": "1530", "TEXT": "The ML/I Macro Processor: A general purpose macro processor called ML/I is described.  ML/I has been implemented on the PDP-7 and I.C.T. Atlas 2 computers and is intended as a tool to allow users to extend any existing programming language by incorporating new statements and other syntactic forms of their own choosing and in their own notation.  This allows a complete user-oriented language to be built up with relative ease."}
{"DOCID": "1531", "TEXT": "The Remaining Trouble Spots in ALGOL 60: This paper lists the ambiguities remaining in the language ALGOL 60, which have been noticed since the publication of the Revised ALGOL 60 Report in 1963."}
{"DOCID": "1532", "TEXT": "The Hardware-Software Complementarity"}
{"DOCID": "1533", "TEXT": "A Marovian Model of the University of Michigan Executive System: A mathematical model of a computer's executive system is postulated and its parameters estimated with the aid of extensive data on the system's operation. Although simplifying assumptions are made, the results predicted by the model agree reasonable well with actual results.  The model is used to study the effects of changes in the executive system and in one of its compilers.  Further applications of the model are discussed."}
{"DOCID": "1534", "TEXT": "DAD, The C.S.I.R.O. Operating System: The design and implementation of the C.S.I.R.O. operating system, DAD, is described in detail. This system is designed for the Control Data 3600 using a large drum backing store and is intended to allow the integration of a remote console (display) subsystem into a conventional job stack environment. The use of the drums, the buffering of input and output on slow peripherals, and the execution of normal job stack work are described.  The display subsystem is described only as it integrates into the rest of the system.  The techniques found useful in the development of DAD are given, and an assessment is made of the validity of various design decisions.  Performance figures based on several months of operation are tabulated."}
{"DOCID": "1535", "TEXT": "A Comment on Index Register Allocation: A technique is presented to reduce the enumeration required by a known procedure for optimal index register allocation in straight-line programs. This technique is based on the construction of a link diagram, which shows at any step the future occurrences of indexes which must be loaded into index registers.  This diagram determines in advance the required register configuration at certain steps of the program, so that the program is subdivided into separate portions to which the allocation procedure may be applied independently."}
{"DOCID": "1536", "TEXT": "Dynamic Computation of Derivatives: It is shown how Wengert's procedure for computation of derivatives can be implemented conveniently by use of compiler-generated complex addition, subtraction, and linkage to complex arithmetic subroutines. Evaluation of a function and derivative proceed in parallel, as in Wengert's procedure, but with the \"imaginary\" parts of variables declared complex bearing the values of the derivatives of the real parts. This technique provides a simple way to compute the derivatives of a function, without the need for deriving and programming the evaluation of explicit formulas for the derivatives."}
{"DOCID": "1537", "TEXT": "Prime Number Generator 1 (Algorithm 310 [A1])"}
{"DOCID": "1538", "TEXT": "Prime Number Generator 2 (Algorithm 311 [A1])"}
{"DOCID": "1539", "TEXT": "Prime Number Generator 1; Prime Number Generator 2 (Algorithm 35[A1]; Algorithm 310[A1]; Algorithm 311[A1])"}
{"DOCID": "1540", "TEXT": "An Algorithm for Class Scheduling With Section Preference: An algorithm for assignment of students to classes in a fixed time schedule that allows students to give a preference for sections within courses is given. If consistent with the objective of balanced sections, these preferences will be honored.  The algorithm is more stochastic than Monte Carlo in nature. Results are given that compare it to a nonpreference assignment algorithm."}
{"DOCID": "1541", "TEXT": "A Language for Modeling and Simulating Dynamic Systems: The general objective of this language is to facilitate both the modeling and experimental aspects of simulation studies.  The ability to represent systems containing highly interactive processes is an essential feature.  The nature of the language, and the role of the process concept, is presented by means of an extended example."}
{"DOCID": "1542", "TEXT": "A Microprogrammed Implementation of EULER on IBM System/360 Model 30: An experimental processing system for the algorithmic language EULER has been implemented in microprogramming on an IBM System/360 Model 30 using a second Read-Only Storage unit.  The system consists of a microprogrammed compiler and a microprogrammed String Language Interpreter, and of an I/O control program written in 360 machine language.  The system is described and results are given in terms of microprogram and main storage space required and compiler and interpreter performance obtained.  The role of microprogramming is stressed, which opens a new dimension in the processing of interpretive code.  The structure and content of a higher level language can be matched by an appropriate interpretive language which can be executed efficiently by microprograms on existing computer hardware."}
{"DOCID": "1543", "TEXT": "Computer Formulation of the Equations of Motion Using Tensor Notation: A means is described for extending the area of application of digital computers beyond the numerical data processing stage and reducing the need for human participation in the formulation of certain types of computer problems.  By the use of tensor calculus and a computer language designed to facilitate symbolic mathematical computation, a method has been devised whereby a digital computer can be used to do non-numeric work, that is, symbolic algebraic manipulation and differentiation. To illustrate the techniques involved, a digital computer has been used to derive the equations of motion of a point mass in a general orthogonal curvilinear coordinate system. Since this operation involves a formulation in terms of first- and second-order differential coefficients, it provides a good demonstration of a computer's capability to do non-numeric work and to assist in the formulation process which normally precedes the numerical data processing stage.  Moreover, this particular problem serves to illustrate the advantages of the mathematical techniques employed.  With the program prepared for this purpose the computer will derive the equations of motion in any coordinate system requested by the user.   Results are presented for the following coordinate systems: cylindrical polar, spherical polar, and prolate spheroidal."}
{"DOCID": "1544", "TEXT": "Tele-CUPL: A Telephone Time Sharing System: A general purpose, remote access, computing system is described, that employs twelve-key keyboard telephones as terminals.  Audio output is provided directly to the telephone terminals, but the system will normally be used in conjunction with remotely located high speed printing devices.  The system is a compatible extension of an existing batch processing system.  A significant element of the system is a scheme for transmitting alphanumeric information by single strokes on a numeric keyboard.  The programmed scanner uses context to eliminate the ambiguity in transmission."}
{"DOCID": "1545", "TEXT": "Legal Safeguards to Insure Privacy in a Computer Society"}
{"DOCID": "1546", "TEXT": "Toward Standards for Handwritten Zero and Oh"}
{"DOCID": "1547", "TEXT": "Gamma Function with Arbitrary Precision (Algorithm 309 [S14])"}
{"DOCID": "1548", "TEXT": "Parsing of Decision Tables: Reduction in the size of decision tables can be accomplished by several techniques.  The techniques considered in this paper are on the parsing of decision tables with regard to horizontal and vertical data structures, job identity, hardware and job priorities, and context relationships.  Such parsing rests upon some conventions for the linkage of decision tables."}
{"DOCID": "1549", "TEXT": "An Efficient Machine-Independent Procedure for Garbage Collection in Various List Structures: A method for returning registers to the free list is an essential part of any list processing system.  In this paper, past solutions of the recovery problem are reviewed and compared.  A new algorithm is presented which offers significant advantages of speed and storage utilization.  The routine for implementing this algorithm can be written in the list language with which it is to be used, thus insuring a degree of machine independence.  Finally, the application of the algorithm to a number of different list structures appearing in the literature is indicated."}
{"DOCID": "1550", "TEXT": "A Comparison of Batch Processing and Instant Turnaround: A study of the programming efforts of students in an introductory programming course is presented and the effects of having instant turnaround (a few minutes) as opposed to conventional batch processing with turnaround times of a few hours are examined.  Among the items compared are the number of computer runs per trip to the computation center, program preparation time, keypunching time, debugging time, number of runs, and elapsed time from the first run to the last run on each problem.  Even though the results are influenced by the fact that \"bonus points\" were given for completion of a programming problem in less than a specified number of runs, there is evidence to support \"Instant\" over \"Batch\"."}
{"DOCID": "1551", "TEXT": "On Compiling Algorithms for Arithmetic Expressions: This paper deals with algorithms concerning arithmetic expressions used in a FORTRAN IV compiler for a HITAC-5020 computer having n accumulators.  The algorithms generate an object code which minimizes the frequency of storing and recovering the partial results of the arithmetic expressions in cases where there are several accumulators."}
{"DOCID": "1552", "TEXT": "The AED Free Storage Package: The most fundamental underlying problem in sophisticated software systems involving elaborate, changing data structure is dynamic storage allocation for flexible problem modeling.  The Free Storage Package of the AED-1 Compiler Systems allows blocks of available storage to be obtained and returned for reuse.  The total available space is partitioned into a hierarchy of free storage zones, each of which has its own characteristics.  Blocks may be of any size, and special provisions allow efficient handling of selected sizes, control of shattering and garbage collection, and sharing of physical space between zones.  The routines of the package perform high level functions automatically, but also allow access and control of fine internal details as well."}
{"DOCID": "1553", "TEXT": "Contextual Understanding by Computers: A further development of a computer program (ELIZA) capable of conversing in natural language is discussed.  The importance of context to both human and machine understanding is stressed.  It is argued that the adequacy of the level of understanding achieved in a particular conversation depends on the purpose of that conversation, and that absolute understanding on the part of either humans or machines is impossible."}
{"DOCID": "1554", "TEXT": "A Computer Technique for Displaying n-Dimensional Hyperobjects: A digital computer and automatic plotter have been used to generate three-dimensional stereoscopic movies of the three-dimensional parallel and perspective projections of four-dimensional hyperobjects rotating in four-dimensional space.  The observed projections and their motions were a direct extension of three-dimensional experience, but no profound \"feeling\" or insight into the fourth spatial dimension was obtained.  The technique can be generalized to n-dimensions and applied to any n-dimensional hyperobject or hypersurface."}
{"DOCID": "1555", "TEXT": "Symmetric Polynomials (Algorithm 305 [C1])"}
{"DOCID": "1556", "TEXT": "Permutations with Repetitions (Algorithm 306 [G6])"}
{"DOCID": "1557", "TEXT": "Symmetric Group Characters (Algorithm 307 [A1])"}
{"DOCID": "1558", "TEXT": "Generation of Permutations in Pseudo-Lexicographic Order (Algorithm  [G6])"}
{"DOCID": "1559", "TEXT": "Permutation Generator; Permutation in Lexicographical Order; Permute; Generation of Permutations in Lexicographical Order (Algorithm 87[G6]; Algorithm 102[G6]; Algorithm 130[G6]; Algorithm 202[G6])"}
{"DOCID": "1560", "TEXT": "Transport; Transportation Problem (Algorithm 258[H]; Algorithm 293[H])"}
{"DOCID": "1561", "TEXT": "The Mutual Primal-Dual Method (Algorithm 285 [H])"}
{"DOCID": "1562", "TEXT": "Airy Function (Algorithm 301 [S20])"}
{"DOCID": "1563", "TEXT": "A Method for Finding Hamilton Paths and Knight's Tours: The use of Warnsdorff's rule for finding a knight's tour is generalized and applied to the problem of finding a Hamilton path in a graph.  A graph-theoretic justification for the method is given."}
{"DOCID": "1564", "TEXT": "Description of Basic Algorithm in DETAB/65 Preprocessor: The basic algorithm for the conversion of decision tables into COBOL code is contained in the generator portion of the DETAB/65 preprocessor.  The generator analyzes a decision table and produces simple COBOL conditional statements.  Core storage is saved by using queueing techniques and extensive indexing and also by outputting the code as it is generated, a line at a time.  The only optimization attempted is the elimination of obviously unnecessary tests on certain conditions in the decision table. Since the preprocessor and this language associated with it were developed for COBOL users, the preprocessor was written in a modular form in required COBOL-61."}
{"DOCID": "1565", "TEXT": "A Language-Independent Macro Processor: A macro processor is described which can be used with almost any source language.  It provides all features normally associated with a macro facility, plus the ability to make arbitrary transformations of the argument strings.  The program is used at the Basser Computing Department, University of Sydney, Sydney, Australia, to process text for eight different compilers."}
{"DOCID": "1566", "TEXT": "Optimal Starting Values for Newton-Raphson Calculation of SQRT(x): The problem of obtaining starting values for the Newton-Raphson calculation of SQRT(x) on a digital computer is considered.  It is shown that the conventionally used best uniform approximations to SQRT(x) do not provide optimal starting values. The problem of obtaining optimal starting values. The problem of obtaining optimal starting values is stated, and several basic results are proved.  A table of optimal polynomial starting values is given."}
{"DOCID": "1567", "TEXT": "On the Representation of Symmetric Polynomials: Relations are given between certain symmetric polynomials in the light of the theory of the symmetric group.  Such an approach unifies earlier work and lends insight to previously published work by Aaron Booker.  A generalization of Graeffe's root-squaring technique for the determination of the roots of a polynomial is suggested."}
{"DOCID": "1568", "TEXT": "Plotting a Function of Three Independent Variables: A method is developed for constructing an approximate plot of a function of three independent variables.  The plot is similar to a conventional contour map except that there are three scales to represent the independent variables.  Scale values of the three independent variables are added vectorially, and the value of the function is then read from the values associated with nearby contours."}
{"DOCID": "1569", "TEXT": "Implementing Phrase-Structure Productions in PL/I: A method is described for implementing the productions of a context-free phrase structure grammar in a PL/I procedure whose structure and statements parallel the structure and notation of the grammar."}
{"DOCID": "1570", "TEXT": "String Processing Techniques: The internal organization of string processing systems is discussed.  Six techniques for data structures are presented and evaluated on the basis of: (1) creation of strings; (2) examination of strings; and (3) alteration of strings.  Speed of operation, storage requirements, effect on paging, and programmer convenience are also considered.  One of the techniques, single-word linked blocks, is used in an example demonstrating an implementation of a SNOBOL string processing language on an IBM System/360."}
{"DOCID": "1571", "TEXT": "A User-Oriented Time-Shared Online System: An existing system and planned additions within the Data Processing Laboratory of the Brain Research Institute at UCLA is described.  The system represents an attempt to provide research workers of the Institute with the ability to interact directly with a highly sophisticated digital computing complex in the most direct and simple fashion possible. It is anticipated that, with the accumulation of experience using the present system, significant advances will be possible in the system design through determination of interface parameters between the biological scientist and the digital computer."}
{"DOCID": "1572", "TEXT": "The Simulation of Time sharing Systems: The development of new large scale time-sharing systems has raised a number of problems for computation center management.  Not only is it necessary to develop an appropriate hardware configuration for these systems, but appropriate software adjustments must be made.  Unfortunately, these systems often do not respond to changes in the manner that intuition would suggest, and there are few guides to assist in the analysis of performance characteristics.  The development of a comprehensive simulation model to assist in the investigation of these questions is described in this paper.  The resulting model has a general purpose design and can be used to study a variety of time-sharing systems.  It can also be used to assist in the design and development of new time-sharing algorithms or techniques.  For the sake of efficiency and greater applicability, the model was implemented in a limited FORTRAN subset that is compatible with most FORTRAN IV compilers. The use of the simulation is demonstrated by a study of the IBM 360/67 time-sharing system."}
{"DOCID": "1573", "TEXT": "An Adaptive Quadrature Procedure with Random Panel Sizes (Algorithm [D1])"}
{"DOCID": "1574", "TEXT": "Normal Curve Integral (Algorithm 304 [S15])"}
{"DOCID": "1575", "TEXT": "Incomplete Beta Ratio (Algorithm 179 [S14])"}
{"DOCID": "1576", "TEXT": "Eigenvalues of a Real Symmetric Matrix by the QR Method (Algorithm 253 [F2])"}
{"DOCID": "1577", "TEXT": "Eigenvalues and Eigenvectors of a Real Symmetric Matrix by the QR Method (Algorithm 254 [F2])"}
{"DOCID": "1578", "TEXT": "Generalized Least Squares Fit By Orthogonal Polynomials (Algorithm 296 [E2])"}
{"DOCID": "1579", "TEXT": "Real Error Function, ERF(x) (Algorithm 123 [S15])"}
{"DOCID": "1580", "TEXT": "Error Function-Large X (Algorithm 180 [S15])"}
{"DOCID": "1581", "TEXT": "Complementary Error Function-Large X (Algorithm 181 [S15])"}
{"DOCID": "1582", "TEXT": "GAUSS (Algorithm 209 [S15])"}
{"DOCID": "1583", "TEXT": "Normal Distribution Function (Algorithm 226 [S15])"}
{"DOCID": "1584", "TEXT": "Procedure for the Normal Distribution Functions (Algorithm 272 [S15])"}
{"DOCID": "1585", "TEXT": "Normal Curve Integral (Algorithm 304 [S15])"}
{"DOCID": "1586", "TEXT": "A Generalized Bairstow Algorithm: The Bairstow algorithm is generalized to the case of a polynomial which is itself a linear combination of polynomials satisfying a three-term recursion. Convergence properties of the method are derived."}
{"DOCID": "1587", "TEXT": "Storage Allocation in a Certain Iterative Process: A method of core storage allocation in a certain iterative process is described and estimates of the machine time required are given.  The method is applicable to iterative processes in which input data items once chosen are never again needed.  In this method the input data is continuously relocated and the space made available apportioned to the output tables when an overflow occurs.  Some important special cases are considered in which considerable simplification occurs."}
{"DOCID": "1588", "TEXT": "PL/I List Processing: The concepts of list processing have been introduced into the PL/I language.  With these new facilities, it is possible to write PL/I procedures that operate on simple and complex data list organizations. Most list-processing languages have suffered from their inability to deal directly with complex data structures and/or from their inability to perform the complete range of programming language operations upon the data list structures.  These two problems have been eliminated in the list-processing facilities of PL/I.  The basic concepts of list processing and the philosophy of the PL/I language extensions are discussed.  In addition, several detailed list-processing examples are provided."}
{"DOCID": "1589", "TEXT": "DIALOG: A Conversational Programming System with a Graphical Orientation: DIALOG is an algebraic language for online use with a graphical input-output console device. It is a computational aid for the casual user, which provides basic facilities for graphical and numeric input and display, online and off line program preparation and storage, and hard copy presentation of results.  Use of the system requires a minimum of experience or instruction, since the growth of an overlaying system control language has been prevented, and there are no processor-oriented statements, like variable type or dimension declarations.  Moreover, in the online situation the processor interacts with the graphical keyboard on a character-by-character basis so as to restrict the programmer's choice of input symbols to those which are syntactically correct. DIALOG has been in daily operation at the IIT Research Institute since February, 1966."}
{"DOCID": "1590", "TEXT": "Pitch Period Determination of Speech Sounds: A computer procedure which determines pitch periods by the recognition of the peak structure of the speech waveform is described.  Speech sounds were sampled by a microphone and an analog-to-digital converter attached to an interconnected IBM 7090-PDP-1 system.  These utterances were recorded at the normal noise level of the computer room but were not band-compressed or phase-distorted in any manner. A sequence of operations defined on the speech wave selects a list of points along the waveform as candidates for pitch markers.  These markers are validated by an error detection and correction procedure.About 95 percent of the pitch periods were recognized correctly within 1 to 2 times real-time on the IBM 7090."}
{"DOCID": "1591", "TEXT": "A Model for a Multifunctional Teaching System: A teaching system model that was incorporated into an operating system of a large computer is described.  The model transferred control to the operating system to execute functions other than teaching, and then recovered control in order to resume teaching.  The teaching system (ABAC-II) was written to run under the operating system (IBSYS) for the IBM 7044 Graphic System.  Because the teaching system automatically terminated and rescheduled itself, a student studying a course presented at a cathode-ray display terminal could switch readily between student mode and programmer mode.  During the latter, the full resources of the operating system (language processors, compilers, library and user's programs) were at his disposal.  He could for example, write, assemble, debug, and execute at the terminal a program written in any language processed by the operating system. A course could therefore include text material interleaved with programming problems which the student could solve without leaving the terminal.  Exercises in simulation and gaming could also be provided.  The implications of a teaching system with this degree of flexibility for industrial and executive training as well as academic education are discussed.  In addition, the advantages of this type of system for computer programming and operation are also considered."}
{"DOCID": "1592", "TEXT": "String Similarity and Misspellings: The problem of programming a computer to determine whether or not a string of characters is a misspelling of a given word was considered.  A numberof algorithms were evaluated-some proposed by other writers, some by the author.  These techniques were tested on a collection of misspellings made by students at various grade levels.  While many of the methods were clearly unsatisfactory, some gave as few as 2.1 percent incorrect determinations."}
{"DOCID": "1593", "TEXT": "A Simple Technique for Digital Division: A simple and economical method for digital division is described.  The method is suitable for divisors whose leading character is either radix less one or is unity with the next character equal to zero; also the method is direct and needs only half the number of arithmetic operations needed by a variant of the Harvard iterative method, described by Gilman, which is suitable for similar divisors."}
{"DOCID": "1594", "TEXT": "An Algorithm for Generating Permutations: An algorithm is described which under repeated application generates all permutations of K elements.  Only the previously generated permutation, the constant K, and a temporary index are needed. Starting with a particular ordering of K elements (abcd), repeated application of the algorithm will generate K-1 additional permutations by K-1 successive rotations.  From the initial circular ordering of K objects, another circular ordering can be obtained by rotating the K-1 lowest elements.  For each new K-1 circular ordering, another K-2 can be obtained by rotating the K-2 lowest elements.  By continuing in this manner, applications of the algorithm will generate all (K-1)! circular orderings, or since each circular ordering yields K permutations the algorithm generates all K! permutations."}
{"DOCID": "1595", "TEXT": "On the Computer Enumeration of Finite Topologies: The problem of enumerating the number of topologies which can be formed from a finite point set is considered both theoretically and computationally. Certain fundamental results are established, leading to an algorithm for enumerating finite topologies, and computed results are given for n <= 7. An interesting side result of the computational work was the unearthing of a theoretical error which had been induced into the literature; the use of the computer in combinatorics represents, chronologically, an early application, and this side result underscores its continuing usefulness in this area."}
{"DOCID": "1596", "TEXT": "Airy Function (Algorithm 301 [S20])"}
{"DOCID": "1597", "TEXT": "Transpose Vector Stored Array (Algorithm 302 [K2])"}
{"DOCID": "1598", "TEXT": "Least Squares Fit By Orthogonal Polynomials (Algorithm 28 [E2])"}
{"DOCID": "1599", "TEXT": "Numerical Solution of the Polynomial Equation (Algorithm 300 [C2])"}
{"DOCID": "1600", "TEXT": "Chebyshev Quadrature (Algorithm 279 [D1])"}
{"DOCID": "1601", "TEXT": "Parallel Numerical Methods for the Solution of Equations: Classical iterative procedures for the numerical solution of equations provide at each stage a single new approximation to the root in question.  A technique is given for the development of numerical procedures which provide, at each stage, several approximations to a solution of an equation.  The s8everal approximations obtained in any iteration are computationally independent, making the methods of interest in a parallel processing environment.  Convergence is insured by extracting the \"best information\" at each iteration.  Several families of numerical procedures which use the technique of the procedures in a parallel processing environment are developed and measurements of these statistics are reported.  These measurements are interpreted in a parallel processing environment.  In such an environment the procedures obtained are superior to standard algorithms."}
{"DOCID": "1602", "TEXT": "POSE: A Language for Posing Problems to a Computer: A language, POSE, is described which is a drastic departure from the FORTRAN/ALGOL type, though it does utilize FORTRAN formula and logic representations (and actually contains FORTRAN VI as a subset). With the new language, the user need only describe his problem in \"equation-like\" form. The method of solution is automatically provided in conjunction with the translation from equation form to computer instruction. In this way the POSE language user can solve difficult computational problems (like the solution of differential equation) without requiring a knowledge of numerical methods or the intricacies of computer subroutine logic. Essentially all clerical operations now required for FORTRAN programming have been automated so that the POSE programmer need not be concerned with these details."}
{"DOCID": "1603", "TEXT": "A Multiprogramming Monitor for Small Machines: INT, a combination hardware/software monitor designed to control a wide variety of real-time input/output devices, is described.  The simple hardware additions provide a uniform device to machine interface for such elements as keyboards graphic input devices, and interval timers.  The software relieves the user program from the details of input/output timing, buffering, and task scheduling and provides parallel processing capability.  User programs communicate with the monitor through a small set of meta-instruction which consists mostly of machine-language subroutine calls."}
{"DOCID": "1604", "TEXT": "Further Analysis of a Computing Center Environment: Empirical distributions of program lengths, execution times, processing times, and loading times of over 10,000 jobs serviced in a university computing center environment are presented.  The data are subdivided according to certain characteristics of users and jobs to obtain selected empirical conditional distributions of those time properties as well as statistical measures of other interesting properties. The results are interpreted in terms of the properties of the system studied."}
{"DOCID": "1605", "TEXT": "An Experimental Comparison of Time Sharing and Batch Processing: The effectiveness for program development of the MIT Compatible Time-Sharing System (CTSS) was compared with that of the IBM IBSYS batch-processing system by means of a statistically designed experiment.  An identical set of four programming problems was assigned to each of a group of four programming subjects.  Influences external to the systems, such as the sequence of problem solution, and programmer and problem characteristics, were specified as design factors in the experiment.  Data was obtained for six variables (e.g., programmer time, computer time, elapsed time, etc.) which were considered to be definitive of \"system effectiveness,\" and analysis of variance techniques were employed to estimate system differences in these variables after differences due to the design factors had been eliminated.  Statistical analysis of the experimental results provided strong evidence of important system differences, as well as a critique of the experimental design itself with implications for further experimentation."}
{"DOCID": "1606", "TEXT": "Chi-Squared Integral (Algorithm 299 [S15])"}
{"DOCID": "1607", "TEXT": "Coulomb Wave Functions (Algorithm 300 [S22])"}
{"DOCID": "1608", "TEXT": "Numerical Integration of Function That Has a Pole: It is common to need to integrate numerically functions that diverge somewhere outside the range of integration.  Even if the divergence occurs quite far away, integration formulas like Simpson's, that depend on fitting a polynomial, usually will be inaccurate: near a pole they will be very bad. A method is described that gives formulas that will integrate functions of this kind accurately if the orders and positions of the poles are known.  Explicit formulas are given that are easy to use on an automatic computer.  It is shown that they can be used for some other singularities as well as poles. If the integral converges, integration can be carried to the singularity.  The accuracy of the integration with a pole of second order is discussed, and, as an example, the new formula is compared with Simpson's. The new formulas are useful even far from the pole, while near the pole their advantage is overwhelming."}
{"DOCID": "1609", "TEXT": "Scheduling University Course Examinations by Computer: A new approach to the problem of scheduling course examinations is presented.  In principle, an examination schedule which requires a minimum number of examination periods and satisfies the constraint that no student be required to take two examinations simultaneously can be found in two steps.  First, course which may have their examinations scheduled at the same period are grouped together in all possible ways.  Then a minimum number of these groups, such that each course is included at least once, are selected. By removing multiple occurrences of courses and then scheduling each group at a different period a minimal schedule can be obtained.  Known algorithms for carrying out these procedures are prohibitively expensive. Approximations to the ideal procedure outlined above are given which yield nonminimal but feasible schedules with a very small expenditure of time.  Results of experiments using these techniques are given.  These are encouraging and indicate that further experimentation would be worthwhile."}
{"DOCID": "1610", "TEXT": "A Method for the Solution of Transportation Problems with Tall Matrices: A method is presented for the solution of the transportation problem having a cost matrix with few columns.  The computer implementation of this method shows it to be very fast and efficient.  Application are indicted for the personnel classification problem as well as the classical transportation problem. An example is worked out in detail."}
{"DOCID": "1611", "TEXT": "Scheduling Project Networks: Some of the basic concepts and terminology of project networking are developed.  The Critical Path Algorithm incorporated in the C-E-I-R proprietary scheduling system RAMPS (Resource Allocation and Multi-Project Scheduling) is described.  The error detection and network analysis features of the algorithm are also described."}
{"DOCID": "1612", "TEXT": "Top-to-bottom Parsing Rehabilitated?: This note is concerned with the efficiency of the Top-to-Bottom parsing algorithm as used in connection with programming language grammars.  It is shown, for instance, that retracing of unprofitable paths can often be eliminated by a suitable rearrangement of the productions defining the grammar.  The essential weakness of the method is in dealing with complicated syntactic structures which are in practice only sparsely occupied, e.g., arithmetic expressions."}
{"DOCID": "1613", "TEXT": "One-Pass Compilation of Arithmetic Expressions for a Parallel Processor: Under the assumption that a processor may have a multiplicity of arithmetic units, a compiler for such a processor should produce object code to take advantage of possible parallelism of operation. Most of the presently known compilation techniques are inadequate for such a processor because they produce expression structures that must be evaluated serially. A technique is presented here for compiling arithmetic expressions into structures that can be evaluated with a high degree of parallelism.  The algorithm is a variant of the so-called \"top-down\" analysis technique, and requires only one pass of the input text."}
{"DOCID": "1614", "TEXT": "A Proposal for Definitions in ALGOL: An extension to ALGOL is proposed for adding new data types and operators to the language. Definitions may occur in any block heading and terminate with the block.  They are an integral part of the program and are not fixed in the language.  Even the behavior of existing operators may be redefined. The processing of text containing defined contexts features a \"replacement rule\" that eliminates unnecessary iterations and temporary storage.  Examples of definition sets are given for real and complex matrices, complex numbers, file processing, and list manipulation."}
{"DOCID": "1615", "TEXT": "An Algorithm for Generating Root Locus Diagrams: A technique for using a digital computer to draw both ordinary and time-lag root locus diagrams is described.  Ordinary diagrams are drawn much faster and more accurately than ever before.  Time-lag diagrams, which had been impossible to obtain, are drawn with the same speed and accuracy as ordinary diagrams."}
{"DOCID": "1616", "TEXT": "Tensor Calculations on Computer: Appendix: In the main text of the paper [Comm. ACM 9, 12 (Dec. 196), 864], a FORMAC program was discussed which is capable of calculating various quantities of interest in tensor calculus.  This Appendix is intended as an example of the program output.  Chrisoffel symbols calculated for 12 basic orthogonal coordinate systems are listed."}
{"DOCID": "1617", "TEXT": "Eigenvalues and Eigenvectors of the Symmetric System (Algorithm 297 [F2])"}
{"DOCID": "1618", "TEXT": "Determination of the Square-Root of a Positive Definite Matrix (Algorithm 298 [F1])"}
{"DOCID": "1619", "TEXT": "Error-Free Methods for Statistical Computations: Neely has discussed computational error generated by some algorithms used to compute various statistics.  In the present paper methods are described which are error-free, simple in concept, and usually less costly in machine time than those mentioned by Neely."}
{"DOCID": "1620", "TEXT": "Methods of Evaluating Polynomial Approximations in Function Evaluation Routines: The method of nested multiplication is commonly used in function evaluation routines to evaluate approximation polynomials.  New polynomial evaluation methods have been developed in recent years which require fewer multiplications than nested multiplication and may therefore be preferable for use in function evaluation routines.  Although some of these methods do not appear to be practically useful because of rounding-error difficulties, several methods of evaluating low-degree polynomials have been found to be satisfactory.  Three such methods are described and illustrated."}
{"DOCID": "1621", "TEXT": "Computer Typesetting of ALGOL: An application of computer-aided typesetting is introduced.  A working method is described for publishing ALGOL by computerized translation from Hardware into Reference representation, computerized planning of typographical lay-out and computerized control of a typesetting machine.  The point is made that experts in science, technology, and programming are guaranteed a correct ALGOL documentation without spending valuable time and power on typographic considerations and proofreading."}
{"DOCID": "1622", "TEXT": "An Efficient Procedure for the Generation of Closed Subsets: An efficient algorithm is described for generating subsets of a set S which satisfy constraints of the form: \"If s(i) is a member of the subset, then s(j) must also be a member of the subset.\"  The algorithm has been programmed in the WISP language and successfully run on the IBM 7094 in connection with a routine to detect feedback in multidimensional iterative networks."}
{"DOCID": "1623", "TEXT": "An Application of FORMAC: A nonlinear circuit analysis problem is stated and the way in which it was solved using FORMAC is indicated.  The solution of the problem using FORMAC was notable since several other methods that were tried failed.  The problem is straightforward (although untenable by hand) but nevertheless involved an elaborate use of the FORMAC language.  The program was fairly large and utilized practically every command.  In particular, it made extensive use of the PART command.  Several tricks were necessary in order to circumvent some of the shortcomings of the FORMAC system.  This paper is more concerned with the use of programming techniques in FORMAC than with the actual engineering problem, although readers may be interested in the problem because it is stated in a general (mathematical) sense and could be of interest in areas other than circuit analysis."}
{"DOCID": "1624", "TEXT": "Automatic Dimensioning: Examples of algorithm that will accomplish automatic storage reservation without the need for explicit array declarations are described."}
{"DOCID": "1625", "TEXT": "On the Automatic Simplification of Source-Language Programs: Methods of simplification that can be applied automatically to programs written in an ALGOL-like language are discussed.  The simplifications are based on the form of the program and the knowledge obtained by a processor, without any understanding of what the program is supposed to do.  These methods have been implemented in a processor called SURE that accepts a program written in JOVIAL and outputs an equivalent JOVIAL program that may be shorter and may be executed faster than the original.  SURE is described, some of the problems encountered in automatic improvement at the source-language level are discussed, and further types of automatic program improvement are suggested."}
{"DOCID": "1626", "TEXT": "Structure of a LISP System Using Two-Level Storage: In an ideal list-processing system there would be enough core memory to contain all the data and programs.  Described in this paper are a number of techniques that have been used to build a LISP system utilizing a drum for its principal storage medium, with a surprisingly low time penalty for use of this slow storage device.  The techniques include careful segmentation of system programs, allocation of virtual memory to allow address arithmetic for type determination, and a special algorithm for building reasonably linearized lists.  A scheme for binding variables is described which is good in this environment and allows for complete compatibility between compiled and interpreted programs with no special declarations."}
{"DOCID": "1627", "TEXT": "Application of Level Changing to a Multilevel Storage Organization: A technique for organizing the devices of a computer storage system is described.  This technique, called the multilevel store, provides a means for economically satisfying the requirements for very large storage capacities of certain data management and information retrieval systems.  The concept of level changing is introduced and its application to the multilevel store is discussed.  A possible means for physically organizing the information for efficient use of the multilevel store is presented."}
{"DOCID": "1628", "TEXT": "The Emergence of a Profession: Computer programming deals with an enormous variety of activities and is carried on by people with a great variety of backgrounds.  It seems clear that part but not all of this activity is evolving toward a distinct professional field, but that the scope of this emerging profession, and some of its economic, social, and educational characteristics are as yet by no means well defined.  In this paper, these issues are examined and some opinions about them are expressed."}
{"DOCID": "1629", "TEXT": "Stat-Pack: A Biostatistical Programming Package: A package of FORTRAN statistical programs for use on almost any small to medium size (40k characters or 8k words) for which a FORTRAN II compiler exists is described and its availability is announced. The major design criteria of ease of use, ease of modification, flexibility of input and detail of output are described."}
{"DOCID": "1630", "TEXT": "Computer Representation of Planar Regions by Their Skeletons: Any region can be regarded as a union of maximal neighborhoods of its points, and can be specified by the centers and radii of these neighborhoods; this set is a sort of\"skeleton\" of the region.  The storage required to represent a region in this way is comparable to that required when it is represented by encoding its boundary.  Moreover, the skeleton representation seems to have advantages when it is necessary to determine repeatedly whether points are inside or outside the region, or to perform set-theoretic operations on regions."}
{"DOCID": "1631", "TEXT": "Testing a Random Number Generator: The first 1,000,000 numbers produced by the random number generator used in the General Purpose Systems Simulator (GPSS) were subjected to statistical tests.  The tests are described and the results of the tests are presented.  These particular tests indicate that the numbers are satisfactory.  It is recommended that suitable tests be applied to all random numbers used in computer simulations."}
{"DOCID": "1632", "TEXT": "Programming the Tabular Method of Analysis of Variance for Factorial Experiments: The ease of programming the tabular method of analysis of variance for complete factorial experiments in a FORTRAN language is demonstrated.  In this method, the total sum of squares is partitioned into orthogonal single degree of freedom sums of squares; main effect and interaction sums of squares are then obtained by appropriate pooling of the single degree of freedom sums of squares.  Program segments to accomplish the procedure are presented.  Modifications to handle hierarchical designs and replicated experiments are mentioned. A FORTRAN II program for an IBM 7094 is described briefly."}
{"DOCID": "1633", "TEXT": "A Modified Newton Method for Polynomials: A modified Newton method for polynomials is discussed.  It is assumed one has approximations for all the roots of the polynomial.  Three variations are described.  If the roots are simple, it is shown that under appropriate conditions, two of the variations are cubically convergent."}
{"DOCID": "1634", "TEXT": "27 bits Are Not Enough for 8-digit Accuracy: From the inequality 10^8 < 2^27, we are likely to conclude that we can represent 8-digit decimal floating-point numbers accurately by 27-bit floating-point numbers.  However, we need 28 significant bits to represent some 8-digit numbers accurately. In general, we can show that if 10^p < 2^q-1, then q significant bits are always enough for p-digit decimal accuracy.  Finally, we can define a compact 27-bit floating-point representation that will give 28 significant bits, for numbers of practical importance."}
{"DOCID": "1635", "TEXT": "Parameters for Pseudo Runge-Kutta Methods: The object of this note is to present a choice of the free parameters in the third- and fourth-order pseudo Runge-Kutta methods involving two points. This choice of parameters causes a bound on the principal part of the truncation error term to be near the minimum for the fourth-order method and at the minimum for the third-order method."}
{"DOCID": "1636", "TEXT": "Invariant Imbeding and the Numerical Integration of Boundary-Value Problems for Unstable Linear Systems of Ordinary Differential Equations: In such diverse areas as radiative transfer in planetary atmospheres and optimal guidance and control, two-point boundary-value problems for unstable systems arise, greatly complicating the numerical solution.  An invariant imbeding technique is presented which is useful in overcoming these frequently encountered instabilities, and the results of some numerical experiments are given."}
{"DOCID": "1637", "TEXT": "Problems in the Statistical Analysis of Simulation Experiments: The Comparison of Means and the Length of Sample Records: Research is continued into statistical analysis of simulation experiments containing autocorrelated time series.  It is shown how to estimate the lengths of sample records needed to use certain large sample results in measuring stability.  Analogies between autocorrelated data and independent observations are described.  A way to test the difference of the mean of two experiments is suggested.  It is shown how the variance of the sample mean relates to the spectrum of the generating process, and estimation of the quantities of interest is described. The results expand the possibilities of statistical spectral analysis as applied to simulation experiments."}
{"DOCID": "1638", "TEXT": "Sorting by Replacement Selecting: In sorting by replacement selecting, the expected length of a sequence beginning with the i-th element (i>1) is proved to be 2F, in accordance with a conjecture of E. H. Friend, where F is the number of memory cells used.  The expected length of the j-th sequence is determined to be F times a j-th degree polynomial in e, such that the value of this polynomial approaches 2 as j approaches infinity.  Recursive formulas are obtained for both the mean and the standard deviation of the length of the j-th sequence. The mathematical proofs of these results are based upon the assumption that n, the number of items to be sorted, is infinite, but it is shown that the error due to the finiteness of n approaches zero rapidly as n increases."}
{"DOCID": "1639", "TEXT": "Exponential Curve Fit (Algorithm 295 [E2])"}
{"DOCID": "1640", "TEXT": "Generalized Least Squared Fit By Orthogonal Polynomials (Algorithm 296 [E2])"}
{"DOCID": "1641", "TEXT": "A Use of Fast and Slow Memories in List-Processing Languages: A scheme is described which permitting a substantial increase in memory space utilized to store list-structured data.  It consists in reducing to one level a nonhomogeneous store composed of fast (core) and slow (disk or drum) memories.  The space available in slow memory is divided into pages each containing a given number of machine words.  The reduction to a one-level memory is performed by a program which leaves the most often called pages in the fast memory. When a new page from slow store is requested, the page in core having the longest period of inactivity is transferred back to the slow store.  The complete scheme has been implemented in connection with a LISP embedding into ALGOL, using an IBM 7044 with 32k of core memory and disks.  Gains in memory space were about 100-fold.  As often happens in programming applications the price of the additional space is computer time.  Although the disks have an access time 10^4 times slower than core, tests indicate that the actual slow down varied from 3 to 10, depending on the number of pages available in the fast store."}
{"DOCID": "1642", "TEXT": "Time Sharing on a Computer with a Small Memory: Techniques to make time sharing attractive on a computer with a small central memory are presented. \"Small\" is taken to mean that only one user program plus a monitor will fit into the memory at any time. The techniques depend on having two levels of secondary storage: level 1, several times larger than the main memory and quite fast; and level 2, many times larger and slower than level 1."}
{"DOCID": "1643", "TEXT": "An Improvement to Iterative Methods of Polynomial Factorization: Methods of polynomial factorization which find the zeros one at a time require the division of the polynomial by the accepted factor.  It is shown how the accuracy of this division may be increased by dividing in order of both ascending and descending powers of the variable and choosing a crossover point which minimizes a very simply calculated error criterion."}
{"DOCID": "1644", "TEXT": "On the Computation of Least Squares Polynomials: Rounding error accumulated during digital computation of a least squares polynomial makes the computed polynomial only an approximation to the true least square polynomial.  A simple method for adjust ing the constant term of the computed polynomial to get a better approximation to the true least squares polynomial is described."}
{"DOCID": "1645", "TEXT": "A Note on Computing Approximations to the Exponential Function: Two methods are discussed which result in near minimax rational approximations to the exponential function and at the same time retain the desirable property that the approximation for negative values of the argument is the reciprocal of the approximation for corresponding positive values.  These methods lead to approximations which are much superior to the commonly used convergents of the Gaussian continued fraction for the exponential.  Coefficients and errors are given for the intervals [-.5*ln 2, .5*ln 2] and [-ln 2, ln 2]."}
{"DOCID": "1646", "TEXT": "DITRAN-A Compiler Emphasizing Diagnostics: DITRAN (Diagnostic FORTRAN) is an implementation of ASA Basic FORTRAN with rather extensive error checking capabilities both at compilation time and during execution of a program.  The need for improved diagnostic capabilities and some objectives to be met by any compiler are discussed.  Attention is given to the design and implementation of DITRAN and the particular techniques employed to provide the diagnostic features.  The handling of error messages by a general macro approach is described.  Special features which provide teaching aids for use by instructors are noted."}
{"DOCID": "1647", "TEXT": "WATFOR-The University of Waterloo FORTRAN IV Compiler: WATFOR is an in-core, load-and-go compiler which has been implemented within the IBM 7040/44 operating system.  FORTRAN IV was selected as the source language in order to achieve maximum language compatibility with other available compiling systems, in particular the IBM 7040/44 FORTRAN IV system. The principal advantage of the WATFOR compiler is that it translates FORTRAN IV programs at speeds of up to 100 statements per second.  Since the compiler resides core there is virtually no system overhead, and hence large batches of \"student\" programs may be processed very efficiently.  The compiler also provides extensive error diagnostics, during both the compilation and the execution phases of a program run. This feature makes the system attractive to both learners and learned users alike."}
{"DOCID": "1648", "TEXT": "Uniform Random (Algorithm 294 [G5])"}
{"DOCID": "1649", "TEXT": "Data Directed Input-Output in FORTRAN: A statement which is similar to the NAMELIST statement of FORTRAN IV has been incorporated in the FORTRAN 63 compiler.  The FORTRAN 63 implementation allows a greater flexibility and simplicity than the FORTRAN IV feature.  The Hollerith names, the location, the mode and the dimensions of a variable can be discovered by means of standard FORTRAN statements. Methods of using this information are illustrated in relation to general purpose data directed input and output routines; some other uses such as matrix manipulation are discussed."}
{"DOCID": "1650", "TEXT": "A Unifying Computational Method for the Analysis of Complete Factorial Experiments: A computational method which may be used for the calculation of sums of squares in the analysis of variance of complete factorial experiments and in the computation of main effect or interaction means is described.  The method is elucidated as unifying since one method can be used for a variety of purposes each previously requiring different methods.  The programming advantages of such a method are obvious. The following variants are discussed: (1) the standard analysis of variance; (2) analyses omitting certain levels of one or more factors; (3) separate analyses for some levels of a factor or for combinations of levels of more than one factor.  These are performed simultaneously; (4) the calculation of main effect or interaction means.  The mean expects the data in standard order and it leaves the data in that order so that many analyses of the same data can be performed without rearrangement.  The total sum of squares, excluding a replication sum of squares, is partitioned into all polynomial partitions and their interactions each with one degree of freedom.  This is so even if factors have unequally spaced factor levels."}
{"DOCID": "1651", "TEXT": "An Interpretive Input Routine for Linear Programming: In this descriptive article an input code is presented which greatly simplifies data input to any linear programming solution routine, for subsequent use either as a pedagogical device or for solving rather small LP problems.  This latter (limited) use derives not at all from inherent limitations in the code itself, but from an efficiency evaluation: large LP problems would doubtless benefit from an input system more suited for bulk data handling than the input code described.  From a user's standpoint, input appears almost exactly as a textbook presentation of the LP problem (limited only by a keypunch's inability to write subscripts, etc.).  The input interpreter scans column wise, thus no fixed format data preparation is required.  The user may also, under very general requirements only, liberally use editorial comments throughout the input deck as an aid in identification, e.g., of row constraints. The article includes examples of input, output from a solution routine presently in use, and a skeleton flowchart of the input interpreter."}
{"DOCID": "1652", "TEXT": "A Code for Non-numeric Information Processing Applications in Online Systems: A code has been specifically designed to simplify the internal information processing operations within an online computer system with respect to non-numeric applications, and to maximize the transfer rate of the information channel linking the system and the system user.  The code has direct application to problems in area such as information retrieval, document classification, computer-aided teaching and text editing.  This code, called IPC (Information Processing Code), is an 8-bit code set constructed so that 7, 6, 5 and 4-bit subsets can be easily derived from the basic set.  The code set is organized so that simple binary operations can distinguish between the numeric alphabetic, special symbol and control character codes.  The number of usable characters within the basic set size may be expanded either by use of escape codes included in the set, or by suitable interpretation of otherwise unassigned codes on the basis of the requirements of local environments."}
{"DOCID": "1653", "TEXT": "System Performance Evaluation: Survey and Appraisal: The state of the art of system performance evaluation is reviewed and evaluation goals and problems are examined.  Throughput, turnaround, and availability are defined as fundamental measures of performance; overhead and CPU speed are placed in perspective.  The appropriateness of instruction mixes, kernels, simulators, and other tools is discussed, as well as pitfalls which may be encountered when using them.  Analysis, simulation, and synthesis are presented as three levels of approach to evaluation, requiring successively greater amounts of information. The central role of measurement in performance evaluation and in the development of evaluation methods is explored."}
{"DOCID": "1654", "TEXT": "A University's Educational Program in Computer Science: After a review of the power of contemporary computers, computer science is defined in several ways.  The objectives of computer science education are stated, and it is asserted that in a North American university these will be achieved only through a computer science department.  The program at Stanford University is reviewed as an example.  The appendices include syllabic of Ph.D. qualifying examinations for Stanford's Computer Science Department."}
{"DOCID": "1655", "TEXT": "Code Extension Procedures for Information Interchange* (Proposed USA Standard)"}
{"DOCID": "1656", "TEXT": "Procedures for the Standardization Process* (Proposed USA Standard)"}
{"DOCID": "1657", "TEXT": "Implementation of the SHARER2 Time-Sharing System: A simple mechanism is described for the execution of part of a program with its own memory protection.  This allows such a program to act as a suboperating system.  An improved version of the SHARER time-sharing system using this feature is described."}
{"DOCID": "1658", "TEXT": "Analysis of Algorithms for the Zero-One Programming Problem: This paper is concerned with a review and examination of several existing algorithms for the zero-one programming problem.  Computational experience is summarized.  The machine time and storage requirements of several of the algorithms are compared over several test problems of small and intermediate size.  Computer experiments still provide little hope of solving problems with over 100 variables with a reasonable amount of machine time."}
{"DOCID": "1659", "TEXT": "Computational Linguistics in a Ph.D. Computer Science Program: This report contains recommendations for a course curriculum on computational linguistics in a Ph.D. computer science program.  A classification of the subject areas contained in computational linguistics is presented, and ten courses in these areas are described. A basic bibliography in computational linguistics is appended."}
{"DOCID": "1660", "TEXT": "Index By Subject To algorithms, 1960-1968"}
{"DOCID": "1661", "TEXT": "Multint (Algorithm 32 [D1])"}
{"DOCID": "1662", "TEXT": "Eigenvalues and Eigenvectors of a Real General Matrix [F2])"}
{"DOCID": "1663", "TEXT": "Generator of Random Numbers Satisfying the Poisson distribution [G5])"}
{"DOCID": "1664", "TEXT": "An Algorithm for Deriving the Equations of Mathematical Physics by Symbolic Manipulation: A method is described whereby a digital computer can be used to derive the equations of mathematical physics in any curvilinear coordinate system requested by the user.  The effectiveness of the technique is demonstrated by using it to derive the Navier-Stokes equations of fluid motion and the continuity equation.  To derive these equations by this method, the user need know only the coordinate transformation equations relating the curvilinear coordinates of interest to an orthogonal Cartesian triad. When this program is used and the coordinate transformation equations are supplied as input, the computer will derive the Navier-Stokes equations and the continuity equation.  The equations obtained will be relative to the curvilinear coordinate system specified by the transformation equations used as input.  In this paper the emphasis is on theoretical considerations and methodology rather than on programming details. Results are presented for cylindrical polar and spherical polar coordinate systems."}
{"DOCID": "1665", "TEXT": "Automatic Generation of Efficient Lexical Processors Using Finite State Techniques: The practical application of the theory of finite-state automata to automatically generate lexical processors is dealt with in this tutorial article by the use of the AED RWORD system, developed at M.I.T. as part of the AED-1 system.  This system accepts as input description of the multicharacter items or of words allowable in a language given in terms of a subset of regular expressions. The output of the system is a lexical processor which reads a string of characters and combines them into the items as defined by the regular expressions.  Each output item is identified by a code number together with a pointer to a block of storage containing the characters and character count in the item.  The processors produced by the system are based on finite-state machines. Each state of a \"machine\" corresponds to a unique condition in the lexical processing of a character string.  At each state a character is read, and the machine changes to a new state.  At each transition appropriate actions are taken based on the particular character read.  The system has been in operation since 1966, and processors generated have compared favorably in speed to carefully hand-coded programs to accomplish the same task.  Lexical processors for AED-O and MAD are among the many which have been produced.  The techniques employed are independent of the nature of the items being evaluated.  If the word \"events\" is substituted for character string, these processors may be described as generalized decision-making mechanisms based upon an ordered sequence of events.  This allows the system to be used in a range of applications outside the area of lexical processing.  However convenient these advantages may be, speed is the most important consideration. In designing a system for automatic generation of a lexical processor, the goal was a processor which completely eliminated backup or rereading, which was nearly as fast as hand-coded processors, which would analyze the language and detect errors, and which would be convenient and easy to use."}
{"DOCID": "1666", "TEXT": "Solution of Linear Programs in 0-1 Variables by Implicit Enumeration (Algorithm 341 [H])"}
{"DOCID": "1667", "TEXT": "Roots of Polynomials by a Root-Squaring and Resultant Routine (Algorithm 340 [C2])"}
{"DOCID": "1668", "TEXT": "An Algol Procedure for the Fast Fourier Transform with Arbitrary Factors (Algorithm 339 [C6])"}
{"DOCID": "1669", "TEXT": "Algol Procedures for the Fast Fourier Transform (Algorithm 338 [C6])"}
{"DOCID": "1670", "TEXT": "Correspondences of 8-Bit and Hollerith Codes for Computer Environments (A USASI Tutorial Standard)"}
{"DOCID": "1671", "TEXT": "A Phonological Rule Tester: The design and implementation of a system to alleviate the problem of rule evaluation for the linguist in the area of phonology are presented.  It permits the user to define, on-line, sets of rules statable within the framework presented in The Sound Patterns of English by Chomsky and Halle, 1968, to define phonemes as bundles of specified distinctive features, to define data as strings of phonemes with associated grammatical structure, to test the effect of applying rules to the data, and to store both the definitions and results.  The rule application facility described in detail was implemented by translating linguistic rules to rules in FLIP, a format-directed list processor embedded in LISP. This made the system construction easy while providing sophisticated capabilities for the linguist. The system is written in BBN LISP on the Scientific Data System 940 computer and is designed to be used on-line in interactive fashion, with control returned to the user after each command is executed."}
{"DOCID": "1672", "TEXT": "Practical Error Coefficients in the Integration of Periodic Analytic Functions by the Trapezoidal Rule: Theoretical and practical values of error coefficients useful in bounding the error in integrating periodic analytic functions with the trapezoidal rule are tabulated for various ranges of the parameters."}
{"DOCID": "1673", "TEXT": "Approximate Solution of Initial Boundary Wave Equation Problems by Boundary-Value Techniques: A new boundary-value technique is proposed for the treatment of initial-boundary-value problems for linear and mildly nonlinear wave equations.   Several illustrative examples are offered to demonstrate the ease with which the method can be applied."}
{"DOCID": "1674", "TEXT": "One-Line Random Number Generators and Their Use in Combinations: Some one-line random number generators, i.e. generators requiring a single FORTRAN instruction are discussed, and some short FORTRAN programs which mix several such generators are described.  The aim is to provide methods for incorporating random number generators directly in FORTRAN programs, by means of a few in-line instructions.  The advantages are speed (avoiding linkage to and from a subroutine), convenience, and versatility.  Anyone wishing to experiment with generators, either using congruential generators by themselves or mixing several generators to provide a composite with potentially better statistical properties than the library generators currently available, may wish to consider some of the simple FORTRAN program discussed here."}
{"DOCID": "1675", "TEXT": "A Note on a Relevance Estimate and Its Improvement: In this paper the effect of iterating the improvement procedure is examined.  It is shown that applications of the improvement factor beyond the first time are ineffectual, and that the factor is but a scale factor."}
{"DOCID": "1676", "TEXT": "The LRLTRAN Compiler: Extensive software problems confront an organization which possesses a number of different computers and which frequently acquires new ones. To maintain cohesion, a system must be developed, written in a high level language, which minimizes machine dependencies and isolates those which are necessary. A language and a compiler for the language are discussed here.  The language, called LRLTRAN, is a heavily augmented FORTRAN.  The tree-pass compiler makes use internally of a postfix Polish notation (pass I to pass II) and a tree representation referred to as a \"composite blocking table\" (pass I to pass III). Machine-independent optimization occurs in pass II and DO-loop and machine-dependent optimization in pass III."}
{"DOCID": "1677", "TEXT": "Storage Organization in Programming Systems: The system of program and data representation that has been in use on the Rice University computer for five years is described.  Each logical entity in storage occupies a block of consecutive memory locations. Each block is labeled by a codeword and may contain a program, a data vector, or codewords which in turn label blocks to form arrays.  This storage arrangement is discussed with its realized advantages or programming systems: simplicity of programmed addressing, flexibility of data structures, efficiency of memory utilization, variability of system composition during execution, means of linkage between programs and from programs to data, and basis for storage protection. The application of labeled blocks may be extended to areas of time-sharing and multimedia storage control.  On the basis of experience at rice, some ideas on such extensions are presented."}
{"DOCID": "1678", "TEXT": "Automata, Formal Languages, Abstract Switching, and Computability in a Ph.D. Computer Science Program: A number of courses are listed in the area describe as automata, formal languages, abstract switching, and computability, that might be available to a Ph.D. student in computer science.  A brief catalog description of each course is applied and the role of each of the courses in the graduate program is discussed."}
{"DOCID": "1679", "TEXT": "A Fast Fourier Transform Algorithm for Real-Valued Series: A new procedure is presented for calculating the complex, discrete Fourier transform of real-valued time series.  This procedure is described for an example where the number of points in the series is an integral power of two.  This algorithm preserves the order and symmetry of the Cooley-Turkey fast Fourier transform algorithm while effecting the two-to-one reduction in computation and storage which can be achieved when the series is real.  Also discussed are hardware and software implementations of the algorithm which perform only (N/4) log2 (N/2) complex multiply and add operations, and which require only N real storage locations in analyzing each N-point record."}
{"DOCID": "1680", "TEXT": "A General-Purpose Display Processing and Tutorial System: ADEPT (A display-Expedited Processing and Tutorial) system is described.  This system was designed to improve man-computer communications by employing a display unit to interleave tutoring with other computer operations such as simulation, programming, and information retrieval.  It is written in FORTRAN IV (G) for the IBM System/360, Model 40, and the IBM 2250 display Unit under Operating System/360.  Adept is a cataloged program that controls the standard operating system by terminating and rescheduling itself automatically, relinquishing computer resources allocated to it, and surrendering control to the operating system to perform other jobs.  It expands the power and flexibility of computer-assisted instruction by making immediately available to students, teachers, and other users, the full resources (system-cataloged programs) of the operating system.  Language processors and compilers, simulation models, mathematical solution techniques, stored data, and all other library and user programs can be incorporated into instructional material without reprogramming.  Illustrations of the various applications are presented and their implications are discussed."}
{"DOCID": "1681", "TEXT": "Easy English,a Language for Information Retrieval Through a Remote Typewriter Console: Easy English is a natural command language designed to simplify communication between man and machine through remote typewriter console.  It has been developed for retrieval of documents from a computerized data base, the Moore School Information Systems Laboratory files.  Requests are formulated in a standardized syntactical form (examples of which are presented), and this form is then transformed into an equivalent query expressed in the retrieval system's original Symbolic Command Language, which is briefly described. Operation of easy English is detailed by illustration of the transformations performed upon a sample request up to the point at which the request string is sent to the system.  A macro flowchart of Easy English is included, and an Appendix provides the printout of a retrieval demonstration."}
{"DOCID": "1682", "TEXT": "The Implementation of a BASIC System in a Multiprogramming Environment: The implementation of a remote terminal BASIC system within the context of an existing multiprogramming computer system, the Burroughs B5500, is described. This implementation combines a unique mixture of machine language and interpretive techniques with an incremental compiler."}
{"DOCID": "1683", "TEXT": "Boolean matrix Methods for the Detection of Simple Precedence Grammars: A mechanical procedure is derived for determining whether a given context-free phrase structure grammar is a simple precedence grammar.  This procedure consists of elementary operations on suitably defined Boolean matrices.  Application of the procedure to operator grammars is also given."}
{"DOCID": "1684", "TEXT": "Ambiguity in Limited Entry Decision Tables: The use of decision tables as a tool in systems analysis and for program specification is now becoming accepted.  Rules on redundancy, contradiction, and completeness for limited entry tables were published in 1963.  These are usually used for checking, preceded if necessary by a conversion from extended to limited entry form.  Processors which automatically translate tables to more conventional program usually base their diagnostic facilities on these rules. In this paper it is suggested that these rules are unsatisfactory and that the important aspect of checking is to eliminate ambiguity from tables. Ambiguity is defined and discussed, and a procedure for producing checked-out decision tables is proposed. The theoretical basis of the algorithm used is established. The importance of well-designed diagnostic facilities in decision table processors is emphasized."}
{"DOCID": "1685", "TEXT": "GAN, a System for Generating and Analyzing Activity Networks: GAN, a system for generating activity networks, is designed to save time in the preparation of activity networks and to deal conveniently with network programs.  A defining description of a programming language designed for generating activity network from a set of standard networks is presented.  Also, a general idea of a language for performing calculations on activity networks (scheduling activity networks) is given."}
{"DOCID": "1686", "TEXT": "Computer Synthesis of Holograms for 3-D Display: Optical and digital holography are reviewed. The mathematical model and computational techniques of the authors' digital holographic process are discussed, and applications of computer holography are suggested.  Computer holograms have been made of three-dimensional objects which give faithful reconstructions, even in white light.  A new approach based on point apertures for the image is discussed.  Photographs of the images reconstructed from digital holograms are presented."}
{"DOCID": "1687", "TEXT": "Netflow (Algorithm 248 [H])"}
{"DOCID": "1688", "TEXT": "Netflow (Algorithm 248 [H])"}
{"DOCID": "1689", "TEXT": "Calculation of a Polynomial and its Derivative Values by Horner Scheme (Algorithm 337 [C1])"}
{"DOCID": "1690", "TEXT": "Netflow (Algorithm 336 [H])"}
{"DOCID": "1691", "TEXT": "A Comparison of the Correlational Behavior of Random Number Generators for the IBM 360: Hutchinson states that the \"new\" (prime modulo) multiplicative congruential pseudorandom generator, attributed to D. H. Lehmer, has passed the usual statistical tests for random number generators.  It is here empirically shown that generators of this type can produce sequences whose autocorrelation functions up to lag 50 exhibit evidence of nonrandomness for many multiplicative constants.  An alternative generator proposed by Tausworthe, which uses irreducible polynomials over the field of characteristic two, is shown to be free from this defect.  The applicability of these two generators to the IBM 360 is then discussed. Since computer word size can affect a generator's statistical behavior the older mixed and simple congruential generators, although extensively tested on computers having 36 or more bits per word, may not be optimum generators for the IBM 360."}
{"DOCID": "1692", "TEXT": "Numerical Solution of a Thin Plate Heat Transfer Problem: The numerical solution of a system of linear equations resulting from a discrete approximation to a thin plate heat transfer problem is considered. The slow convergence of point iterative methods is analyzed and shown to be caused by one of the boundary conditions. The difficulty may be removed by a standard line iterative technique."}
{"DOCID": "1693", "TEXT": "GPL, a Truly General Purpose Language: A truly general purpose programming language, GPL, is described which contains facilities for constructing (within the language) new data types as well as facilities for operations performed upon them.  The basic language is minimal in the sense that no basic element can be derived from the others with high efficiency in the object programs.  Constructs like the ALGOL 60 for-statements,and if-statements are not basic; they are special types of procedures. New \"symbols\" (underlined words in ALGOL 60) are implicitly defined by usage in other declarations.  As part words are definable, packed words are handled as easily as full words.  \"Address\" variables (pointers) are included in full generality."}
{"DOCID": "1694", "TEXT": "An Algorithm for the Probability of the Union of a Large Number of Events: An algorithm is presented which efficiently evaluates the probability for the union of n independent and not mutually exclusive events. The problem is that of evaluating the sums of the products of all possible combinations of n variables in minimum time and storage space."}
{"DOCID": "1695", "TEXT": "PLEXUS-An On-Line System for Modeling Neural Networks: A description is presented of PLEXUS, a system which enables a user to construct and specify a neural network, to analyze the output data produced by the network, and to store and retrieve networks and data from a library.  The system, operated entirely from a digital display unit, interacts directly with the user and permits easy and rapid transitions between the various phases of the modeling process. PLEXUS is designed to complement neurophysiological research so that the systematic development of neural models can be coordinated with experimental work.  PLEXUS networks are built up from components representing individual neurons, external stimuli, and interconnecting fibers, each component being of a relatively detailed nature.  Provision is also made for the use of experimental data as input to a network.  Convenient means for specification and modification of a network and extensive error-checking capabilities are provided. Data resulting from the simulation of a network may be analyzed by a variety of techniques ranging from examinations of the gross characteristics of the data to the determination of detailed statistical properties."}
{"DOCID": "1696", "TEXT": "An Algorithm for Identifying the Ergodic Subchains and Transient States of a Stochastic Matrix: An algorithm for identifying the ergodic subchains and transient states of a stochastic matrix is presented.  Applications in Markov renewal programming and in the construction of variable length codes are reviewed, and an updating procedure for dealing with certain sequences of stochastic matrices is discussed.  Computation times are investigated experimentally and compared with those of another recently propose method."}
{"DOCID": "1697", "TEXT": "Graphical Input/Output of Nonstandard Characters: A system developed at Harvard for graphically inputting and outputting nonstandard characters on a computer is printed.  In principle, the system can deal with any orthography, although at present it is limited to 4000 Chinese characters and some mathematical symbols.  New characters can be added to the repertoire of the system by graphical input on a display scope.  Text inputting is accomplished via a display scope or a Rand Tablet.  The organization and operation of the current system are described, and a discussion of the relative merits of such a system is given.  Illustrations of the computer input and output of Chinese characters are included."}
{"DOCID": "1698", "TEXT": "A Statistical Model for Console Behavior in Multiuser Computers: The ability of a computer system to communicate with the outside world efficiently is as important as its ability to perform computations efficiently. It is quite difficult to characterize a particular user, but rather easy to characterize the entire user community. Based on the properties of this community we have postulated a hypothetical \"virtual console.\" No claim is made that a virtual console behaves like any actual console, but the entire collection of virtual consoles models the collection of actual consoles.  Using the model we answer questions like: How many processes are suspended waiting for console input?  What is the maximum rate at which a process can execute?  What bounds can be set on overall buffer requirements?  Answers to these and similar questions are needed in certain aspects of operating system design."}
{"DOCID": "1699", "TEXT": "Experimental Evaluation of Information Retrieval Through a Teletypewriter: Experiments designed to evaluate the capabilities of mechanized information retrieval systems, with emphasis on interactive (man-machine) language and on some of the mechanical and psychological limitations in their design, were conducted at the Moore School information Systems Laboratory.  The basic assumption of the research is that an information retrieval system that provides for man-machine dialogue at a remote inquiry terminal should provide a searcher with many of the tools which would be available to him were he actually performing his search at a library or repository of documents.  Factors involved in evaluation of such a system include ease of use, learning time, and effectiveness of actual retrieval.  Three experiments and the conclusions resulting from them are detailed."}
{"DOCID": "1700", "TEXT": "PEEKABIT, Computer Offspring of Punched Card PEEKABOO, for Natural Language Searching: The \"peekaboo\" idea from punched card information retrieval methods has been mated with the idea of superimposed punching to produce a programming technique which cuts computer run time in half on a test search of 33,000 subject index entries.  A search program using the device has been operational since late 1963.  As an item is entered in the store, an 18-byte mask is created from the item's meaningful words using the inclusive OR operation.  If, at search time, the logical product (using the AND operation) of this mask and a similarly constructed question mask is not equal to the question mask, then one or more question words are not present in the store item. An equality is in conclusive; the words of the store item must be unpacked and compared with question words.  The present store is made up of over 600,000 subject index entries estimated to average 60 characters each.  Longer texts, such as abstracts, could be handled by multiple masks."}
{"DOCID": "1701", "TEXT": "Synchronous Signaling Rates for Data Transmission* (Proposed USA STandard)"}
{"DOCID": "1702", "TEXT": "Commentary on Mr. Mooers' Paper"}
{"DOCID": "1703", "TEXT": "Accommodating Standards and Identification of Programming Languages: The user public wants standardization and reliable identification of programming languages and related services.  One way of achieving these goals illustrated by the methods adopted for TRAC T-64 interactive language, and its related family of languages. Oppressive rigidity usually associated with standardization is avoided by a new accommodation technique accessible to the user to allow local variations with the language.  Explicit standardization of the language is undertaken at the organizational source of the language.  Use of the organizational trademark (TRAC) on the published standards, and services relying upon them, provides a reliable public identification. These methods can be usefully applied to other programming languages and computer services."}
{"DOCID": "1704", "TEXT": "Minimum Excess Cost Curve (ALgorithm 217 [H])"}
{"DOCID": "1705", "TEXT": "A Set of Basic Input-Output Procedures (Algorithm 335 [15]): By means of the primitives in symbol, outsymbol and length, as requested by this journal's Algorithms Policy [Comm. ACM 10 (Nov. 67), 729] a basic set of input-output procedures is defined aiming at quality and flexibility.  Outreal, for instance, is written as a derived procedure; it outputs using the fixed point or the floating point representation, and rounds properly.  Variants can easily be written because of the explicit call of the procedures decompose integer and decompose real.  The highly recommended practice of echoing input is made easy with one subset of derived procedures (ioi, ior, iob, ioa).  The documentation of output in the form of equivalent ALGOL statements is also provided when use is made of the subset oti, otr, otb, ota.  The Berkeley style of providing information on the form of output using prior calls of procedures such as real format is defined.  A use of the parameter outchannel to provide information for simultaneous output to several channels is suggested.  Interrelationship between the declared procedures is furnished in tabular form."}
{"DOCID": "1706", "TEXT": "CHAMP-Character Manipulation Procedures: A new programming language facility for symbol manipulation is described.  String procedures may be declared and called in a standard ALGOL context. ALGOL procedures can in turn be called by string procedures so that numeric and symbolic processes may conveniently be programmed together.  Concatenation and a variant of SNOBOL's pattern matching make up a set of primitive commands.  These are assembled together into conditional expressions which are to be used to provide alternative computational patterns. Arrays of strings are processed using quantifiers. The class of things which may be assigned to an identifier can be restricted by a procedure expressed in the notation.  The language facilities have been implemented in the ALGOL compiler for the Burroughs B5500."}
{"DOCID": "1707", "TEXT": "Generation of Positive Test Matrices with Known Positive Spectra: Sufficient conditions are given for a real matrix to be similar to a positive matrix.  This result is used to construct a similarity transformation which, when applied to a particular upper triangular matrix, yields a positive matrix with a preassigned positive spectrum."}
{"DOCID": "1708", "TEXT": "A Note on the Efficiency of a LISP Computation in a Paged Machine: The problem of the use of two levels of storage for programs is explored in the context of a LISP system which uses core memory as a buffer for a large virtual memory stored on a drum.  Details of timing are given for one particular problem."}
{"DOCID": "1709", "TEXT": "A Modification of Efroymson's Technique for Stepwise Regression Analysis: The computational technique conventionally used for stepwise multiple linear regression requires the storage of an n X n matrix of data.  When the number of variables, n, is large, this requirement taxes the storage capacity of presently used machinery. The near symmetry of the matrices involved permits a modification requiring only half the storage and computations of the conventional algorithm and this additional storage allows the analysis of problems containing more variables.  Alternatively, it permits the analysis of problems containing the same number of variables but with all computations performed in double precision."}
{"DOCID": "1710", "TEXT": "ASP-A Ring Implemented Associative Structure Package: ASP is a general purpose Associative Data Structure Package in which an arbitrary number of data items and an arbitrary number of the relationships between these data items may be represented. A special picture language is described which has proved very useful for drawing ASP structures on paper. ASP structures are built and manipulated by means of a series of macro calls, which are outlined in the Appendix.  Emphasis is on the philosophy of the system rather than a particular implementation, though sufficient information is included to enable the reader to produce his own implementation of ASP."}
{"DOCID": "1711", "TEXT": "When Your Computer Needs a Lawyer: Possible liability for negligence, for other torts (such as slander of credit) and for liability under theories of express or implied warranty (guarantees) are discussed, and legal complications are explained, so that users, operators, owners, and leasors of computers may be alerted to potential legal problems. Focus is also on trouble spots in contracting for data processing services, in automating record keeping operations, in deciding whether or not to automate certain operations, and in complying with statutes and regulation relating to record keeping. Information is given on patents, copyrights and trade secret protection for programs, and the problem of using copyrighted material in information storage and retrieval systems, including the pending copyright and patent revision bills."}
{"DOCID": "1712", "TEXT": "Recovery of Disk Contents After System Failure: A method is discussed by which, after a system malfunction, the contents of disk files can be restored to their status at the time of the failure."}
{"DOCID": "1713", "TEXT": "On Overcoming High-Priority Paralysis in Multiprogramming Systems: A Case His tory: High-priority paralysis is the degradation that can occur in multiprogramming systems when scheduling is based primarily on preassigned priorities. It can be alleviated by modifying the scheduling algorithm to maximize the number of programs active at one time.  The case his tory given in this paper indicates two general methods by which simultaneity can be increased.  Possible refinements in the scheduling algorithm for future improvements are considered briefly."}
{"DOCID": "1714", "TEXT": "Procedure for the Normal Distribution (Algorithm 272 [S15])"}
{"DOCID": "1715", "TEXT": "Direct Search (Algorithm 178 [E4])"}
{"DOCID": "1716", "TEXT": "Normal Random Deviates (Algorithm 334 [G5])"}
{"DOCID": "1717", "TEXT": "Generating Prime Implicants Via Ternary Encoding and Decimal Arithmetic: Decimal arithmetic, ternary encoding of cubes, and topological considerations are used in an algorithm to obtain the extremals and prime implicants of Boolean functions. The algorithm, which has been programmed in the FORTRAN language, generally requires less memory than other minimization procedures, and treats DON'T CARE terms in an efficient manner."}
{"DOCID": "1718", "TEXT": "\"Logical\" Arithmetic on Computers with Two's Complement Binary Arithmetic: Algorithms are presented for multiplication and division of unsigned integer operands in which the digits normally reserved for signs participate as significant arithmetic digits with positive weight."}
{"DOCID": "1719", "TEXT": "A Methodology for Calculating and Optimizing Real-Time System Performance: The continually increasing size, complexity, number of types, and cost of data processing systems are causing serious re-examination within government and industry of the criteria for and methods of calculating and optimizing data processing system cost and performance.  Real-time data processing systems as typified by the automated airline reservation system are discussed in this paper.  Criteria for evaluating performance are described; a methodology for calculating and optimizing is outlined; and the method is illustrated by carrying out a portion of the performance calculation and the optimization of a drum-oriented message switching system."}
{"DOCID": "1720", "TEXT": "Master's Level Computer Science Curricula: The results of a survey of the course work done by master's degree candidates at 25 US universities are presented, and some general comments concerning the emphasis of these programs are given."}
{"DOCID": "1721", "TEXT": "Determination of the Intersection Points of Two Plane Curves by Means of Differential Equations: A new method is proposed to calculate the intersection points of two plane curves.  The theory of singular points off a system of two differential equations is used in developing the method. The intersection point to be determined is identified with such a singular point and appropriate modifications are applied to the system to ensure that the singular point be stable, i.e. all integrals which start in the neighborhood of the singular point will always approach this point if the integral parameter tends to infinity.  In addition a method is described for systematically searching for all intersection points in a prescribed rectangular area."}
{"DOCID": "1722", "TEXT": "Methods of Convergence Improvement for Some Improper Integrals: In the numerical integration of an improper integral of the first kind, it is customary to truncate the integral when the change yielded by the last iteration is less than some predetermined constant. The efficiency of such integration schemes can often be improved by use of recent advances in the theory of nonlinear transformations; however, for several important integrals, e.g. integrals whose integrands are rational polynomials, these transformations fail to yield much improvement.  In this paper, several methods of convergence improvement are developed which greatly improve convergence of some improper integrals, including the integrals of rational polynomials."}
{"DOCID": "1723", "TEXT": "Computer Construction of Project Networks: Project networks are used in PERT and CPM. An algorithm is given for constructing project networks directly from the project precedence relations. The algorithm creates \"dummy\" activities and topologically orders the arcs and nodes.  The number of nodes created is minimal for the given precedence relations.  It has been experimentally programmed in FORTRAN II for the IBM 7094."}
{"DOCID": "1724", "TEXT": "A Generalized Partial Pass Block Sort: The design of a partial pass block sort with arbitrary range of key and number of work files is described. The design is a generalization of the Partial Pass Column Sort by Ashenhurst and the Amphisbaenic Sort by Nagler. The power of the sort is tabulated for various sizes of input file and number of work files. consideration is given to the problem of combining a block sort with internal sorts, and to the best use of direct access storage devices."}
{"DOCID": "1725", "TEXT": "A Simple Proof of Lewin's Ordered-Retrieval Theorem for Associative Memories: An efficient method of ordered retrieval of binary words from an associative memory, as described by Lewin, is based on the use of special readout circuits which indicate the digit values present in the individual digit columns of the memory.  Thus the circuits indicate whether the individual digit columns contain digits of both values, or of only one value, or contain no digits at all (i.e. that the memory is empty).  The use of these circuits, which in this paper are termed column value indicators, reduces considerably the number of memory accesses necessary to retrieve in order a number of distinct words from the memory.  Lewin proves that, for the readout by the described method of m distinct binary words, 2m - 1 memory accesses are necessary.  (Thus he proves that the number of necessary memory accesses of his method, unlike those of other methods, is independent of the word length.)  In this paper a very simple proof of this theorem derived from some elementary aspects of the structure of sets of binary numbers is presented."}
{"DOCID": "1726", "TEXT": "Preliminary Investigation of Techniques for Automated Reading of Unformatted Text: Methods for converting unstructured printed material into computer code are experimentally investigated.  An operator-controlled mode, depending on human demarcation of the various regions of the page for guiding the scanner, is implemented by means of a joystick and a CRT display.  This mode, for which some performance figures are obtained, is thought to be suitable for processing very complicated material, such as technical journals.  For simpler material, for instance the \"claims\" sections of patents, and in applications where the utmost accuracy is not necessary, an unsupervised mode is advocated.  Here, the textual portions of the page are located during a rapid prescan by a rudimentary form of frequency analysis.  These areas are then rescanned at a higher resolution suitable for character recognition. Error rates of the order of 0.1 percent are obtained in a simple problem involving photographs of telephone company meter boards.  Other matters related to the design of a general purpose page reader, such as the segmentation of printed text, the possibility of time-sharing the scanner, interactive man-machine operation, and the facsimile reproduction of illustrations, are discussed."}
{"DOCID": "1727", "TEXT": "One Way of Estimating Frequencies of Jumps in a Program: For the segmentation of a program it is useful to have a reasonable estimation of the values of S(ij), where S(ij) is the mean value of the number of jumps from the i-th instruction on to the j-th instruction in the run time.  In the cases where the S(ij) are estimated directly, the structure of the whole program must be generally taken into account; therefore it is very difficult for the programmer and/or the translator to obtain a good estimation of the S(ij).  It is easier to estimate not S(ij) but the quantities P(ij)=S(ij)*C(i)/SUM[S(ij), j=1,N], where C(i) is an arbitrary positive constant for each i.  Although the P(ij) are, for each i, proportional to S(ij), the estimation of P(ij) is easier, because we must estimate only the \"probabilities\" of events where instruction i is executed after instruction I(i).  This estimation can often be done without considering the structure of the whole program.  In the first part of the paper, using the theory of the Markov chains, an algorithm for the computation of the S(ij) from the P(ij) is found, and some ways of obtaining estimates of the P(ij) are given.  In the second part a variant of this algorithm is derived, avoiding the necessity of computation involving large matrices."}
{"DOCID": "1728", "TEXT": "Further Experimental Data on the Behavior of Programs in a Paging Environment: Results are summarized from an empirical study directed at the measurement of program operating behavior in those multiprogramming systems in which programs are organized into fixed length pages. The data collected from the interpretive execution of a number of paged programs are used to describe the frequency of page faults, i.e. the frequency of those instants at which an executing program requires a page of data or instructions not in main (core) memory. These data are used also for the evaluation of page replacement algorithms and for assessing the effects on performance of changes in the amount of storage allocated to executing programs."}
{"DOCID": "1729", "TEXT": "Minit Algorithm for Linear Programming (Algorithm 333 [H])"}
{"DOCID": "1730", "TEXT": "Jacobi Polynomials (Algorithm 332 [S22])"}
{"DOCID": "1731", "TEXT": "Gaussian Quadrature Formulas (Algorithm 331 [D1])"}
{"DOCID": "1732", "TEXT": "Factorial Analysis of Variance (Algorithm 330 [G1])"}
{"DOCID": "1733", "TEXT": "Distribution of Indistinguishable Objects into Distinguishable slots (Algorithm [G6])"}
{"DOCID": "1734", "TEXT": "Chebyshev Solution to an Overdetermined Linear System (Algorithm 328 [F4])"}
{"DOCID": "1735", "TEXT": "A Futures Market in Computer time: An auction method is described for allocating computer time that allows the price of computer time to fluctuate with the demand and the relative priority of users to be controlled so that more important projects get better access.  This auction is free of the periodic fluctuation in computer use often associated with monthly time allocation schemes."}
{"DOCID": "1736", "TEXT": "Heading Format for Data Transmission (A USAAI Tutorial -- Standards)"}
{"DOCID": "1737", "TEXT": "A Global Parser for Context-Free Phrase Structure Grammars"}
{"DOCID": "1738", "TEXT": "Writing an Outline Debugging Program for the Experienced User: Presently available online debugging routines are often unsatisfactory for the experienced user because they require unnecessarily rigid and complicated typing formats, make it difficult for the user to correct typing errors, and consume excessive memory with intricate features.  In a debugging program it is of prime importance that the program be simple, flexible, and highly efficient to use. Communication between the user and the debugging program can be improved by using certain techniques applicable to most online debugging programs.  These techniques are presented and are illustrated by their use in OPAK (octal package), a debugging program coded for the PDP-5/8 and the SDS-930.  The compromise between economy of utility program core storage and incorporation of elegant debugging features is discussed."}
{"DOCID": "1739", "TEXT": "Regular Expression Search Algorithm: A method for locating specific character strings embedded in character text is described and an implementation of this method in the form of a compiler is discussed.  The compiler accepts a regular expression as source language and produces an IBM 7094 program as object language.  The object program then accepts the text to be searched as input and produces a signal every time an embedded string in the text matches the given regular expression.  Examples, problems, and solution are also presented."}
{"DOCID": "1740", "TEXT": "An Inexpensive Braille Terminal Device: The active use of time-shared facilities for blind programmers requires a braille terminal system.  Details are given for the construction of a brailler from a model 33 teletype by modifying the print head and increasing the resiliency of the platen. A description of the programming needed to drive the brailler is presented."}
{"DOCID": "1741", "TEXT": "BRAD: The Brookhaven Raster Display: A multiconsole computer display system has been designed that provides very rich displays at low unit cost.  Each BRAD (Brookhaven Raster Display) console can plot tens of thousands of points, or up to 4000 characters at 30 frames per second.  After an initial display system investment of $50,000 each display, with teletype, costs less than $3,000. The technique employed is that of programmatically generating a binary image of the desired display in a computer.  The image is written on a rotating drum memory.  Independent read heads continuously display the picture, which is generated by swept horizontal lines.  A standard TV monitor serves as the display device. The technique has two drawbacks.  A computer must compute any image to be displayed.  Also, the \"pointing\" interaction is more difficult.  This is because the pointing function gives only the coordinates of the point on the screen.  The inverse of the map generation process is required to calculate the coordinates of the point on the screen.  The inverse of the map generation process is required to calculate the coordinates at the selected point in the input space."}
{"DOCID": "1742", "TEXT": "On the Design of Display Processors: The flexibility and power needed in the data channel for a computer display are considered. To work efficiently, such a channel must have a sufficient number of instructions that it is best understood as a small processor rather than a powerful channel. As it was found that successive improvements to the display processor design lie on a circular path, by making improvements one can return to the original simple design plus one new general purpose computer for each trip around.  The degree of physical separation between display and parent computer is a key factor in display processor design."}
{"DOCID": "1743", "TEXT": "Reliable Full-Duplex file Transmission over Half-Duplex Telephone Lines: A field-proven scheme for achieving reliable duplex transmission over a half-duplex communication line is presented, and to demonstrate the difficulty of the problem, another similar scheme, which is only slightly unreliable, is also presented.  A flowchart for the reliable scheme and some interesting examples are given."}
{"DOCID": "1744", "TEXT": "Stable Numerical Methods for Obtaining the Chebyshev Solution to an Overdetermined System of Equations: An implementation of Stiefel's exchange algorithm for determining a Chebyshev solution to an overdetermined system of linear equations is presented, that uses Gaussian LU decomposition with row interchanges.  The implementation is computationally more stable than those usually given in the literature. A generalization of Stiefel's algorithm is developed which permits the occasional exchange of two equations simultaneously."}
{"DOCID": "1745", "TEXT": "A Position Paper on Computing and Communications: The effective operation of free enterprise in creating the envisioned information service industry is dependent upon three accomplishments: (1) the restructuring of our information processing industry so that a clear division of costs is made among computing, communications, and the development of information services; (2) the wide use of multiaccess system concepts so that information services may share in the use of computer installations and so that the cost of their construction is reasonable; and (3) the development of public, message-switched communications services so that adequate provisions are made for information security."}
{"DOCID": "1746", "TEXT": "Protection in an Information Processing Utility: One of the critical problems in the design of an information processing utility that permits flexible sharing of user information is privacy. One solution for this problem is discussed."}
{"DOCID": "1747", "TEXT": "Three Criteria for Designing Computing Systems to Facilitate Debugging: The designer of a computing system should adopt explicit criteria for accepting or rejecting proposed system features.  Three possible criteria of this kind are input recordability, input specifiability, and asynchronous reproducibility of output.  These criteria imply that a user can, if he desires, either know or control all the influences affecting the content and extent of his computer's output.  To define the scope of the criteria, the notion of an abstract machine of a programming language and the notion of a virtual computer are explained.  Examples of applications of the criteria concern the reading of a time-of-day clock,  the synchronization of parallel processes, protection in multiprogrammed systems, and the assignment of capability indexes."}
{"DOCID": "1748", "TEXT": "A Scheduling Philosophy for Multiprocessing Systems: A collection of basic ideas is presented, which have been evolved by various workers over the past four years to provide a suitable framework for the design and analysis of multiprocessing systems. The notions of process and state vector are discussed, and the nature of basic operations on processes is considered.  Some of the connections between processes and protection are analyzed.  A very general approach to priority-oriented scheduling is described, and its relationship to conventional interrupt systems is explained.  Some aspects of time-oriented scheduling are considered. The implementation of the scheduling mechanism is analyzed in detail and the feasibility of embodying it in hardware established. Finally, several methods for interlocking the execution of independent processes are presented and compared."}
{"DOCID": "1749", "TEXT": "The Structure of the \"THE\"-Multiprogramming System: A multiprogramming system is described in which all activities are divided over a number of sequential processes.  These sequential processes are placed at various hierarchical levels, in each of which one or more independent abstractions have been implemented.  The hierarchical structure proved to be vital for the verification of the logical soundness of the design and the correctness of its implementation."}
{"DOCID": "1750", "TEXT": "Considerations in the Design of a Multiple Computer System with Extended Core Storage: The use of large quantities of addressable (but not executable) fast random access memory to heighten the multiprogramming performance of a multicomputer system is discussed.  The general design of the hardware arrangement and the software components and functions of such a system are based on a planned configuration of dual CDC 6600's that share one million words of extended core storage.  In the generalization of such a design, special emphasis is placed on estimating expected gains when compared with the traditional configuration of separate and independent computers without extended core storage. An observation is made on the use of conventional, slower speed, random access storage devices in place of the faster memory."}
{"DOCID": "1751", "TEXT": "The Working Set Model for Program Behavior: Probably the most basic reason behind the absence of a general treatment of resource allocation in modern computer systems is an adequate model for program behavior.  In this paper a new model, the \"working set model,\" is developed. The working set of pages associated with a process, defined to be the collection of its most recently used pages, provides knowledge vital to the dynamic management of paged memories.  \"Process\" and \"working set\" are shown to be manifestations of the same ongoing computational activity; then \"processor demand\" and \"memory demand\" are defined; and resource allocation is formulated as the problem of balancing demands against available equipment."}
{"DOCID": "1752", "TEXT": "Resource Management for a Medium Scale Time-Sharing Operating system: Task scheduling and resource balancing for a medium size virtual memory paging machine are discussed in relation to a combined batch processing and time-sharing environment.  A synopsis is given of the task scheduling and paging algorithms that were implemented, and the results of comparative simulation are given by tracing the development of the algorithms through six predecessor versions.  Throughout the discussion particular emphasis is placed on balancing the system performance relative to the characteristics of all the system resources.  Simulation results relative to alternate hardware characteristics and the effects of program mix and loading variations are also presented."}
{"DOCID": "1753", "TEXT": "Virtual Memory, Processes, and Sharing in MULTICS: Some basic concepts involved in the design of the MULTICS operating system are introduced. MULTICS concepts of processes, address space, and virtual memory are defined and the use of paging and segmentation is explained.  The means by which users may share procedures and data is discussed and the mechanism by which symbolic references are dynamically transformed into virtual machine addresses is described in detail."}
{"DOCID": "1754", "TEXT": "Dynamic Storage Allocation Systems: In many recent computer system designs, hardware facilities have been provided for easing the problems of storage allocation.  A method of characterizing dynamic storage allocation systems-according to the functional capabilities provided and the underlying techniques used-is presented.  The basic purpose of the paper is to provide a useful perspective from which the utility of various hardware facilities may be assessed.  A brief survey of storage allocation facilities in several representative computer systems is included as an appendix."}
{"DOCID": "1755", "TEXT": "Proceedings of the ACM Symposium on Operating system Principles"}
{"DOCID": "1756", "TEXT": "Hollerith Punched Card Code* (Proposed USA Standard)"}
{"DOCID": "1757", "TEXT": "Data Code for Calendar Date for Machine-to-Machine Data Interchange* (Proposed USA Standard)"}
{"DOCID": "1758", "TEXT": "Symmetric Polynomials, (Algorithm 305 [C1])"}
{"DOCID": "1759", "TEXT": "Transportation Problem (Algorithm 293 [H])"}
{"DOCID": "1760", "TEXT": "Normal Curve Integral (Algorithm 304 [S15])"}
{"DOCID": "1761", "TEXT": "Chi-Squared Integral (Algorithm 299 [S15])"}
{"DOCID": "1762", "TEXT": "Dilogarithm (Algorithm 327 [S22])"}
{"DOCID": "1763", "TEXT": "Roots of Low-Order Polynomial Equations (Algorithm 326 [C2])"}
{"DOCID": "1764", "TEXT": "Panel Discussion on Computer Appreciation: Session 19 of the ACM 20 th Anniversary Conference on August 31, 1967, was entitled Education, Design Experiments, and Computer Appreciation.  Its second half consisted of a panel discussion on computer appreciation, organized and chaired by Elliot I. Organick. The four panelists were Charles H. Davidson, Bernard A. Galler, Richard, W. Hamming, and Alan J. Perlis. After making prepared statements, the panelists were joined in discussion by Andries van Dam and Arthur B.Kohn, who had presented papers in the first half.  This is a transcript of the panel discussion, condensed by Dr. Organick and edited by him and the panelists.  Some remarks referred to papers by van Dam and Kahn or to the discussion during the first half of the session.  Pertinent papers are included in the references."}
{"DOCID": "1765", "TEXT": "Expenditures, Sources of Funds, and Utilization of Digital Computers for Research and Instruction in Higher Education: 1964-65 with Projections for 1968-69: The Southern Regional Education Board published a complete report on a survey it conducted to determine the funding and characterize the utilization of computers used for research and instruction in institutions of higher education in the United States. The sampling survey is described and the estimates for this total population are presented."}
{"DOCID": "1766", "TEXT": "Quasilinearization and the Estimation of Differential Operators from Eigenvalues: Given a linear ordinary differential operator containing several unknown constants and a number of its eigenvalues, the values of the unknown constants are estimated.  A precise formulation is provided, and an effective numerical procedure for solution is indicated. The results of some computational experiments are given."}
{"DOCID": "1767", "TEXT": "A General Purpose Graphic Language: Interactive use of computers with graphic terminals will permit many new problems to be solved using machines.  In order to handle a variety of applications, it is expedient to develop a general purpose graphic language that is useful on a number of graphic devices.  A system has been designed to produce such a language quickly and cheaply.  A model graphic language which has been developed with the system is presented."}
{"DOCID": "1768", "TEXT": "A Global Parser for Context-Free Phrase Structure Grammars: An algorithm for analyzing any context-free phrase structure grammar and for generating a program which can then parse any sentence in the language (or indicate that the given sentence is invalid) is described. The parser is of the \"top-to-bottom\" type and is recursive . A number of heuristic procedures whose purpose is to shorten the basic algorithm by quickly ascertaining that certain substrings of the input sentence cannot correspond to the target nonterminal symbols are included.  Both the generating algorithm and the parser have been implemented in RCA SNOBOL and have been tested successfully on a number of artificial grammars and on a subset of ALGOL.  A number of the routines for extracting data about a grammar, such as minimum lengths of N-derivable strings and possible prefixes, are given and may be of interest apart from their application in this particular context."}
{"DOCID": "1769", "TEXT": "The Expanding World of Computers: The onward sweep of automatic processing of information is impeded by nine principal barriers: geography, cost, problem complexity, man-machine communication, inadequate sensors, lack of understanding, distance, time, and size.  The main incentive for breaching these barriers is the universal need for processing information, ever more urgent as the greater part of human work activity changes from production to service.  Computer developments in hardware, programming, time-sharing, education, data communication, and displays are judged by how effectively they remove these barriers, and their barrier-smashing potentialities indicate continued rapid expansion.  Problem-oriented languages are particularly effective over the entire front.  Online computers and time-sharing also rate high by this measure.  Education and increased understanding are basic to all progress with the computer.  This complex but powerful tool is the most important one available to governments and scientists to use in studying the problems being created by the population explosion, and in analyzing possible solutions."}
{"DOCID": "1770", "TEXT": "Rules of Ethics in Information Processing: The background and motivation for the adoption by the ACM Council on November 11, 1966, of a set of Guidelines for Professional Conduct in Information Processing are described.  A brief his tory is given of ethical codes in other professions.  Some reasons for and against adoption of ethical rules are considered, and several sections of the ACM Guidelines are analyzed.  The purpose is to inform about this important aspect of our profession, as well as to stimulate thought and interest."}
{"DOCID": "1771", "TEXT": "CURRICULUM 68 -- Recommendations for Academic Programs in Computer Science -- A Report of the ACM Curriculum Committee on Computer science: This report contains recommendations on academic programs in computer science which were developed by the ACM Curriculum Committee on Computer Science. A classification of the subject areas contained in computer science is presented and twenty-two courses in these areas are described.  Prerequisites, catalog descriptions, detailed outlines, and annotated bibliographies for these courses are included. Specific recommendations which have evolved from the Committee's 1965 Preliminary Recommendations are given for undergraduate programs.  Graduate programs in computer science are discussed and some recommendations are presented for the development of master's degree programs. Ways of developing guidelines for doctoral programs are discussed, but no specific recommendations are made. The importance of service courses, minors, and continuing education in computer science is emphasized.  Attention is given to the organization, staff requirements, computer resources, and other facilities needed to implement computer science educational programs."}
{"DOCID": "1772", "TEXT": "USASCSOCR Dual Case Keyboard Arrangement* (Proposed USA Standard)"}
{"DOCID": "1773", "TEXT": "General Purpose Alphanumeric Keyboard Arrangement for Information Interchange* (Proposed USA Standard)"}
{"DOCID": "1774", "TEXT": "Program Overlay Techniques: The general features of program overlay systems are described.  Three main types -- automatic, semiautomatic and nonautomatic -- are classified, and the programming techniques are explained as a function of machine hardware and other system features.  The implementation of semiautomatic overlay facility in a multiprogrammed system on the CDC 6600 is described in detail, with special reference to real time applications."}
{"DOCID": "1775", "TEXT": "Adjustment of the Inverse of a Symmetric Matrix when Two Symmetric Elements are Changed (Algorithm 325 [F1])"}
{"DOCID": "1776", "TEXT": "Maxflow (Algorithm 324 [H])"}
{"DOCID": "1777", "TEXT": "Generation of Permutations in Lexicographic Order (Algorithm 323 [G6])"}
{"DOCID": "1778", "TEXT": "F-Distribution (Algorithm 322 [S14])"}
{"DOCID": "1779", "TEXT": "t-Test Probabilities (Algorithm [S14])"}
{"DOCID": "1780", "TEXT": "Harmonic Analysis for Symmetrically Distributed Data (Algorithm 320 [C6])"}
{"DOCID": "1781", "TEXT": "Translator Writing systems: A critical review of recent efforts to automate the writing of translators of programming languages is presented.  The formal study of syntax and its application to translator writing are discussed in Section II.  Various approaches to automating the post syntactic (semantic) aspects of translator writing are discussed in Section III, and several related topics in Section IV."}
{"DOCID": "1782", "TEXT": "A Numerical Integration Formula Useful in Fourier Analysis: A numerical integration formula is presented which uses unequal sampling intervals.  The intervals are equally spaced on a log scale.  Such a formulation is useful in Fourier analysis to improve accuracy and ease of usage.  A complete set of formulas for numerical Fourier analysis is given."}
{"DOCID": "1783", "TEXT": "In-and-Out Conversions: Byan in-and-out conversion we mean that a floating-point number in one base is converted into a floating-point number in another base and then converted back to a floating-point number in the original base.  For all combinations of rounding and truncation conversions the question is considered of how many significant digits are needed in the intermediate base to allow such in-and-out conversions to return the original number (when possible), or at least significant digit."}
{"DOCID": "1784", "TEXT": "Practical Error Coefficients for Estimating Quadrature Errors for Analytic Functions: All published error coefficients for estimating quadrature errors for analytic functions were computed on the assumption that the quadrature rule was exact for polynomials up to a given degree. Since these rules use rounded values for the abscissas and weights and since the true values of the integrals of some of the polynomials in question have an infinite binary expression, the quadrature rule is not exact.  Hence these errors must be taken into consideration in computing practical error coefficients."}
{"DOCID": "1785", "TEXT": "Scatter Storage Techniques: Scatter storage techniques as a method for implementing the symbol tables of assemblers and compilers are reviewed and a number of ways of using them more effectively are presented.  Many of the most useful variants of the techniques are documented."}
{"DOCID": "1786", "TEXT": "An Improved Hash Code for Scatter Storage: Introduced is a hash coding method based on fixed-point division rather than multiplication or logical operations.  This new method allows the hash table to have almost any length.  Also a new method of handling collisions is discussed.  Known as quadratic search, this method is faster than random search and free from the \"clusters\" that build up with a linear search."}
{"DOCID": "1787", "TEXT": "Use of Transition Matrices in Compiling: An algorithms is described which constructs from a suitable BNF grammar an efficient left-right recognizer for sentences of the corresponding language. The type of recognizer, used in a number of compilers, operates with a pushdown stack and with a transition matrix.  Two examples illustrate how such recognizers may be used effectively for other purposes besides the usual syntax checking."}
{"DOCID": "1788", "TEXT": "Toward a General Processor for Programming Languages: Many efforts have been made to develop a better way of implementing a higher level programming language than by the construction of a whole new compiler, but so far none has proved generally satisfactory. In this paper, it is contended that a programming language is best described functionally as a body of macro instructions, and that the macro call constitutes a canonical form in terms of which a programming notation may be described.  A supporting discussion of the logical and his torical role of the macro instruction is presented.  Also discussed are the conflict between machine independence and object program efficiency, and the question of where the greatest difficulties lie in compiler construction."}
{"DOCID": "1789", "TEXT": "Logarithm of Gamma Function (Algorithm 291 [S14])"}
{"DOCID": "1790", "TEXT": "Muller's Method for Finding roots of an Arbitrary Function  (Algorithm 196 [C5])"}
{"DOCID": "1791", "TEXT": "Triangular Factors of Modified Matrices (Algorithm 319 [F1])"}
{"DOCID": "1792", "TEXT": "Exploratory Experimental Studies Comparing Online and Off line Programming Performance: Two exploratory experiments were conducted at System Development Corporation to compare debugging performance of programmers working under conditions of on-line and off line access to a computer.  These are the first known studies that measure programmers' performance under controlled conditions for standard tasks.  Statistically significant results of both experiments indicated faster debugging under online conditions, but perhaps the most important practical finding involves the striking individual differences in programmer performance.  Methodological problems encountered in designing and conducting these experiments are described; limitations of the findings are pointed out; hypotheses are presented to account for results; and suggestions are made for further research."}
{"DOCID": "1793", "TEXT": "Presentation of Alphameric Characters for Information Processing* (Proposed American National Standard)"}
{"DOCID": "1794", "TEXT": "A Fast Random Number Generator for IBM 360"}
{"DOCID": "1795", "TEXT": "Optimal Code for Serial and Parallel Computation"}
{"DOCID": "1796", "TEXT": "Index by Subject to Algorithms, 1969"}
{"DOCID": "1797", "TEXT": "Solution of Linear programs in 0-1 (Algorithm 341 [H])"}
{"DOCID": "1798", "TEXT": "Coulomb Wave Functions (Algorithm 300 [S22])"}
{"DOCID": "1799", "TEXT": "Elementary Functions by Continued Fractions (Algorithm 229 [B1])"}
{"DOCID": "1800", "TEXT": "PSIF (Algorithm 147 [S14])"}
{"DOCID": "1801", "TEXT": "Analysis of Variance for Balanced Experiments (Algorithm 367 [G2])"}
{"DOCID": "1802", "TEXT": "Regression Using Certain Direct Product Matrices (Algorithm 366 [G2])"}
{"DOCID": "1803", "TEXT": "Complex Root Finding (Algorithm 365 [C5])"}
{"DOCID": "1804", "TEXT": "Coloring Polygonal Regions (Algorithm 364 [Z])"}
{"DOCID": "1805", "TEXT": "Productivity of Multiprogrammed Computers-Progress in Developing an Analytic Prediction Method: Multiprogramming as it is discussed here is a mode of computer operation in which two or more programs are concurrently in processor memory and proceeding, each using the same central processor unit (CPU) and input-output (I/O) channels.  These programs are actually proceeding intermittently and singly, according to eligibility (readiness to proceed) and priority. It is useful to be able to represent them as proceeding continuously and simultaneously, each at an effective rate, which may be a fraction of that which it would enjoy in the absence of the other programs.  The effective progress rate of each program is sensitive to many detailed characteristics of itself and its co-residents and simulation has been the best available method of predicting it.  This paper presents the results of progress in developing an alternative to simulation, a simulation-tested iterative computation of these rates under certain situations.  The algorithm is sensitive to most of the factors that control the phenomenon, including nonquantitative or topological features of the programs' structures."}
{"DOCID": "1806", "TEXT": "On the Downhill Method: The downhill method is a numerical method for solving complex equations f(z) = 0 on which the only restriction is that the function w = f(z) must be analytical.  An introduction to this method is given and a critical review of relating literature is presented.  Although in theory the method always converges, it is shown that a fundamental dilemma exists which may cause a breakdown in practical applications. To avoid this difficulty and to improve the rate of convergence toward a root, some modifications of the original method are proposed and a program (FORTRAN) based on the modified method is given in Algorithm 365.  Some numerical examples are included."}
{"DOCID": "1807", "TEXT": "Optimization of Expressions in Fortran: A method of optimizing the computation of arithmetic and indexing expressions of a Fortran program is presented.  The method is based on a linear analysis of the definition points of the variables and the branching and DO loop structure of the program. The objectives of the processing are (1) to eliminate redundant calculations when references are made to common subexpression values, (2) to remove invariant calculations from DO loops, (3) to efficiently compute subscripts containing DO iteration variables, and (4) to provide efficient index register usage.  The method presented requires at least a three-pass compiler, the second of which is scanned backward.  It has been used in the development of several FORTRAN compilers that have proved to produce excellent object code without significantly reducing the compilation speed."}
{"DOCID": "1808", "TEXT": "Advanced Cryptographic Techniques for Computers: Cryptographic techniques which can be used to maintain the confidentiality of information processed by computers are dealt with.  Special emphasis is paid to the unique characteristics of computer files that make many cryptographic methods of little use. Relative security, costs, and preferred methods are included in this paper."}
{"DOCID": "1809", "TEXT": "Numerical Analysis in a Ph.D. Computer Science Program: Numerical Analysis is the study of methods and procedures used to obtain \"approximate solutions\" to mathematical problems.  Much of the emphasis is on scientific calculation.  The difficulties of education in such a broad area center around the question of background and emphasis.  The Numerical Analysis program in the Computer Science Department should emphasize an awareness of the problems of computer implementation and experimental procedures.  Nevertheless, there is a need for a solid background in applied mathematics."}
{"DOCID": "1810", "TEXT": "Is Automatic \"Folding\" of Programs Efficient Enough To Displace Manual?: The operation of \"folding\" a program into the available memory is discussed.  Measurements by Brown et al. and by Nelson on an automatic folding mechanism of simple design, a demand paging unit built at the IBM Research Center by Belady, Nelson, O'Neil, and others, permitting its quality to be compared with that of manual folding, are discussed, and it is shown that given some care in use the unit performs satisfactorily under the conditions tested, even though it is operating across a memory-to-storage interface with a very large speed difference.  The disadvantages of prefolding, which is required when the folding is manual, are examined, and a number of the important troubles which beset computing today are shown to arise from, or be aggravated by, this source.  It is concluded that a folding mechanism will probably become a normal part of most computing systems."}
{"DOCID": "1811", "TEXT": "A Case Study in Programming for Parallel-Processors: An affirmative partial answer is provided to the question of whether it is possible to program parallel-processor computing systems to efficiently decrease execution time for useful problems.  Parallel-processor systems are multiprocessor systems in which several of the processors can simultaneously execute separate tasks of a single job, thus cooperating to decrease the solution time of a computational problem. The processors have independent instruction counters, meaning that each processor executes its own task program relatively independently of the other processors.  Communication between cooperating processors is by means of data in storage shared by all processors.  A program for the determination of the distribution of current in an electrical network was written for a parallel-processor computing system, and execution of this program was simulated.  The data gathered from simulation runs demonstrate the efficient solution of this problem, typical of a large class of important problems.  It is shown that, with proper programming, solution time when N processors are applied approaches 1/N times the solution time for a single processor, while improper programming can actually lead to an increase of solution time with the number of processors. Stability of the method of solution was also investigated."}
{"DOCID": "1812", "TEXT": "More on Fortran Random Number Generators"}
{"DOCID": "1813", "TEXT": "Generation of Permutations in Pseudo-Lexicographic Order (Algorithm 308 [G6])"}
{"DOCID": "1814", "TEXT": "Direct Search (Algorithm 178 [E4])"}
{"DOCID": "1815", "TEXT": "Direct Search (Algorithm 178 [E4])"}
{"DOCID": "1816", "TEXT": "Generalized Least Squares Fit By Orthogonal Polynomials (Algorithm 296 [E2])"}
{"DOCID": "1817", "TEXT": "Computation of Fourier Coefficients (Algorithm 255 [C6])"}
{"DOCID": "1818", "TEXT": "Associated Legendre Functions of the First Kind for Real or Imaginary Arguments (Algorithm 47 [S16])"}
{"DOCID": "1819", "TEXT": "Complex Error Function (Algorithm 363 [S15])"}
{"DOCID": "1820", "TEXT": "Generation of Random Permutations (Algorithm 362 [G6])"}
{"DOCID": "1821", "TEXT": "Permanent Function of a Square Matrix I and II (Algorithm 361 [G6])"}
{"DOCID": "1822", "TEXT": "Shortest-Path Forest with Topological Ordering (Algorithm [H])"}
{"DOCID": "1823", "TEXT": "Factorial Analysis of Variance (Algorithm [G1])"}
{"DOCID": "1824", "TEXT": "APAREL-A Parse-Request Language: APAREL is described: this language is an extension to an algorithmic language (PL/I) that provides the pattern-matching capabilities normally found only in special purpose languages such as SNOBOL4 and TMG.  This capability is provided through parse-requests stated in a BNF-like format.  These parse-requests form their own programming language with special sequencing rules.  Upon successfully completing a parse-request, an associated piece of PL/I code is executed.  This code has available for use, as normal PL/I strings the various pieces (at all levels) of the parse.  It also has available as normal PL/I variables, the information concerning which of the various alternatives were successful.  Convenient facilities for multiple input-output streams, the initiation of sequences of parse-requests as a subroutine, and parse-time semantic checks are also included.  APAREL has proven convenient in building a powerful SYNTAX and FUNCTION macro system, an algebraic language preprocessor debugging system, an on-line command parser, a translator for Dataless Programming, and as a general string manipulator."}
{"DOCID": "1825", "TEXT": "A Practical Method for Constructing LR(k) Processors: A practical method for constructing LR(k) processors is developed.  These processors are capable of recognizing and parsing an input during a single no-backup scan in a number of steps equal to the length of the input plus the number of steps in its derivation.  The technique presented here is based on the original method described by Knuth, but decreases both the effort required to construct the processor and the size of the processor produced.  This procedure involves partitioning the given grammar into a number of smaller parts.  If an LR(k) processor can be constructed for each part (using Knuth's algorithm) and if certain conditions relating these individual processors are satisfied, then an LR(k) processor for the entire grammar can be constructed for them. Using this procedure, an LR(1) parser for ALGOL has been obtained."}
{"DOCID": "1826", "TEXT": "A LISP Garbage-Collector for Virtual-Memory Computer Systems: In this paper a garbage-collection algorithm for list-processing systems which operate within very large virtual memories is described.  The object of the algorithm is more the compaction of active storage than the discovery of free storage.  Because free storage is never really exhausted, the decision to garbage collect is not easily made; therefore, various criteria of this decision are discussed."}
{"DOCID": "1827", "TEXT": "Performance Monitoring in a Time-Sharing System: A software measurement facility which is part of a general purpose time-sharing system is described. The Date Collection Facility (DCF) has been implemented in the Michigan Terminal System (MTS) for the System/360 model 67.  It exists for the purpose of monitoring operating system and user program behavior and performance.  The overall structure of MTS is outlined in order to explain the implementation of the DCF.  Events in the system are identified and recorded from within the supervisor, and dumped to magnetic tape by an auxiliary program for off-line processing. Events in user programs which are unrelated to system actions are recorded with a supervisor call. The time of occurrence of each event is accurately recorded, and data items are further identified by job and type.  The overhead associated with data collection and its interference with normal jobs is carefully analyzed, and both are shown to be minimal.  Several examples are given of information obtained with the facility and of applications in which it has been useful.  Some general guidelines are offered for the construction of future monitoring programs."}
{"DOCID": "1828", "TEXT": "Synchronization in a Parallel-Accessed Data Base: The following problem is considered:  Given a data base which can be manipulated simultaneously by more than one process, what are the rules for synchronization which will maximize the amount of parallel activity allowed.  It is assumed that the data base can be represented as a graph.  An example of such a data base is a hierarchy of directories for an on-line file system.  Methods for synchronization of processes are examined; their validity is discussed and their performance compared."}
{"DOCID": "1829", "TEXT": "An Interactive Graphical Display Monitor in a Batch-Processing Environment with Remote Entry: A graphic monitor program is described.  It was developed at Carnegie-Mellon University for the CDC G21 computer, which is a general purpose, batch-processing system with remote entry.  The existing G21 system and the graphics hardware are described. The graphic monitor is a resident auxiliary monitor which provides comprehensive managerial capability over the graphical system in response to commands from the human user.  It also will respond to commands from a user program through a similar interface, where routine calls take the place of manual actions.  Thus the human and program can interact on a symmetrical and equal basis through the medium of the graphic monitor. The choice made in designing the graphic monitor, given the constraints of the existing hardware and computer system, are discussed.  The structure of the monitor program and the human and program interfaces are described.  There is also a transient swapping version with a small resident part, and provision for swapped used submonitors."}
{"DOCID": "1830", "TEXT": "Retrieval Times for a Packed Direct Access Inverted File"}
{"DOCID": "1831", "TEXT": "A Comment on Optimal Tree Structures"}
{"DOCID": "1832", "TEXT": "Minimax Logarithmic Error"}
{"DOCID": "1833", "TEXT": "An Ambiguity in the Description of ALGOL 60"}
{"DOCID": "1834", "TEXT": "An Axiomatic Basis for Computer Programming: In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics.  This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs.  Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed.  Finally, it is argued that important advantages, both theoretical and practical, may follow from a pursuance of these topics."}
{"DOCID": "1835", "TEXT": "The IITRAN Programming Language: The IITRAN language, developed to be used by students, and its important important features are described. IITRAN is a procedure-oriented language with a one-level block structure and a variety of data types.  Several novel and powerful features are included.  A discussion of design principles to be followed in a student language is given."}
{"DOCID": "1836", "TEXT": "A New Method for Determining Linear Precedence Functions for Precedence Grammars: The precedence relations of a precedence grammar can be precisely described by a two-dimensional precedence matrix.  Often the information in the matrix can be represented more concisely by a pair of vectors, called linear precedence functions.  A new algorithm is presented for obtaining the linear precedence functions when given the precedence matrix; this algorithm is shown to possess several computational advantages."}
{"DOCID": "1837", "TEXT": "An Algol Convolution Procedure Based on the Fast Fourier Transform (Algorithm 345 [C6])"}
{"DOCID": "1838", "TEXT": "Normal Curve Integral (Algorithm 304 [S15])"}
{"DOCID": "1839", "TEXT": "Singular Value Decomposition of a Complex Matrix (Algorithm 358 [F1, 4,5])"}
{"DOCID": "1840", "TEXT": "An Efficient Prime Number Generator (Algorithm 357 [A1])"}
{"DOCID": "1841", "TEXT": "A Prime Number Generator Using The Treesort Principle (Algorithm 356 [A1])"}
{"DOCID": "1842", "TEXT": "An Algorithm for Generating Ising Configurations (Algorithm 355 [Z])"}
{"DOCID": "1843", "TEXT": "The Choice of Base: A digital computer is considered, whose memory words are composed on N r-state devices plus two sign bits (two state devices).  The choice of base B for the internal representation of floating-point numbers on such a computer is discussed.  It is shown that in a certain sense B= r is best."}
{"DOCID": "1844", "TEXT": "A Modular Computer Sharing System: An alternative approach to the design and organization of a general purpose interactive multiterminal computing system is presented.  The system organization described is a conceptually simple arrangement of a bank of interchangeable computers, each of which is a memory/processor pair, that are assigned to process terminal jobs as they arrive.  One of the computers serves as the master or control computer and supervises the collection and distribution of messages from and to the remote terminals.  In the simplest form there is a disk drive for each connected terminal.  A crosspoint switching network allows any such disk drive to be connected to any computer. Thus, while each active terminal user \"occupies\" a dedicated disk drive, he may share the computer with many other terminal users in a simple manner. The ratio of users to computers is dependent on both the size and power of the machines used and the computation requirements of the particular mix of users. This system organization is inherently a simpler and therefore more reliable approach to time-sharing computers and has the potential of a highly available system at relatively low cost.  Economic configurations are possible for a range of systems sizes that span at least one order of magnitude.  Finally, problem programs developed by remote terminal users can be run on a dedicated batch system if compatible computers are used."}
{"DOCID": "1845", "TEXT": "Loader Standardization for Overlay Programs: The overlay capability is described for four of the third generation computer systems: CDC-6000, GE-635, IBM-360, and UNIVAC-1108.  A critique of the first three systems is based on actual experience with a large overlaid trajectory simulation program; a short history and description of this program is presented.  A standardization of minimum capabilities for loaders is recommended so that programs which must operate under more than one computer system may be easily converted and maintained.  A proposal that overlay software incorporates a memory occupation specification concept instead of the conditional tree structure is delineated.  This concept provides more efficient and cost-effective utilization of the memory as well as increased flexibility in program structure."}
{"DOCID": "1846", "TEXT": "On Simulating Networks of Parallel Processes in Which Simultaneous Events May Occur: Some of the problems of simulating discrete event systems, particularly computer systems, on a conventional digital computer are dealt with.  The systems are assumed to be described as a network of interconnected sequential processes.  Briefly reviewed are the common techniques used to handle such simulations when simultaneous events do not occur, can be ignored, or can be handled by simple priority rules.  Following this, the problem of dealing with simultaneous events in separate processes is introduced. An abstraction of this problem is developed which admits solution for a majority of commonly encountered problems.  The technique will either find a method of simulating the parallel events or report that none can be found.  In some of the latter cases it is shown to be possible to find a solution by extending the information available to the solution technique, but in many cases the technique becomes computationally unfeasible when the additional information is provided."}
{"DOCID": "1847", "TEXT": "An Algorithm for Finding a Fundamental Set of Cycles of a Graph: A fast method is presented for finding a fundamental set of cycles for an undirected finite graph.  A spanning tree is grown and the vertices examined in turn, unexamined vertices being stored in a pushdown list to await examination.  One stage in the process is to take the top element v of the pushdown list and examine it, i.e. inspect all those edges (v,z) of the graph for which z has not yet been examined.  If z is already in the tree, a fundamental cycle is added; if not, the edge (v,z) is placed in the tree.  There is exactly one such stage for each of the n vertices of the graph.  For large n, the store required in creases as n^2 and the time as n^g where g depends on the type of graph involved. g is bounded below by 2 and above by 3, and it is shown that both bounds are attained.  In terms of storage our algorithm is similar to that of Gotlieb and Corneil and superior to that of Welch; in terms of speed it is similar to that of Welch and superior to that of Gotlieb and Corneil.  Testsshow our algorithm to be remarkably efficient (g=2) on random graphs."}
{"DOCID": "1848", "TEXT": "The Damped Taylor's Series Method for Minimizing a Sum of Squares and for Solving Systems of Nonlinear Equations (Algorithm 315 [E4, C5])"}
{"DOCID": "1849", "TEXT": "Function Minimization (Algorithm 251 [E4])"}
{"DOCID": "1850", "TEXT": "Generation of Permutations in Lexicographic Order (Algorithm 323 [G6])"}
{"DOCID": "1851", "TEXT": "Generator of Spanning Trees (Algorithms 354 [H])"}
{"DOCID": "1852", "TEXT": "A Base for a Mobile Programming System: An algorithm for a macro processor which has been used as the base of an implementation, by bootstrapping, of processors for programming languages is described.  This algorithm can be easily implemented on contemporary computing machines.  Experience with programming languages whose implementation is based on this algorithm indicates that such a language can be transferred to a new machine in less than one man-week without using the old machine."}
{"DOCID": "1853", "TEXT": "Compact List Representation: Definition, Garbage Collection, and System Implementation: Compact lists are stored sequentially in memory, rather than chained with pointers.  Since this is not always convenient, the Swym system permits a list to be chained, compact, or any combination of the two.  A description is given of that list representation and the operators implemented (most are similar to those of LISP 1.5).  The system garbage collector attempts to make all lists compact; it relocates and rearranges all of list storage using temporary storage. This unique list-compacting garbage collection algorithm is presented in detail.  Several classes of the macros used to implement the system are described. Finally, consideration is given to those design factors essential to the success of a plex processing system implementation."}
{"DOCID": "1854", "TEXT": "On Multiprogramming, Machine Coding, and Computer Organization: The author feels that the interrupt feature which is available in most modern computers is a potent source of programming pitfalls and errors, and that it therefore may heavily contribute to the unreliability of programs making use of it.  A programming scheme is presented which avoids the concept of the interrupt and permits the specification of concurrent (or pseudoconcurrent) activities in a supposedly more perspicuous manner.  It is intended to serve as a basis for the construction of operating systems, which are prime examples of programs with concurrent activities.  The scheme includes a set of basic instructions for the generation, termination, and synchronization of parallel processes.  A set of routines representing these instructions and thereby simulating a hypothetical machine organization has been implemented and test on the IBM System/360.  Two programs using these instructions, written in PL360, are presented."}
{"DOCID": "1855", "TEXT": "A Program for the Syntactic Analysis of English Sentences: A program is described which produces syntactic analyses of English sentences with respect to a transformational grammar.  The main features of the analyzer are that it uses only a limited dictionary of English words and that it pursues all analysis paths simultaneously while processing the sentence from left to right.  The form of representation used for the dictionary and the grammar is indicated and an outline account is given of the analysis procedure. Techniques for keeping the size of the analysis record within reasonable limits and for avoiding the need for dynamic application of certain transformational rules are described.   A number of examples of output produced by the program are given.  The output includes timing information."}
{"DOCID": "1856", "TEXT": "The Teachable Language Comprehender: A Simulation Program and Theory of Language: The Teachable Language Comprehender (TLC) is a program designed to be capable of being taught to \"comprehend\" English text.  When text which the program has not seen before is input to it, it comprehends that text by correctly relating each (explicit or implicit) assertion of the new text to a large memory. This memory is a \"semantic network\" representing factual assertions about the world.  The program also creates copies of the parts of its memory which have been found to relate to the new text, adapting and combining these copies to represent the meaning of the new text.  By this means, the meaning of all text the program successfully comprehends is encoded into the same format as that of the memory.  In this form it can be added into the memory.  Both factual assertions for the memory and the capabilities for correctly relating text to the memory's prior content are to be taught to the program as they are needed. TLC presently contains a relatively small number of examples of such assertions and capabilities, but within the system, notations for expressing either of these are provided.  Thus the program now corresponds to a general process for comprehending language, and it provides a methodology for adding the additional information this process requires to actually comprehend text of any particular kind.  The memory structure and comprehension process of TLC allow new factual assertions and capabilities for relating text to such stored assertions to generalize automatically.  That is, once such an assertion or capability is put into the system, it becomes available to help comprehend a great many other sentences in the future. Thus the addition of a single factual assertion or linguistic capability will often provide a large increment in TLC's effective knowledge of the world and in its overall ability to comprehend text.  The program's strategy is presented as a general theory of language comprehension."}
{"DOCID": "1857", "TEXT": "Filon Quadrature (Algorithm [D1])"}
{"DOCID": "1858", "TEXT": "An Algorithm for Filon Quadrature: An algorithm for Filon quadrature is described. Considerable attention has been devoted to an analysis of the round-off and truncation errors. The algorithm includes an automatic error control feature."}
{"DOCID": "1859", "TEXT": "Error Bounds for Periodic Quintic Splines: Explicit error bounds for periodic quintic spline interpolation are developed.  The first (third) derivative of the periodic spline is shown to be a sixth (fourth) order approximation at the mesh points to the first (third) derivative of the function being interpolated."}
{"DOCID": "1860", "TEXT": "An Algol-Based Associative Language: A high level programming language for large, complex associative structures has been designed and implemented.  The underlying data structure has been implemented using a hash-coding technique. The discussion includes a comparison with other work and examples of applications of the language."}
{"DOCID": "1861", "TEXT": "The MAD Definition Facility: One of the first definition facilities for higher level languages is described.  Users of the language can define new operators and/or data types into the MAD language, so that their use appears as if they were predefined.  Information is given on how one writes definitions, as well as on much of the motivation behind the form in which definitions are written. Some conclusions are drawn about future definitional facilities."}
{"DOCID": "1862", "TEXT": "Computing Capabilities at Argentine and Chilean Universities: The author reports on a trip to universities in Argentina and Chile during November 1968, describing university conditions and computing activities.  As elsewhere, these universities are experiencing student discontent with the status quo and the solutions they are attempting contrast: Argentina is excluding students from participating in university government; Chile is allowing such participation.  University computing service and academic activities are limited. The number of computers is small and so is the capacity, none larger than an IBM 360/40; with some exception, computing science academic programs are rare. This situation is by no means attributable to those responsible for computing developments, who strive for excellence; rather the \"system\" is hard to over-come.  Universities, especially those with strong European traditions, adapt slowly to new academic resources and disciplines; superimposed are the severe technological and economic constraints of the developing nation.  Consequently, in the absence of conscious government emphasis on strengthening computing capabilities, future progress may be retarded."}
{"DOCID": "1863", "TEXT": "Minit Algorithm for Linear Programming (Algorithm 333 [H])"}
{"DOCID": "1864", "TEXT": "Generation of Hilbert Derived Test Matrix (Algorithm 274 [F1])"}
{"DOCID": "1865", "TEXT": "Algol 60 Reference Language Editor (Algorithm 268 [R2])"}
{"DOCID": "1866", "TEXT": "Characteristic Values and Associated Solutions of Mathieu's Differential Equation (Algorithm 352 [S22])"}
{"DOCID": "1867", "TEXT": "On the Expected Lengths of Sequences Generated in Sorting by Replacement Selecting: In the replacement-selecting technique of sorting, one is interested in the ratio L(j) of the expected length of the j-th sequence generated by the technique to the number of memory cells used. Using complex-variable theory, it is shown that L(j) -> 2 and that, asymptotically, the average interval between sign changes of L(j)-2 is 2.6662."}
{"DOCID": "1868", "TEXT": "On Obtaining Correct Input:A New Approach: Most information put into machine readable form, whether from scientific or business origins, is still keypunched.  This paper is addressed toward the difficulty of obtaining correctly keypunched and key verified data and an alternative method is suggested in which the computer itself is used to rule out the possibility of errors in input.  This technique is explained and illustrated by reference to a working program which involves essentially two phases: in the first phase errors are detected by the machine, and subsequently, in the second phase, they are corrected by it."}
{"DOCID": "1869", "TEXT": "Block Structures, Indirect Addressing, and Garbage Collection: Programming languages have included explicit or implicit block structures to provide a naming convenience for the programmer.  However, when indirect addressing is used, as in SNOBOL, naming constraints may be introduced.  Two modifications to SNOBOL are described, resulting in two desirable consequences: (1) naming constraints disappear even when there is indirect addressing within function definitions; and (2) there is a significant saving in the number of calls to the garbage collector, because some garbage is collected, at little expense, each time a function returns to its calling program.  These modifications have been implemented as an extension to a SNOBOL dialect."}
{"DOCID": "1870", "TEXT": "Some Techniques for Using Pseudorandom Numbers in Computer Simulation: An algorithm is described by which uniform pseudorandom integers may be used to construct binary \"numbers\" in which the probability that each bit in the word is a 1-bit and can assume any desired parameter value.  Techniques for making use of such \"numbers\" in simulation programming are described."}
{"DOCID": "1871", "TEXT": "Automatic Contour Map: Some methods for contour mapping by means of a digital plotter are discussed, and a new method is presented that is simple enough to be implemented by programs with a rather small number of instructions (about 120 FORTRAN IV instructions are required).  Comparisons with some methods proposed by other authors are also performed,  A FORTRAN IV program implementing the proposed method is available at the Istituto di Elettrotecnica ed Elettronica, Politencnico di Milano."}
{"DOCID": "1872", "TEXT": "Chebyshev Interpolation and Quadrature Formulas of Very High Degree (Errata)"}
{"DOCID": "1873", "TEXT": "Accelerating LP Algorithms: It is shown how a novel method for computing (related) inner products can accelerate the pricing phase of LP algorithms.  Other LP applications are indicated."}
{"DOCID": "1874", "TEXT": "Generating Pseudorandom Numbers on a Two's Complement Machine such as the IBM 360: The familiar multiplicative congruential generator is examined in the context of the type of two's complement arithmetic used in the IBM 360 series. Different sequences of residues are considered and relationships established among them.  It is shown that a sequence of positive and negative residues may be produced more simply and economically than with the conventional approach and yet have twice the period of the latter without loss of desirable statistical properties.  Another easily generated sequence involving absolute values is also shown to have twice the period but with less attractive statistical properties.  The statistical properties of these sequences are given and related to previously established criteria."}
{"DOCID": "1875", "TEXT": "Polynomial and Spline Approximation by Quadratic Programming: The problem of approximation to a given function, or of fitting a given set of data, where the approximating function is required to have certain of its derivations of specified sign over the whole range of approximation, is studied.  Two approaches are presented, in each of which quadratic programming is used to provide both the constraints on the derivatives and the selection of the function which yields the best fit.  The first is a modified Bernstein polynomial scheme, and the second is a spline fit."}
{"DOCID": "1876", "TEXT": "Generation of Test Matrices Having Certain Sign Patterns and Prescribed Positive Spectra: A class of orthogonal transformations is presented whose members transform a given positive diagonal matrix into a matrix having one of four special sign patterns."}
{"DOCID": "1877", "TEXT": "Prevention of System Deadlocks: A well-known problem in the design of operating systems is the selection of a resource allocation policy that will prevent deadlock.  Deadlock is the situation in which resources have been allocated to various tasks in such a way that none of the tasks can continue.  The various published solutions have been somewhat restrictive: either they do not handle the problem in sufficient generality or they suggest policies which will on occasion refuse a request which could have been safely granted.  Algorithms are presented which examine a request in the light of the current allocation of resources and determine whether or not the granting of the request will introduce the possibility of a deadlock.  Proofs given in the appendixes show that the conditions imposed by the algorithms are both necessary and sufficient to prevent deadlock.  The algorithms have been successfully used in the THE system."}
{"DOCID": "1878", "TEXT": "Recovery of Reentrant List Structures in SLIP: One consequence of the reference-count-based space-recovery system employed by SLIP is that reentrant list structures are not recovered even when explicitly erased.  LISP-like garbage-collection schemes are free of this impediment.  They however, depend on being able to find and mark nodes that are reachable from program variables.  By tracing all descendants from program variables may then be identified and collected.  The list-creating function LIST of SLIP may be amended to mark those lists for which the programmer wishes to assume responsibility. Given this modification, a LISP-like garbage collector that recovers abandoned reentrant list structures may then be appended to the SLIP system."}
{"DOCID": "1879", "TEXT": "A Note on Storage Fragmentation and Program Segmentation: The main purpose of this paper is the presentation of some of the results of a series of simulation experiments investigating the phenomenon of storage fragmentation. Two different types of storage fragmentation are distinguished: (1) external fragmentation, namely the loss in storage utilization caused by the inability to make use of all available storage after it has been fragmented into a large number of separate blocks; and (2) internal fragmentation, the loss of utilization caused by rounding up a request for storage, rather than allocating only the exact number of words required. The most striking result is the apparently general rule that rounding up requests for storage, to reduce the number of different sizes of blocks coexisting in storage, causes more loss of storage by increased internal fragmentation than is saved by decreased external fragmentation.  Described also are a method of segment allocation and an accompanying technique for segment addressing which take advantage of the above result.  Evidence is presented of possible advantages of the method over conventional paging techniques."}
{"DOCID": "1880", "TEXT": "Chebyshev Solution to an Overdetermined Linear System (Algorithm 328 [F4])"}
{"DOCID": "1881", "TEXT": "Transpose Vector Stored Array (Algorithm 302 [K2])"}
{"DOCID": "1882", "TEXT": "Determination of the Square Root of a Positive Definite Matrix (Algorithm 298 [F1])"}
{"DOCID": "1883", "TEXT": "Modified Romberg Quadrature(Algorithm [D1])"}
{"DOCID": "1884", "TEXT": "An Anomaly in Space-Time Characteristics of Certain Programs Running in a Paging Machine: The running time of programs in a paging machine generally increases as the store in which programs are constrained to run decreases.  Experiments, however, have revealed cases in which the reverse is true: a decrease in the size of the store is accompanied by a decrease in running time.  An informal discussion of the anomalous behavior is given, and for the case of the FIFO replacement algorithm a formal treatment is presented."}
{"DOCID": "1885", "TEXT": "A Computer System for Transformational Grammar: A comprehensive system for transformational grammar has been designed and implemented on the IBM 360/67 computer.  The system deals with the transformational model of syntax, along the lines of Chomsky's Aspects of the Theory of Syntax. The major innovations include a full,formal description of the syntax of a transformational grammar, a directed random phrase structure generator, a lexical insertion algorithm, an extended definition of analysis, and a simple problem-oriented programming language in which the algorithm for application of transformations can be expressed.  In this paper we present the system as a whole, first discussing the general attitudes underlying the development of the system, then outlining the system and discussing its more important special features.  References are given to papers which consider some particular aspect of the system in detail."}
{"DOCID": "1886", "TEXT": "Generation of Optimal Code for Expressions via Factorization: Given a set of expressions which are to be compiled, methods are presented for increasing the efficiency of the object code produced by first factoring the expressions, i.e. finding a set of subexpressions each of which occurs in two or more other expressions or subexpressions.  Once all the factors have been ascertained, a sequencing procedure is applied which orders the factors and expressions such that all information is computed in the correct sequence and factors need be retained in memory a minimal amount of time.  An assignment algorithm is then executed in order to minimize the total number of temporary storage cells required to hold the results of evaluating the factors.  In order to make these techniques computationally feasible, heuristic procedures are applied, and hence global optimal results are not necessarily generated.  The factorization algorithms are also applicable to the problem of factoring Boolean switching expressions and of factoring polynomials encountered in symbol manipulating systems."}
{"DOCID": "1887", "TEXT": "A Recursive Relation for the Determinant of a Pentadiagonal Matrix: A recursive relation, relating leading principal minors, is developed for the determinant of a pentadiagonal matrix.  A numerical example is included to indicate its use in calculating eigenvalues."}
{"DOCID": "1888", "TEXT": "Spline Function Methods for Nonlinear Boundary-Value Problems: The solution of the nonlinear differential equation Y\"=F(x,Y,Y') with two-point boundary conditions is approximated by a quintic or cubic spline function y(x).  The method is well suited to nonuniform mesh size and dynamic mesh size allocation.  For uniform mesh size h, the error in the quintic spline y(x) is O(h^4), with typical error one-third that from Numerov's method.  Requiring the differential equation to be satisfied at the mesh points results in a set of difference equations, which are block tridiagonal and so are easily solved by relaxation or other standard methods."}
{"DOCID": "1889", "TEXT": "Introducing Computing to Smaller Colleges and Universities -- A Progress Report: By technical means that are now routine, computer service for smaller colleges and universities can be provided by remote terminals of a central facility. Access, however, is not enough-effective organizational and educational methodology for introducing computing at such institutions must also be developed.  The experience of two years with a statewide network involving-41 institutions is discussed. Lessons include the importance of a separate organization representing the small colleges, the necessity for on-campus training for the institutions, the need for some special programming and documentation to support such users,and the development of curriculum by evolutionary means."}
{"DOCID": "1890", "TEXT": "Simulation of Traffic Flows in a Network: A computer simulation program which deals with traffic flows in the network of a large area is described.  Each road is segmented into blocks of several ten-meter lengths and is represented by a bidirectional list in computer memory.  The movement of cars, i.e. the transfer of cars from one block to the next, is expressed by a proper formula.  This formula is based on the supposition that the speed of cars in a block is determined only by the density of cars in the block, and this speed-versus-density curve is empirically given the numerical values.  This simulation scheme has its excellent point in that it makes it possible to trace the dynamic behavior of traffic flows in a variety of situations, some examples of which are given for an actual area of the city of Kyoto, Japan."}
{"DOCID": "1891", "TEXT": "Three-Dimensional Computer Display: A stereographic display terminal has been produced using the raster display (BRAD) recently developed at Brookhaven.  The system uses a rotating refresh memory to feed standard television monitors. To produce a stereographic display the computer calculates the projected video images of an object, viewed from two separate points.  The resulting video maps are stored on separate refresh bands of the rotating memory.  The two output signals are connected to separate color guns of a color television monitor, thus creating a superimposed image on the screen.  Optical separation is achieved by viewing the image through color filters.  The display is interactive and can be viewed by a large group of people at the same time."}
{"DOCID": "1892", "TEXT": "Degree of Multiprogramming in Page-on-Demand Systems: A simple stochastic model is described which offers a base for understanding the relationship between the number of programs permitted to share memory (the degree of multiprogramming), drum traffic rates, and central processing unit utilization in page-on-demand, multiprogrammed, time-shared computer systems.  The model preserves, as a key feature, the property of page-demand statistics which implies a \"burst\" of page demands at the beginning of any job or quantum execution.  The model, a Markov chain, is analyzed numerically and the results are presented graphically for a wide range of key environment-descriptive parameters.  Implications of the results to time-shared system design and programming are discussed, and a calculation of the optimal degree of multiprogramming for a wide range of parameters is presented graphically."}
{"DOCID": "1893", "TEXT": "Roots of Polynomials by a Root-Squaring and Resultant routine (Algorithm 340 [C2])"}
{"DOCID": "1894", "TEXT": "Normal Random Deviates (Algorithm 334 [G5])"}
{"DOCID": "1895", "TEXT": "Gaussian Quadrature Formulas (Algorithm 331 [D1])"}
{"DOCID": "1896", "TEXT": "Regular Coulomb Wave Functions (Algorithm 292 S22])"}
{"DOCID": "1897", "TEXT": "Coulomb Wave Functions (Algorithm 300 [S22])"}
{"DOCID": "1898", "TEXT": "Regular Coulomb Wave Functions (Algorithm 292 [S22])"}
{"DOCID": "1899", "TEXT": "Simplex Method Procedure Employing Lu Decomposition (Algorithm 350 [H])"}
{"DOCID": "1900", "TEXT": "Clarification of Fortran Standards-Initial Progress: In 1966 after four years of effort, FORTRAN became the first programming language standardized in the United States.  Since that initial achievement, study and application of the standard specifications have revealed the need for maintenance of the standards. As the result of work initiated in 1967, an initial set of clarifying interpretations has been prepared. The nature of the maintenance, corrections to the standard specifications, and completed interpretations are reported."}
{"DOCID": "1901", "TEXT": "Dynamic Space-Sharing in Computer Systems: A formalization of relationships between space-shading program behavior, and processor efficiency in computer systems is presented.  Concepts of value and cost of space allocation per task are defined and then value and cost are combined to develop a single parameter termed value per unit cost.  The intent is to illustrate a possible analytic approach to the investigation of the problems of space-sharing and to demonstrate the method on sample problems."}
{"DOCID": "1902", "TEXT": "An Automatic Grading Scheme for Simple Programming Exercises: A discussion is given of alterations that were made to a typical university operating system to record the results of programming exercises in three different languages, including assembly language. In this computer-controlled grading scheme provision is made for testing with programmer-supplied data and for final runs with system-supplied data.  Exercises run under the scheme may be mixed with other programs, and no special recognition of exercises by the operators is necessary."}
{"DOCID": "1903", "TEXT": "Chebyshev Interpolation and Quadrature Formulas of Very High Degree"}
{"DOCID": "1904", "TEXT": "Rough and Ready Error Estimates in Gaussian Integration of Analytic Functions"}
{"DOCID": "1905", "TEXT": "The Simplex Method of Linear Programming Using LU Decomposition: Standard computer implementations of Dantzig's simplex method for linear programming are based upon forming the inverse of the basic matrix and updating the inverse after every step of the method. These implementations have bad round-off error properties. This paper gives the theoretical background for an implementation which is based upon the LU decomposition, computed with row interchanges, of the basic matrix.  The implementation is slow, but has good round-off error behavior.  The implementation appears as CACM Algorithm 350."}
{"DOCID": "1906", "TEXT": "Automated Printed Circuit Routing with a Stepping Aperture: A computer program for routing interconnections on a two-sided printed circuit board with a regular pattern of lines, pins (terminals), and vias (feed-through holes) is described.  In this program, each interconnection is given a planned routing-typically, down from the upper pin, through a via, and horizontally to the lower pin.  From the top, a virtual aperture (i.e. a long horizontal slit) is stepped down the board.  The planned routing is the basis for rerouting interconnections within the aperture to resolve conflicts for lines and vias below the aperture and to maximize the effective line usage. If a conflict has not been resolved before the aperture arrives at the lower pin,interconnections are deleted to resolve the conflict.  Extensions of this technique to the control of crosstalk between routed interconnections and to the problem of obtaining 100 percent interconnect are also discussed."}
{"DOCID": "1907", "TEXT": "A Note on Reliable Full-Duplex Transmission over Half-Duplex Links: A simple procedure for achieving reliable full-duplex transmission over half-duplex links is proposed. The scheme is compared with another of the same type, which has recently been described in the literature.  Finally, some comments are made on another group of related transmission procedures which have been shown to be unreliable under some circumstances."}
{"DOCID": "1908", "TEXT": "Time-Sharing and Batch-Processing:  An Experimental Comparison of Their Values in a Problem - Solving Situation: An experimental comparison of problem-solving using time-sharing and batch-processing computer systems conducted at MIT is described in this paper. This study is the first known attempt to evaluate two such systems for what may well be the predominant user population within the next decade-the professionals who, as nonprogrammers, are using the computer as an aid in decision-making and problem-solving rather than as a programming end in itself.  Statistically and logically significant results indicate equal cost for usage of the two computer systems; however, a much higher level of performance is attained by time-sharing users.  There are indications that significantly lower costs would have resulted if the time-sharing users had stopped work when they reached a performance level equal to that of the batch users.  The users' speed of problem-solving and their attitudes made time-sharing the more favorable system."}
{"DOCID": "1909", "TEXT": "Computation of Jn(x) by Numerical Integration: It is shown to be practical to compute Jn(x) by numerical integration of its integral representation using the trapezoidal rule. The error in this approximation was studied empirically."}
{"DOCID": "1910", "TEXT": "An Algorithm for Solving a Special Class of Tridiagonal Systems of Linear Equations: An algorithm is presented for solving a system of linear equation Bu=k where B is tridiagonal and of a special form.  It is shown that this algorithm is almost twice as fast as the Gaussian elimination method usually suggested for solving such systems. In addition, explicit formulas for the inverse and determinant of the matrix B are given."}
{"DOCID": "1911", "TEXT": "On Coordination Reduction and Sentence Analysis: A class of coordination phenomena in natural languages is considered within the frame work of transformational theory.  To account for these phenomena it is proposed that certain machinery be added to the syntactic component of a transformational grammar. This machinery includes certain rule schemata, the conditions under which they are to be applied, and conditions determining the sequence of subtrees on which they are to be performed.  A solution to the syntactic analysis problem for this class of grammars is outlined.  Precise specification of both the generative procedure of this paper and its inverse is given in the form of LISP function definitions."}
{"DOCID": "1912", "TEXT": "Simulation of Outpatient Appointment Systems: An experimental computer program is described which simulates appointment systems employed by outpatient departments of hospitals.  Both major kinds of appointment systems-individual and block-can be simulated.  The purpose of the Simulator is to enable the user to evaluate the effectiveness of alternative appointment systems in a given clinical environment."}
{"DOCID": "1913", "TEXT": "Polygamma Functions with Arbitrary Precision (Algorithm 349 [S14])"}
{"DOCID": "1914", "TEXT": "Matrix Scaling by Integer Programming (Algorithm 348 [F1])"}
{"DOCID": "1915", "TEXT": "An Algorithm for Hidden Line Elimination: The algorithm presented causes the elimination of hidden lines in the representation of a perspective view of concave and convex plane-faced objects on the picture plane.  All the edges of the objects are considered sequentially, and all planes which hide every point of an edge are found.  The computing time increases roughly as the square of the number of edges. The algorithm takes advantage of a reduced number of concave points and automatically recognizes if only one object with no concave points is considered. In this last case, the result is obtained in a much simpler way."}
{"DOCID": "1916", "TEXT": "Analysis of Boolean Program Models for Time-Shared, Paged Environments: Directed graphs or their associated matrices are frequently used to represent the logical structure of sequences of computer instructions.  Such techniques are used and, in addition, data references are represented in a nondirected model. The complete structural specification of a program is represented by a combined model.  A transformation of the combined model yields a new model in which additional timing information is also contained.  Analysis of these models prior to execution yields information valuable in determining segmentation of instructions and data for a time-shared environment, as well as for initial page loading; during execution, the analysis may be used for \"look ahead\" control of page turning."}
{"DOCID": "1917", "TEXT": "An Algol Procedure for the Fast Fourier Transform with Arbitrary Factors (Algorithm 339 [C6])"}
{"DOCID": "1918", "TEXT": "Distribution of Indistinguishable Objects into Distinguishable Slots (Algorithm 329 [G6])"}
{"DOCID": "1919", "TEXT": "An Efficient Algorithm for Sorting with Minimal Storage (Algorithm 347 [M1])"}
{"DOCID": "1920", "TEXT": "F-Test Probabilities (Algorithm 346 [S14])"}
{"DOCID": "1921", "TEXT": "An Algol Convolution Procedure Based on the Fast Fourier Transform (Algorithm 345 [C6])"}
{"DOCID": "1922", "TEXT": "Proposed USA Standard (Data Communication Control Procedures for the USA Standarad Code for Information Interchange)"}
{"DOCID": "1923", "TEXT": "Pseudofiles: An approach to system interfaces for high level languages using basic input/output support facilities is described.  It is shown that this technique can provide potentially inexpensive methods for programs to communicate with deeply embedded facilities such as command language processors."}
{"DOCID": "1924", "TEXT": "Organizing Matrices and Matrix Operations for Paged Memory Systems: Matrix representations and operations are examined for the purpose of minimizing the page faulting occurring in a paged memory system.  It is shown that carefully designed matrix algorithms can lead to enormous savings in the number of page faults occurring when only a small part of the total matrix can be in main memory at one time.  Examination of addition, multiplication, and inversion algorithms shows that a partitioned matrix representation (i.e. one submatrix or partition per page) in most cases induced fewer page faults than a row-by-row representation. The number of page-pulls required by these matrix manipulation algorithms is also studied as a function of the number of pages of main memory available to the algorithm."}
{"DOCID": "1925", "TEXT": "Concepts of Use in Contour Map Processing: Generalized techniques whose use can simplify the solution of problems relating to contour maps.  One of these techniques makes use of the topological properties of contour maps.  The topology is represented by a graphical structure in which adjacent contour lines appear as connected nodes.  Another generalized technique consists of utilizing geometrical properties to determine the characteristics of straight lines drawn on the contour map.  Both of these techniques have been applied to the problem of locating the ground track of an aircraft from elevation readings obtained during a flight."}
{"DOCID": "1926", "TEXT": "Description of FORMAT, a Text-Processing Program: FORMAT is a production program which facilitates the editing and printing of \"finished\" documents directly on the printer of a relatively small (64k) computer system.  It features good performance, totally free-form input, very flexible formatting capabilities including up to eight columns per page, automatic capitalization, aids for index construction, and a minimum of nontext items.  It is written entirely in FORTRAN IV."}
{"DOCID": "1927", "TEXT": "Information Science in a Ph.D. Computer Science Program: This report contains recommendations on a sample course curriculum in the general area of information organization and information system design in a Ph.D. Computer Science Program.  The subject area is first briefly described, followed by a listing of some desirable graduate-level courses.  Suitable bibliographies are appended."}
{"DOCID": "1928", "TEXT": "Exclusive Simulation of Activity in Digital Networks: A technique for simulating the detailed logic networks of large and active digital systems is described.  Essential objectives sought are improved ease and economy in model generation, economy in execution time and space, and a facility for handling simultaneous activities.  The main results obtained are a clear and useful separation of structural and behavioral model description, a reduction of manual tasks in converting Boolean logic into a structural model, the elimination of manual processes in achieving exclusive simulation of activity, an event-scheduling technique which does not deteriorate in economy as the event queue grows in length, and a simulation procedure which deals effectively with any mixture of serial and simultaneous activities.  The passage of time is simulated in a precise, quantitative fashion and systems to be simulated may be combinations of synchronous and asynchronous logic.  Certain aspects of the techniques described may be used for the simulation of network structures other than digital networks."}
{"DOCID": "1929", "TEXT": "Images from Computers and Microfilm Plotters: Digital computers are widely used for the processing of information and data of all kinds, including the pictorial information contained in photographs and other graphical representations.  Efficient conversion facilities for putting graphical information into the computer and retrieving it in graphical form are therefore much needed.  One of the most commonly employed devices for obtaining permanent graphical output from digital computers is the microfilm plotter. Regrettably, present models have no provision for producing images with a continuous gray scale or \"half tones.\" In this note several programming techniques are described for obtaining half tone pictures from a microfilm plotter under the control of a digital computer.  Illustrative examples of several methods are given."}
{"DOCID": "1930", "TEXT": "Extremely Portable Random Number Generator: Extremely portable subroutines are sometimes needed for which moderate quality and efficiency suffice.  Typically, this occurs for library functions (like random number generation and in core sorting) which are not entirely universal or are not used in a standardized way.  The literature on random number generators does not seem to contain an algorithm that meets requirements of this sort.  An extremely portable 8-line FORTRAN program is provided which based on an important paper by Coveyou and MacPherson (1967).Using their methods, Fourier analysis is applied to the probability function for the consecutive n-tuples provided by our generator (with n less than or equal to 4).  While the small modulus which must be used to maintain portability prevents the quality of the generator from being high, the generator compares well with the bounds established in the above mentioned paper."}
{"DOCID": "1931", "TEXT": "Interval Arithmetic Determinant Evaluation and Its Use in Testing for a Chebyshev System: Two recent papers, one by Hansen and one by Hansen and R. R. Smith, have shown how Interval Arithmetic (I.A.) can be used effectively to bound errors in matrix computations.  In the present paper a method proposed by Hasen and R. R. Smith is compared with straightforward use of I.A. in determinant evaluation.  Computational results show the accuracy and running times that can be expected when using I.A. for determinant evaluation.  An application using I.A. determinants in a program to test a set of functions to see if they form a Chebyshev system is then presented."}
{"DOCID": "1932", "TEXT": "The Logarithmic Error and Newton's Method for the Square Root: The problem of obtaining optimal starting values for the calculation of the square root using Newton's method is considered.  It has been pointed out elsewhere that if relative error is used as the measure of goodness of fit, optimal results are not obtained when the initial approximation is a best fit.  It is shown here that if, instead, the so-called logarithmic error is used, then a best initial fit is optimal for both types of error.  Moreover, use of the logarithmic error appears to simplify the problem of determining the optimal initial approximation."}
{"DOCID": "1933", "TEXT": "Coding the Lehmer Pseudo-random Number Generator: An algorithm and coding technique is presented for quick evaluation of the Lehmer pseudo-random number generator modulo 2**31 - 1, a prime Mersenne number with produces 2**31 - 2 numbers, on a p-bit (greater than 31) computer.  The computation method is extendible to limited problems in modular arithmetic. Prime factorization for 2**61 - 2 and a primitive root for 2**61 - 1, the next largest prime Mersenne number, are given for possible construction of a pseudo-random number generator of increased cycle length."}
{"DOCID": "1934", "TEXT": "On Arithmetic Expressions and Trees: A description is given of how a tree representing the evaluation of an arithmetic expression can be drawn in such a way that the number of accumulators needed for the computation can be represented in a straightforward manner.  This representation reduces the choice of the best order of computation to a specific problem under the theory of graphs. An algorithm to solve this problem is presented."}
{"DOCID": "1935", "TEXT": "Randomized Binary Search Technique: A mathematical model is developed for the mean and variance of the number of trials to recover a given document in a randomly received list of files. The search method described is binary in nature and offers new potential for information retrieval systems."}
{"DOCID": "1936", "TEXT": "Variable Length Tree Structures Having Minimum Average Search Time: Sussenguth suggests in a paper (1963) that a file should be organized as a doubly-chained tree structure if it is necessary both to search and to update frequently.  Such a structure provides a compromise between the fast search/slow update characteristics of binary searching and the slow search/fast update characteristics of serial searching.  His method, however, contains the limiting restriction that all terminal nodes lie on the same level of the tree.  This paper considers the effect of relaxing this restriction. First, trees which have the property that a priori the filial set of each node is well defined are studied. It is proved that coding the nodes within each filial set with respect to the number of terminal nodes reachable from each is necessary and sufficient to guarantee minimum average search time.  Then the more general case (that is, where the entire structure of the tree is changeable) is treated.  A procedure is developed for constructing a tree with a minimum average search time.  A simple closed expression for this minimum average search time is obtained as a function of the number of terminal nodes.  The storage capacity required to implement the doubly-chained tree structure on a digital computer is also determined.  Finally, the total cost of the structure, using Sussenguth's cost criterion, is computed. It is shown that significant improvements in both the average search time and the total cost can be obtained by relaxing Sussenguth's restriction that all terminal nodes lie on the same level of the tree."}
{"DOCID": "1937", "TEXT": "CODAS: A Data Display System: CODAS, a Customer Oriented Data System, is a user-oriented data retrieval and display system. The command language of the system provides the user with an easy means for specifying data retrieval and display requests.  Data is displayed as tables and graphs produced in a format ready for publication. In this paper the statements of the request language and the general system design are described."}
{"DOCID": "1938", "TEXT": "Some Criteria for Time-Sharing System Performance: Time-sharing systems, as defined in this article, are those multiaccess systems which permit a terminal user to utilize essentially the full resources of the system while sharing its time with other terminal users.  It is each terminal user's ability to utilize the full resources of the system that makes quantitative evaluation of time-sharing systems particularly difficult.  Six criteria are described which have been successfully used to perform first-level quantitative time-sharing system performance evaluation."}
{"DOCID": "1939", "TEXT": "Directed Random Generation of Sentences: The problem of producing sentences of a transformational grammar by using a random generator to create phrase structure trees for input to the lexical insertion and transformational phases is discussed. A purely random generator will produce base trees which will be blocked by the transformations, and which are frequently too long to be of practical interest. A solution is offered in the form of a computer program which allows the user to constrain and direct the generation by the simple but powerful device of restricted subtrees.  The program is a directed random generator which accepts as input a subtree with restrictions and produces around it a tree which satisfies the restrictions and is ready for the next phase of the grammar.  The underlying linguistic model is that at Noam Chomsky, as presented in Aspects of the Theory of Syntax.  The program is written in FORTRAN IV for the IBM 360/67 and is part of a unified computer system for transformational grammar. It is currently being used with several partial grammars of English."}
{"DOCID": "1940", "TEXT": "Calculation of a Polynomial and its Derivative Values by Horner Scheme (Algorithm 337 [C1])"}
{"DOCID": "1941", "TEXT": "F-Distribution (Algorithm 322 [S14])"}
{"DOCID": "1942", "TEXT": "Finding a Solution of N Functional Equations in N Unknowns (Algorithm 314 [C5])"}
{"DOCID": "1943", "TEXT": "Complete Elliptic Integrals (Algorithm 165 [S21])"}
{"DOCID": "1944", "TEXT": "Student's t-Distribution (Algorithm 344 [S14])"}
{"DOCID": "1945", "TEXT": "The Role of Programming in a Ph.D. Computer Science Program: In this general paper the role of programming in advanced graduate training is discussed. Subject matter related to programming as well as programming per se is considered.  The  importance and application of formalism are considered and also the need for good empirical experimentation.  A brief outline for a sequence of courses is included, and subject headings that have been obtained from an extensive bibliography are given.  A bibliography of programming references is included."}
{"DOCID": "1946", "TEXT": "Computing Polynomial Resultants: Bezout's Determinant vs. Collins' Reduced P.R.S. Algorithm: Algorithms for computing the resultant of two polynomials in several variables, a key repetitive step of computation in solving systems of polynomial equations by elimination, are studied.  Determining the best algorithm for computer implementation depends upon the extent to which extraneous factors are introduced, the extent of propagation of errors caused by truncation of real coefficients, memory requirements, and computing speed.  Preliminary considerations narrow the choice of the best algorithm to Bezout's determinant and Collins' reduced polynomial remainder sequence (p.r.s.) algorithm.  Detailed tests performed on sample problems conclusively show that Bezout's determinant is superior in all respects except for univariate polynomials, in which case Collins' reduced p.r.s. algorithm is somewhat faster.  In particular Bezout's determinant proves to be strikingly superior in numerical accuracy, displaying excellent stability with regard to round-off errors. Results of tests are reported in detail."}
{"DOCID": "1947", "TEXT": "Object code Optimization: Methods of analyzing the control flow and data flow of programs during compilation are applied to transforming the program to improve object time efficiency. Dominance relationships, indicating which statements are necessarily executed before others, are used to do global common expression elimination and loop identification.  Implementation of these and other optimizations in OS/360 FORTRAN H are described."}
{"DOCID": "1948", "TEXT": "Computers in Group Theory: a Survey: Computers are being applied to an increasingly diverse range of problems in group theory. The most important areas of application at present are coset enumeration, subgroup lattices, automorphism groups of finite groups, character tables, and commutator calculus.  Group theory programs range from simple combinatorial or numerical programs to large symbol manipulation systems.  In this survey the more important algorithms in use are described and contrasted, and results which have been obtained using existing programs are indicated.  An extensive bibliography is included."}
{"DOCID": "1949", "TEXT": "Finiteness Assumptions and Intellectual Isolation of Computer Scientists"}
{"DOCID": "1950", "TEXT": "Efficient Handling of Binary Data"}
{"DOCID": "1951", "TEXT": "Estimates of Distributions of Random Variables for Certain Computer Communications Traffic Models: A study of multiaccess computer communications has characterized the distributions underlying an elementary model of the user-computer interactive process.  The model used is elementary in the sense that many of the random variables that generally are of interest in computer communications studies can be decomposed into the elements of this model.  Data were examined from four operational multiaccess systems, and the model is shown to be robust; that is each of the variables of the model has the same distribution independent of which of the four systems is being examined. It is shown that the gamma distribution can be used to describe the discrete variables.  Approximations to the gamma distribution by the exponential distribution are discussed for the systems studied."}
{"DOCID": "1952", "TEXT": "Index by Subject to Algorithms, 1970"}
{"DOCID": "1953", "TEXT": "Exponential Integral Ei(x) (Algorithms 385 $S13))"}
{"DOCID": "1954", "TEXT": "Eigenvalues and Eigenvectors of a Real Symmetric Matrix (Algorithm 384 $F2))"}
{"DOCID": "1955", "TEXT": "Characteristic Values and Associated Solutions of Mathieu's Differential Equation (Algorithm 352 $S22))"}
{"DOCID": "1956", "TEXT": "Optimum Merging from Mass Storage: An algorithm is displayed which yields the merge orders such that the total read time, defined to be the sum of seek time plus data-transfer time, is minimized for a sort using mass storage. The analysis is parameterized in terms of the ratio of seek time to the time it takes to fill available core with records, and the file size in units of core lengths; and thus it can be applied to any conventional CPU/mass storage combination.  An explicit formula for total read time is derived, in terms of the parameters, which correlates very well with the total read time calculated using the optimum merge orders yielded by the algorithm.  The formula involves the roots of a simple transcendental equation.  A short table of these roots is included.  Numerical results are graphically displayed for a wide range of the parameters.  It is found that the normalized read time for optimum merging on a given hardware configuration is proportional to the file length times the logarithm of the file length."}
{"DOCID": "1957", "TEXT": "The List Set Generator: A Construct for Evaluating Set Expressions: The list set generator is defined and algorithms for its use are given.  The list set generator is a construct which may be added to a list processing system or any system that handles sets.  It efficiently generates the set which results from any expression involving sets and set operators.  The efficiency derives from evaluating the expression as a whole and in parallel, rather than evaluating subexpressions and then using those sets to arrive at the final result."}
{"DOCID": "1958", "TEXT": "Improving Round-off in Runge-Kutta Computations with Gill's Method: A Runge-Kutta-Gill scheme in common use is based on an incomplete adaptation for floating point operations of Gill's method.  An improved version reduces round-off error significantly.  In this note the heart of the scheme is presented in Fortran language.  It is then shown how an improved version of the method can be obtained with the addition of two Fortran statements.  The two version is a significant improvement.  A numerical example comparing the two is included."}
{"DOCID": "1959", "TEXT": "An Interrupt Based Organization for Management Information Systems: A programming structure, language constructs, and a supervisory system organization are proposed for the design and coding of large shared data base systems.  The bases for this organization are a generalized interrupt structure and the newly introduced concept of \"file tagging,\" which is the process of associating program structures and interrupt generating conditions with items in the data base.  An algorithm for resolving conflicts which arise in scheduling the interrupt processing routines is presented.  DPL, a programming language and supervisory system in which these concepts are implemented, is used to illustrated the new organization which is proposed for management information systems."}
{"DOCID": "1960", "TEXT": "Process Management and Resource Sharing in the Multiaccess System ESOPE: The main design principles of the multiaccess system ESOPE are described. Emphasis is placed on basic ideas underlying the design rather than on implementation details.  The main features of the system include the ability given to any user to schedule his own parallel processes using system primitive operations, the file-memory relationship, and the allocation-scheduling policy, which dynamically takes into account recent information about user behavior."}
{"DOCID": "1961", "TEXT": "An Efficient Search Algorithm to Find the Elementary Circuits of a Graph: A theoretically most efficient search algorithm is presented which uses an exhaustive search to find all of the elementary circuits of a graph.  The algorithm can be easily modified to find all of the elementary circuits with a particular attribute such as length.  A rigorous proof of the algorithm is given as well as an example of its application.  Empirical bounds are presented relating the speed of the algorithm to the number of vertices and the number of arcs.  The speed is also related to the number of circuits in the graph to give a relation between speed and complexity. Extensions to undirected and s-graphs are discussed."}
{"DOCID": "1962", "TEXT": "GROOVE-A Program to Compose, Store, and Edit Functions of Time: A program which makes possible creating, storing, reproducing, and editing functions of time is described.  The functions are typical of those generated by human beings.  Multiple functions (up to 14) are produced for long periods of time (up to several hours) at sufficiently high sampling rates to describe fast human reactions (up to 200 samples per second).  The functions can be used for a variety of purposes such as the control of machine tools or sound synthesizers or anything a person normally controls.  The program operates on a small computer (DDP-224).  Functions are stored on a disk file.  Functions may be created by real-time human inputs to the computer which can interact with already stored functions and computed functions.  Real-time feedback from the process being controlled is an important link in the system.  The environment for effective man-machine interaction has been carefully nurtured."}
{"DOCID": "1963", "TEXT": "Condition Numbers of PEI Matrices"}
{"DOCID": "1964", "TEXT": "Comment on the Working Set Model for Program Behavior"}
{"DOCID": "1965", "TEXT": "Correction to \"Logical\" Arithmetic on Computers with Two's Complement Binary Arithmetic"}
{"DOCID": "1966", "TEXT": "A Generalized Method for Generating Argument/Function Values"}
{"DOCID": "1967", "TEXT": "An Improved Algorithm to Produce Complex Primes (Algorithm 401 $A1))"}
{"DOCID": "1968", "TEXT": "Eigenvalues and Eigenvectors of a Real General Matrix (Algorithm 343 $F1))"}
{"DOCID": "1969", "TEXT": "Increasing the Efficiency of Quicksort (Algorithm 402 $M1))"}
{"DOCID": "1970", "TEXT": "Unrecorded Magnetic Tape for Information Interchange (9 Track-200 and 800 CPI, NRZI and 1600 CPI, PE)* (Proposed American National Standard)"}
{"DOCID": "1971", "TEXT": "Recorded Magnetic Tape for Information Interchange (1600 CPI, Phase Encoded)* (Proposed American National Standard)"}
{"DOCID": "1972", "TEXT": "A  Nonrecursive List Compacting Algorithm: A simple nonrecursive list structure compacting scheme or garbage collector suitable for both compact and LISP-like list structures is presented. The algorithm avoids the need for recursion by using the partial structure as it is built up to keep track of those lists that have been copied."}
{"DOCID": "1973", "TEXT": "The Linear Quotient Hash Code: A new method of hash coding is presented and is shown to possess desirable attributes.  Specifically, the algorithm is simple, efficient, and exhaustive, while needing little time per probe and using few probes per lookup.  Performance data and implementation hints are also given."}
{"DOCID": "1974", "TEXT": "NEATER2: A PL/I Source Statement Reformatter: NEATER2 accepts a PL/I source program and operates on it to produce a reformatted version.  When in the LOGICAL mode, NEATER2 indicates the logical structure of the source program in the indentation pattern of its output.  Logic errors discovered through NEATER2 logical analysis are discovered much more economically than is possible through compilation and trial runs.  A number of options are available to give the user full control over the output format and to maximize the utility of NEATER2 as an aid during the early stages of development of a PL/I source deck.  One option, USAGE, causes NEATER2 to insert into each logical unit of coding a statement which will case the number of times each one is executed to be recorded during execution.  This feature is expected to provide a major aid in optimization of PL/I programs."}
{"DOCID": "1975", "TEXT": "A Multiple-Precision Division Algorithm: A generalized division algorithm for use with positive integral operands is presented.  Depending upon the algebraic relationship of the first two ciphers of the divisor, one or at most two adjustments to the original divisor and dividend must be performed before the division operation can be initiated. The uniqueness of this method will cause each trial cipher in the quotient to be either equal to or one greater than its final replacement."}
{"DOCID": "1976", "TEXT": "Multi-attribute Retrieval with Combined Indexes: In this paper a file organization scheme designed to replace the use of the popular secondary index filing scheme (or inverted files on secondary key fields) is described. Through the use of redundancy and storing keys (or access numbers of the records) that satisfy different combinations of secondary index values in \"buckets,\" it is possible to retrieve all keys satisfying any input query derived from a subset of fields by a single access to an index file, although each bucket may be used for many combinations of values and a combination of buckets may be required for a given query.  The method which, in its degenerate case, becomes the conventional secondary index filing scheme works similarly but has the following advantages: (1) the elimination of multiple accesses in many cases; (2) the elimination of false drops; (3) the elimination of computer time to perform intersection of key sets each qualified for one secondary index field only; and (4) the avoidance of long strings of keys when an index field appearing in a query has very few possible values.  Redundancy, in some cases, is the same as the secondary indexing method. In the general case, trade-off between the number of accesses for query and redundancy exists."}
{"DOCID": "1977", "TEXT": "An Interactive Display for Approximation by Linear Programming: An interactive program with a graphical display has been developed for the approximation of data by means of a linear combination of functions (including splines) selected by the user.  The coefficients of the approximation are determined by linear programming so as to minimize the error in either the L1 or L-infinity norm.  Auxiliary conditions such as monotonicity or convexity of the approximation can also be imposed. This interactive system is described and several examples of its use are given."}
{"DOCID": "1978", "TEXT": "The Use of Interactive Graphics To Solve Numerical Problems: With the advent of on-line (time-sharing) computer systems and graphic terminals, we have available a new dimension in numerical problem solving capabilities.  Rather than simply use the new power to achieve fast turnaround, we can develop interactive routines which are easy to use and also take advantage of the insight and visual capabilities of the human problem solver.  Several on-line systems for general purpose mathematical problem solving have already been implemented as well as some special purpose systems for solving problems in a particular area such as ordinary differential equations.  The advantage of restricting the problem area is that the interface with a user can be greatly simplified. In this paper we discuss some of the advantages accrued by such systems and design considerations for interactive routines.  Furthermore, an implementation of an on-line least squares data-fitting program, PEG, is presented with results obtained from empirical data.  In conclusion, area for future work in this field are discussed."}
{"DOCID": "1979", "TEXT": "Numerical Inversion of Laplace Transforms (Algorithm 368 $D5))"}
{"DOCID": "1980", "TEXT": "An Efficient Algorithm for Sorting with Minimal Storage (Algorithm 347 $M1))"}
{"DOCID": "1981", "TEXT": "Normal Curve Integral (Algorithm 304 $S15))"}
{"DOCID": "1982", "TEXT": "Modified Havie Integration (Algorithm 400 $D1))"}
{"DOCID": "1983", "TEXT": "Spanning Tree $H) (Algorithm 399)"}
{"DOCID": "1984", "TEXT": "Tableless Date Conversion $Z) (Algorithm 398)"}
{"DOCID": "1985", "TEXT": "An Integer Programming Problem $H) (Algorithm 397)"}
{"DOCID": "1986", "TEXT": "Student's t-Quantiles $S14) (Algorithm 396)"}
{"DOCID": "1987", "TEXT": "Student's t-Distribution $S14) (Algorithm 395)"}
{"DOCID": "1988", "TEXT": "A Formalism for Translator Interactions: A formalism is presented for describing the actions of processors for programming languages-compilers, interpreters, assemblers-and their interactions in complex systems such as compiler-compilers or extendible languages. The formalism here might be used to define and answer such a question as \"Can one do bootstrapping using a meta-compiler whose metaphase is interpretive?\"  In addition an algorithm is presented for deciding whether or not a given system can be produced from a given set of component processors."}
{"DOCID": "1989", "TEXT": "Transition Network Grammars for Natural Language Analysis: The use of augmented transition network grammars for the analysis of natural language sentences is described.  Structure-building actions associated with the arcs of the grammar network allow for the reordering, restructuring, and copying of constituents necessary to produce deep-structure representations of the type normally obtained from a transformational analysis, and conditions on the arcs allow for a powerful selectivity which can rule out meaningless analyses and take advantage of semantic information to guide the parsing.  The advantage of this model for natural language analysis are discussed in detail and illustrated by examples.  An implementation of an experimental parsing system for transition network grammars is briefly described."}
{"DOCID": "1990", "TEXT": "Numerical Constants (Algorithm)"}
{"DOCID": "1991", "TEXT": "On the Number of Automorphisms of a Singly Generated Automaton"}
{"DOCID": "1992", "TEXT": "Comment on Bell's Quadratic Quotient Method for Hash Code Searching"}
{"DOCID": "1993", "TEXT": "Regular Coulomb Wave Functions (Algorithm 292 $S22))"}
{"DOCID": "1994", "TEXT": "Decision Table Translation $H) (Algorithm 394)"}
{"DOCID": "1995", "TEXT": "Special Series Summation with Arbitrary Precision $C6) (Algorithm 393)"}
{"DOCID": "1996", "TEXT": "Systems of Hyperbolic PDE $D3) (Algorithm 392)"}
{"DOCID": "1997", "TEXT": "Increasing the Efficiency of Quicksort: A method is presented for the analysis of various generalizations of quicksort.  The average asymptotic number of comparisons needed is shown to be an log^2(n).  A formula is derived expressing a in terms of the probability distribution of the \"bound\" of a partition.  This formula assumes a particularly simple form for a generalization already considered by Hoare, namely, choice of the bound as median of a random sample. The main contribution of this paper is another generalization of quicksort, which uses a bounding interval instead of a single element as bound.  This generalization turns out to be easy to implement in a computer program.  A numerical approximation shows that a = 1.140 for this version of quicksort compared with 1.386 for the original.  This implies a decrease in number of comparisons of 18 percent; actual tests showed about 15 percent saving in computing time."}
{"DOCID": "1998", "TEXT": "Complex Matrix Inversion Versus Real: A comparison of complex matrix with real matrix inversion is made.  It is shown that the complex inversion can be up to twice as fast as the real inversion.  Further, the rounding error bound for complex inversion is about one-eighth that of real, for Gaussian elimination.  Using extended inner product accumulation the bound is half of the real system."}
{"DOCID": "1999", "TEXT": "Optimal Starting Approximations for Generating Square Root for Slow or No Divide: On machine with slow or no division, it is preferable to use an iterative scheme for the square root different from the classical Heron scheme.  The problem of optimal initial approximants is considered, and some optimal polynomial initial approximations are tabulated."}
{"DOCID": "2000", "TEXT": "A Variation of the Goodman-Lance Method for the Solution of Two-Point Boundary Value Problems: A recently published method for the interpolative solution of nonlinear equations is improved, and applied to give a significant variation of the Goodman-Lance method for the solution of two-point boundary value problems. The resulting method applies in particular to the numerical solution of optimal control problems in the Euler-Lagrange formulation. Quantitative estimates are presented which indicate that the variation is nearly twice as fast on some problems in the latter context."}
{"DOCID": "2001", "TEXT": "Integrating Square Roots: Differential equation of the (y')^2 = f(y) are difficult to integrate numerically because of the singularity at points where f(y) vanishes.  A simple trick removes the singularity."}
{"DOCID": "2002", "TEXT": "AMESPLOT-A Higher Level Data Plotting Software System: AMESPLOT is an extensible software system designed to make the display of data as simple, painless, and neat as possible.  The system described is hardware-independent and has been implemented on a variety of installations, of different manufacturers, having diverse configurations.  The elements common to all types of data plots are outlined and the way in which these elements may be combined into a system based on simple modules is demonstrated. These modules are specified independently and are independent of the axis systems or other attributes of the plot.  This enables plots of any complexity to be constructed by adding or replacing modules.  The basic syntax of AMESPLOT is outlined, and a brief description is given of its current utility software, consisting of \"macros\" to produce self-scaled plots, formal tablets of text-interspersed with subplots, map coastlines, and 3-D plots.  The system was formulate d in a way such that the user could supply the minimum of information, and it should be fully integrable with user's program written in most conventional higher languages.  The functions of positioning, locating, and scaling (in the layout of multiple subplots) of axes, labels, and all other elements of the plot are handled automatically by the software system unless the user specifies otherwise.  The structuring of plots from multiple, independent, self-contained subplots is described. Transformation, projection, scaling, rotation, or shifting of entire plots or subplots by the action of one or more simple modules is possible.  The user may interact freely with AMESPLOT at three levels, enabling him to construct his own data markers, alphabetic characters, and transformations, and to produce a variety of artistic and other effects."}
{"DOCID": "2003", "TEXT": "An Interactive Software System for Computers-Aided Design:  An Application to Circuit Project: The characteristics of an interactive software system, intended to constitute an interface between designer and computer during various steps of the design process, are presented.  The main emphasis is given to the description of the features of the two high level user oriented languages, operating at different levels, on which the interaction is based.  The first one is IMOL, an interactive monitor language, which is designed to perform the overall and control functions of the software system; its design criteria provide the user with commands which are both simple and efficient in order to perform all the functions needed in computer-aided circuit design.  The second one is COIF, a circuit oriented graphic language, which is designed to describe, generate, and manipulate graphic problem specifications; it is an  extension of Fortran with graphic-type variables, so that the designer who is familiar with Fortran need not learn a new language.  The application to computer-aided circuit design is in particular examined; on the other hand, the adopted design criteria provide sufficient generality to extend the use of the two languages to different computer-assisted applications."}
{"DOCID": "2004", "TEXT": "A Procedure for Generation of Three-dimensional Half-toned Computer Graphics Presentations: A description is given of an algorithm for producing computer generated half-tone presentations of three-dimensional polygonal surface structures. This algorithm achieves a significant increase in speed of computation over the Warnock algorithm developed at the University of Utah and implemented also on the Coordinated Science Laboratory CDC 1604 computer system at the University of Illinois.  The history leading to the algorithm development and then the algorithm itself are described. Results are presented and are compared with computer runs achieved by the Warnock approach.  An extension of the procedure to variable position illumination sources is also given."}
{"DOCID": "2005", "TEXT": "Proposed Revision of American National Standard X3.21-1967, \"Rectangular Holes in Twelve-Row Punched Cards\"*"}
{"DOCID": "2006", "TEXT": "Proposed American National Standard"}
{"DOCID": "2007", "TEXT": "Algorithms Policy/Revised August 1970"}
{"DOCID": "2008", "TEXT": "Gaussian Quadrature Formulas (Algorithm 331 $D1))"}
{"DOCID": "2009", "TEXT": "Simpson's Rule for Multiple Integration (Algorithm 233 $D1))"}
{"DOCID": "2010", "TEXT": "Unitary Symmetric Polynomials $Z) (Algorithm 391)"}
{"DOCID": "2011", "TEXT": "Sequency Ordered Walsh Functions $S22) (Algorithm 390)"}
{"DOCID": "2012", "TEXT": "Binary Ordered Walsh Functions $S22) (Algorithm 389)"}
{"DOCID": "2013", "TEXT": "Rademacher Function $S22) (Algorithm 388)"}
{"DOCID": "2014", "TEXT": "Function Minimization and Linear Search $E4) (Algorithm 387)"}
{"DOCID": "2015", "TEXT": "A Technique for Generating Almost Optimal Floyd-Evans Productions for Precedence Grammars: A technique is developed for generating almost optimal Floyd-Evans productions given a precedence grammar. A graph formulation is used for the problem of merging productions.  The productions generated correspond to the minimum cost inverse-arborescence of that graph.  The validity of the technique is demonstrated for weak precedence grammars defined here, but the productions mechanically generated for any precedence grammar can often be modified in such a way that correct, almost optimal parsers are obtained."}
{"DOCID": "2016", "TEXT": "The Instrumentation of Multics: An array of measuring tools devised to aid in the implementation of a prototype computer utility is discussed.  These tools include special hardware clocks and data channels, general purpose programmed probing and recording tools, and specialized measurement facilities.  Some particular measurements of interest in a system which combines demand paging with multiprogramming are described in detail.  Where appropriate, insight into effectiveness (or lack there of) of individual tools is provided."}
{"DOCID": "2017", "TEXT": "Sorting in a Paging Environment: This sorting study was part of an extensive measurement project undertaken on the M44/44X, an experimental paging system which was conceived and implemented at IBM Research in order to explore the virtual machine concept.  The study was concerned with the implementation of sorting procedures in the context of the dynamic paging environment characteristic of virtual memory machines.  Descriptions of the experimental sort programs and analysis of the performance measurement results obtained for them are presented. The insight gained from the experimental effort is used to arrive at a set of broad guidelines for writing sort programs for a paging environment."}
{"DOCID": "2018", "TEXT": "Full Table Quadratic Searching for Scatter Storage: The quadratic residue search method for hash tables avoids much of the clustering experienced with a linear search method.  The simple quadratic search only accesses half the table.  It has been shown that when the length of the table is a prime of the form 4n+3, where n is an integer, the whole table may be accessed by two quadratic searches plus a separate access for the original entry point. A search method is presented which is computationally simple, has all the advantages of the quadratic search, and yet accesses all the table in one sweep."}
{"DOCID": "2019", "TEXT": "Normalization Techniques for Hand printed Numerals: Family of pattern standardization techniques based on geometrical projection is applied to a file of digitized hand printed numerals obtained from sales clerks. The principle involves transforming a quadrilateral specified in terms of the convex hull of each pattern into a square. The amount of overlap within each class of characters versus the amount between classes is used to evaluate the degree of normalization achieved with respect to other published methods including size and shear normalization through moments."}
{"DOCID": "2020", "TEXT": "The Allocation of Computer Resources-Is Pricing the Answer?: The widespread use of complex third generation computing systems has led to a much broader concern about the means by which the resources of these systems are allocated among the user community.  One means that is suggested more and more frequently is a pricing procedure.  In this paper the manner in which one would like to allocate computing resources is considered, and then the extent to which a pricing mechanism fits this mold is discussed.  Inasmuch as pricing must serve as a rationing mechanism at times, consideration is given to the means by which prices can be adjusted flexibly in order to make a dynamic allocation of resources.  Consideration is also given to the means by which users can be insulated from the harmful effects of frequent price fluctuations.  Although the subject of pricing has been given a lot of attention recently, a number of misconceptions persist about its purpose and its operation.  An attempt is made to clarify some of these misunderstandings and to highlight the advantages and disadvantages and to highlight the advantages and disadvantages of pricing. Two illustrative pricing systems are also discussed in order to demonstrate the applicability of pricing in quite different environments."}
{"DOCID": "2021", "TEXT": "A Comment on Axiomatic Approaches to Programming"}
{"DOCID": "2022", "TEXT": "Note on an Anomaly in Paging"}
{"DOCID": "2023", "TEXT": "A Note on Data Base Deadlocks"}
{"DOCID": "2024", "TEXT": "Comments on a Paper by Lowe"}
{"DOCID": "2025", "TEXT": "Student's t-Distribution; Jacobi Polynomials; Modified Romberg Quadrature; Factorial Analysis of Variance; (Algorithms 332,344,351,359)"}
{"DOCID": "2026", "TEXT": "Exponential Integral (Algorithm 385 $S13))"}
{"DOCID": "2027", "TEXT": "Ricatti-Bessel Functions of First and Second Kind (Algorithm 22 $S17))"}
{"DOCID": "2028", "TEXT": "Greatest Common Divisor of n Integers and Multipliers $A1) (Algorithm 386)"}
{"DOCID": "2029", "TEXT": "Exponential Integral $S13) (Algorithm 385)"}
{"DOCID": "2030", "TEXT": "Context-Sensitive Parsing: This paper presents a canonical form for context-sensitive derivations and a parsing algorithm which finds each context-sensitive analysis once and only once.  The amount of memory required by the algorithm is essentially no more than the required to store a single complete derivation.  In addition, a modified version of the basic algorithm is presented which blocks infinite analyses for grammars which contain loops.  The algorithm is also compared with several previous parsers for context-sensitive grammars and general rewriting systems, and the difference between the two types of analyses is discussed.  The algorithm appears to be complementary to an algorithm by S. Kuno in several respects, including the space-time trade-off and the degree of context dependence involved."}
{"DOCID": "2031", "TEXT": "Algorithm and Bound for the Greatest Common Divisor of n Integers: A new version of the Euclidean algorithm for finding the greatest common divisor of n integers a(i) and multipliers x(i) such that gcd = x(1)a(1) + ... + x(n)a(n) is presented.  The number of arithmetic operations and the number of storage locations are linear in n.  A theorem of Lame that gives a bound for the number of iterations of the Euclidean algorithm for two integers is extended to the case of n integers.  An algorithm to construct a minimal set of multipliers is presented.  A Fortran program for the algorithm appears as Comm. ACM Algorithm 386."}
{"DOCID": "2032", "TEXT": "File Structures Using Hashing Functions: A general method of file structuring is proposed which uses a hashing function to define tree structure.  Two types of such trees are examined, and their relation to trees studied in the past is explained.Results for the probability distributions of path lengths are derived and illustrated."}
{"DOCID": "2033", "TEXT": "Space/Time Trade-offs in Hash Coding with Allowable Errors: In this paper trade-offs among certain computational factors a given set of messages.  Two new hash-coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency.  The new methods are intended to reduce the amount of space required to contain the hash-coded information from that associated with conventional methods.  The reduction in space is accomplished by exploiting the possibility that a small fraction of errors of commission may be tolerable in some applications, in particular, applications in which a large amount of data is involved and a core resident hash area is consequently not feasible using conventional methods.  In such applications, it is envisaged that overall performance could be improved by using a smaller core resident hash area in conjunction with the new methods and, when necessary, by using some secondary and perhaps time-consuming test to \"catch\" the small fraction of errors associated with new methods.  An example is discussed which illustrates possible areas of application for the new methods.  Analysis of the paradigm problem demonstrates that allowing a small number of test messages to be falsely identified as members of the given set will permit a much smaller hash area to be used without increasing reject time."}
{"DOCID": "2034", "TEXT": "The Mobile Programming System: STAGE2: STAGE2 is the second level of a bootstrap sequence which is easily implemented on any computer.  It is a flexible, provided by STAGE2 are summarized, and the implementation techniques which have made it possible to have STAGE2 running on a new machine with less than one man-week of effort are discussed.  The approach has been successful on over 15 machines of widely varying characteristics."}
{"DOCID": "2035", "TEXT": "Conversational Access to a 2048-Word Machine: LAP6 is an on-line system running on a 2048-word LINC which provides full facilities for text editing, automatic filing and file maintenance, and program preparation and assembly.  It focuses on the preparation and editing of continuously displayed 23,040-character text strings (manuscripts) which can be positioned anywhere by the user and edited by simply adding and deleting lines as though working directly on an elastic scroll. Other features are available through a uniform command set which itself can be augmented by the user.  The machine, although small, aids program design by providing display scope and premarked randomly addressable LINC tapes as standard items, in an environment similar to that of a sophisticated terminal.  The tapes are logically similar to a disk.  Priority was given to the design of efficient tape algorithms to minimize the limitations of the small memory.  Techniques developed for handling scroll editing, filing, and the layered system structure are outlined.  LAP6 is used by about 2000 people in 11 countries. Its design was strongly influenced by performance criteria established in interviews held with LINC users themselves during the specification period."}
{"DOCID": "2036", "TEXT": "An Interactive Command Generating Facility: A facility to permit conversationally controlled tasks to be executed in a noninteractive environment is proposed. A means by which programs can generate interactive time-sharing commands and receive the corresponding output response is presented.  The commands will be invoked as if they had been typed at a console keyboard.  It is argued that this facility will help overcome some of the current limitations in man-computer communication. A set of functions to accomplish the above which could be embedded into any string processing language is suggested, and necessary information pertinent to implementation of the facility on existing time-sharing systems is given."}
{"DOCID": "2037", "TEXT": "Permutations of a Set with Repetitions (Algorithm 383 $G6))"}
{"DOCID": "2038", "TEXT": "Combinations of M Out of N Objects (Algorithm 382 $G6))"}
{"DOCID": "2039", "TEXT": "Permanent Function of a Square Matrix I and II (Algorithm 361 $G6))"}
{"DOCID": "2040", "TEXT": "Modified Romberg Quadrature (Algorithm 351 $D1))"}
{"DOCID": "2041", "TEXT": "Shellsort (Algorithm 201 $M1))"}
{"DOCID": "2042", "TEXT": "Treesort 3 (Algorithm 245 $M1)): The certification of an algorithm can take the form of a proof that the algorithm is correct.  As an illustrative but practical example, Algorithm 245, TREESORT 3 for sorting an array, is proved correct."}
{"DOCID": "2043", "TEXT": "Eigenvalues and Eigenvectors of a Real Symmetric Matrix $F2) (Algorithm 384)"}
{"DOCID": "2044", "TEXT": "Permutations of a Set with Repetitions (Algorithm 383 $G6))"}
{"DOCID": "2045", "TEXT": "Combinations of M Out of N Objects (Algorithm 382 $G6))"}
{"DOCID": "2046", "TEXT": "A Relational Model of Data for Large Shared Data Banks: Future users of large data banks must be protected from having to know how the data is organized in the machine (the internal representation).  A prompting service which supplies such information is not a satisfactory solution.  Activities of users at terminals and most application programs should remain unaffected when the internal representation of data is changed and even when some aspects of the external representation are changed.  Change in data representation will often be needed as a result of changes in query, update, and report traffic and natural growth in the types of stored information.  Existing noninferential, formatted data systems provide users with tree-structured files or slightly more general network models of the data.  In Section 1, inadequacies of these models are discussed. A model based on n-ary relations, a normal form for data base relations, and the concept of a universal form for data base relations, and the concept of a universal data sublanguage are introduced.  In Section 2, certain operations on relations (other than logical inference) are discussed and applied to the problems of redundancy and consistency in the user's model."}
{"DOCID": "2047", "TEXT": "Incorporating Origin Shifts into the QR Algorithm for Symmetric Tridiagonal Matrices: The QR iteration for the eigenvalues of a symmetric tridiagonal matrix can be accelerated by incorporating a sequence of origin shifts.  The origin shift may be either subtracted directly from the diagonal elements of the matrix or incorporated by means of an implicit algorithm.  Both methods have drawbacks: the direct method can unnecessarily degrade small eigenvalues, while the implicit method can effectively loose the shift and thereby retard the convergence.  This paper presents a new method which has neither drawback."}
{"DOCID": "2048", "TEXT": "Comparison of Several Adaptive Newton-Cotes Quadrature Routines in Evaluating Definite Integrals with Peaked Integrands: This report compares the performance of five different adaptive quadrature schemes, based on Newton-Cotes (2N + 1) point rules (N = 1, 2, 3, 4, 5), in approximating the set of definite integrals INTEGRAL$1/(x^2 + p^2)) dx with relative accuracy e."}
{"DOCID": "2049", "TEXT": "Accurate Floating-Point Summation: This paper describes an alternate method for summing a set of floating-point numbers.  Comparison of the error bound for this method with that of the standard summation method shows that it is considerably less sensitive to propagation of round-off error."}
{"DOCID": "2050", "TEXT": "Automatic Parsing for Content Analysis: Although automatic syntactic and semantic analysis is not yet possible for all of an unrestricted natural language text, some applications, of which content analysis is one, do not have such a stringent coverage requirement. Preliminary studies show that the Harvard Syntactic Analyzer can produce correct and unambiguous identification of the subject and object of certain verbs for approximately half of the relevant occurrences. This provides a degree of coverage for content analysis variables which compares favorably to manual methods, in variables which compares favorably to manual methods, in which only a sample of the total available text is normally processed."}
{"DOCID": "2051", "TEXT": "A PL/I Program to Assist the Comparative Linguist: A practical PL/I program is described which can assist comparative linguists to determine the regular sound correspondences between genetically related languages. The investigator must arrange data for input by aligning pairs of suspected cognates.  The program tabulates the correspondences, and uses list processing techniques to sort and count them. Each pair of words is then assigned a relative value that is a function of the total frequency in the data of each correspondence found in that pair of words.  The output is a list of all correspondence types with their frequency of occurrence in the data, and a separate listing of each correspondence with all word-pairs showing that correspondence (unless their relative value is below an arbitrarily chosen cutoff point).  The article explains the usefulness, as well as the limitations, of the programs, and illustrates its use with a small portion of hypothetical data."}
{"DOCID": "2052", "TEXT": "Scheduling to Reduce Conflict in Meetings: Conflicts in scheduling can be treated as defining an undirected linear graph independently of the relation of the activities in conflict to additional constraints of time and space.  Each connected component of such a graph, which can be found by an algorithm described by Gotlieb and Corneil, corresponds to a set of events that must be scheduled at different times."}
{"DOCID": "2053", "TEXT": "On the Conversion of Decision Tables to Computer Programs: The use of execution time diagnostics in pinpointing ambiguities in decision tables is discussed.  It is pointed out that any attempt at resolving ambiguities at compile time will, in general, be impossible.  It is shown that, as a consequence, tree methods of converting decision tables to programs are inadequate in regard to ambiguity detection. Two algorithms for programming decision tables whose merits are simplicity of implementation and detection of ambiguities at execution time are presented. The first algorithm is for limited entry decision tables and clarifies the importance of proper coding of the information in the decision table.  The second algorithm programs a mixed entry decision table directly without going through the intermediate step of conversion to a limited entry form, thereby resulting in storage economy.  A comparison of the algorithms and others proposed in the literature is made.  Some features of a decision table to Fortran IV translator for the IBM 7044 developed by the authors are given."}
{"DOCID": "2054", "TEXT": "On the Feasibility of Voice Input to an On-line Computer Processing System: An on-line digital computer processing system is considered in which an ordinary telephone is the complete terminal device, input to the computer being provided as a sequence of spoken words, and output to the user being audio responses from the machine.  The feasibility of implementing such a system with a FORTRAN-like algebraic compiler as the object processor is considered.  Details of a specific word recognition program are given.  This technique depends on three simplifying restrictions, namely, a \"small\" vocabulary set, \"known\" speakers, and a \"moment of silence\" between each input word.  Experimental results are presented giving error rates for different experimental conditions as well as the machine resources required to accommodate several users at a time. The results show that at this time it is both economically and logically feasible to handle at least 40 users at a time with an IBM 360/65 computer."}
{"DOCID": "2055", "TEXT": "Subroutine to Perform In-Situ Transposition of a Rectangular Matrix (Algorithm 380)"}
{"DOCID": "2056", "TEXT": "Gomory (Algorithm 263A $H))"}
{"DOCID": "2057", "TEXT": "Random Vectors Uniform in Solid Angle (Algorithm 381 $G5))"}
{"DOCID": "2058", "TEXT": "In-Situ Transposition of a Rectangular Matrix (Algorithm 380 $F1))"}
{"DOCID": "2059", "TEXT": "A Language for Treating Graphs: A language for the representation of graph is described, and the formulation of graph operations such as node and/or link deletion or insertion, union, intersection, comparison, and traversal of graphs is given.  Graphs are represented by linked lists.  The language is syntactically defined as an extension to ALGOL 60, and it is translated into ALGOL by means of a syntax-driven compiler.  Application areas for this language are operation research, network problems, control theory, traffic problems, etc."}
{"DOCID": "2060", "TEXT": "GEDANKEN-A Simple Typeless Language Based on the Principle of Completeness and the Reference Concept: GEDANKEN is an experimental programming language with the following characteristics.  (1) Any value which is permitted in some context of the language is permissible in any other meaningful context.  In particular, functions and labels are permissible  results of functions and values of variables.  (2) Assignment and indirect addressing are formalized by introducing values, called reference, which in turn possess other values.  The assignment operation always affects the relation between some reference and its value,  (3) All compound data structures are treated as functions.  (4) Type declarations are not permitted.  The functional approach to data structures and the use of references insure that any process which accepts some data structure will accept any logically equivalent structure, regardless of its internal representation.  More generally, any data structure may be implicit; i.e. it may be specified by giving an arbitrary algorithm for computing or accessing its components.  The existence of label variables permits the construction of coroutines, quasi-parallel processes, and other unorthodox control mechanisms. A variety of programming examples illustrates the generality of the language. Limitations and possible extensions are discussed briefly."}
{"DOCID": "2061", "TEXT": "An Algorithm for the Construction Of Bounded-Context Parsers: An algorithm is described which accepts an arbitrary context-free grammar and constructs a bounded-context parser for it whenever such a parser exists.  In the first part of the paper the definition of a context-free grammar and the working of a bounded-context parser are recalled.  The notion of reduction class for a context-free grammar is then introduced and its connection with the structure of a bounded-context parser is indicated.  Next, pushdown automata which generate the different reduction classes of a context-free grammar are defined.  Finally, the algorithm is described; it essentially carries out an exhaustive study of all possible runs of the pushdown automata generating the reduction classes. In the second part, the utility of the algorithm is discuss ed in the light of the experience gained from its use in compiler design. The algorithm is claimed to be particularly useful in the simultaneous design of a language and a compiler for it."}
{"DOCID": "2062", "TEXT": "The Application of Sequential Sampling to Simulation: An Example Inventory Model: Four different sequential sampling procedures are applied to the analysis of data generated by a computer simulation experiment with a multi-item inventory model.  For each procedure the cost of computer time required to achieve given levels of statistical precision is calculated.  Also the cost of computer time using comparable fixed sample size methods is calculated.  The computer costs of fixed sample size procedures versus sequential sampling procedures are compared."}
{"DOCID": "2063", "TEXT": "Translation Equations (Errata)"}
{"DOCID": "2064", "TEXT": "Operations on Generalized Arrays with the Genie Compiler: Operations on vectors, matrices, and higher dimensional storage arrays are standard features of most compilers today.  The elements of such structures are usually restricted to be scalars.  For many sophisticated applications this restriction can impose cumbersome data representations. An efficient system has been devised and implemented which allows the elements of multidimensional arrays to themselves be multidimensional arrays.  This system was developed from a storage structure in which the location, length, and content of each array is described by a codeword which can be interpreted by the system.  Code words may describe arrays containing more codewords, thus providing all needed descriptive information for hyperstructures of any form."}
{"DOCID": "2065", "TEXT": "A Programming System for the On-line Analysis of Biomedical Images: A preliminary description of the software for a computer-display system is given with special emphasis on the  man-machine interaction. This system is intended for a wide variety of biomedical applications. As an example, the methods are applied to the karyotyping of chromosomes.  The system is separated into four programming tasks: picture transformations, file maintenance, picture structuring, and display management.  Picture structuring is considered as the vehicle for man-machine communication. A prototype data format for pictures, called a picture-form, is developed. Structure operators are defined which manipulate picture-forms to produce new pictures-forms.  Many of the ideas are taken from the symbolic mathematical laboratory at MIT conceived by Marvin Minsky."}
{"DOCID": "2066", "TEXT": "An Algol Construction for Procedures as Parameters of Procedures"}
{"DOCID": "2067", "TEXT": "Comment on Lawler's Multilevel Boolean Minimization"}
{"DOCID": "2068", "TEXT": "Comment on Multiprogramming Under a Page on Demand Strategy"}
{"DOCID": "2069", "TEXT": "Comments on a Paper by Wallace and Mason"}
{"DOCID": "2070", "TEXT": "A Formal System for Information Retrieval from Files"}
{"DOCID": "2071", "TEXT": "Filon Quadrature (Algorithm 353 $D1))"}
{"DOCID": "2072", "TEXT": "Modified Romberg Quadrature (Algorithm 351 $D1))"}
{"DOCID": "2073", "TEXT": "Solution of Linear Programs in 0-1 Variables by Implicit Enumeration (Algorithm 341 $H))"}
{"DOCID": "2074", "TEXT": "Sqank (Algorithm 379 $D1))"}
{"DOCID": "2075", "TEXT": "Discretized Newton-Like Method for Solving a System of Simultaneous Nonlinear Equations (Algorithm 378 $C5))"}
{"DOCID": "2076", "TEXT": "Cubic Splines on Uniform Meshes: A very simple procedure is presented for constructing cubic splines, periodic or nonperiodic, on uniform meshes.  Arcs of two cubics suffice to construct a basis of cardinal splines.  An algorithm is given which requires only minimal storage and computation and permits easy trade-off of one against the other."}
{"DOCID": "2077", "TEXT": "The Cyclical Majority Problem: The problem of the cyclical majority is presented and some new, simulated results for 3, 4, 5, ..., 40 issues ad 3, 5, 7, ..., 37 judges are reported."}
{"DOCID": "2078", "TEXT": "Representations for Space Planning: Problems involving the arrangement of objects in two- or three-space where the objective function primarily consists of derivatives of the distance between objects or their arrangement are called space planning problems.  The representational requirements for this problem area are defined and compared with current computer graphic languages.  Four alternative data structures that allow automated space planning are described and compared."}
{"DOCID": "2079", "TEXT": "On Multiprogramming, Machine Coding, and Computer Organization"}
{"DOCID": "2080", "TEXT": "The Nucleus of a Multiprogramming System: This paper describes the philosophy and structure of a multiprogramming system that can be extended with a hierarchy of operating systems to suit diverse requirements of program scheduling and resource allocation.  The system nucleus simulates an environment in which program execution and input/output are handled uniformly as parallel, cooperating process es.  A fundamental set of primitives allows the dynamic creation and control of a hierarchy of processes as well as the communication among them."}
{"DOCID": "2081", "TEXT": "Some Complete Calculi for Matrices: A matrix calculus is introduced with the intention of developing data structures suitable for a high level algorithmic language for mathematical programming. The paper investigates how the special structure of matrices can be described and utilized for efficient computing by saving memory space and superfluous operations.  Sequences of Matrices (and sequences of sequences of matrices) are considered, and matrix operators areext ended to sequence operators and cumulative operators.  Algorithms are given which use symbol manipulation of matrix expressions so as to find the forms best suited for computation.  These forms are called normal forms.  Several completeness results are obtained in the sense that for each expression an equivalent expression in normal form can be found within a specified calculus."}
{"DOCID": "2082", "TEXT": "Syntax-Directed Documentation For PL 360: The language PL 360, together with its phrase structure grammar, is used as a concrete basis for illustrating an idea called syntax-directed documentation. This idea is (1) to use the phrase structure of a program to define the structure of a formal documentation for that program; (2) to use the syntactic types and identifiers in the resulting structure to trigger the automatic formation of questions to the programmer, whose answers will become part of that documentation; and (3) to provide automatic storage and retrieval facilities so that other programmers who want to understand or modify the program can access the resulting documentation, which is cross-indexed in various ways by syntactic types and objects.  A small PL 360 program, already found in the literature, is worked out as an example."}
{"DOCID": "2083", "TEXT": "Creation and Control of Internal Data Bases Under a Fortran Programming Environment: A method is described for the definition of a user's COMMON structure and the automatic generation of the necessary COMMON, DIMENSION, EQUIVALENCE, and type declarations for each of the user's routines.  The definition for the COMMON is contained in an easy to modify form, thus allowing the control of general communications of data between routines. The described system has been implemented on the IBM 7094, CDC 6000 series, and the IBM 360.  The method has proved to be invaluable for the definition and control of COMMON in many large-scale programs."}
{"DOCID": "2084", "TEXT": "A Note on the Complement of Inherently Ambiguous Context-Free Languages"}
{"DOCID": "2085", "TEXT": "Comment on a Paging Anomaly"}
{"DOCID": "2086", "TEXT": "Another Method of Converting from Hexadecimal to Decimal"}
{"DOCID": "2087", "TEXT": "A Number System for the Permutations"}
{"DOCID": "2088", "TEXT": "Netflow (ALgorithm 336 $H))"}
{"DOCID": "2089", "TEXT": "Prime Number (Algorithm 310 $A1))"}
{"DOCID": "2090", "TEXT": "Symbolic Expansion of Algebraic Expressions (Algorithm 377 $R2))"}
{"DOCID": "2091", "TEXT": "PDEL-A Language for Partial Differential Equations: Conventional computer methods available to solve continuous system problems characterized by partial differential equations are very time-consuming and cumbersome.  A convenient, easy to learn and to use, high level problem oriented language to solve and study partial differential equation problems has been designed; a practical translator for the language has also been designed, and a working version of it has been constructed for a significant portion of the language.  This Partial Differential Equation Language, PDEL, is outlined, and the highlights of the translator are briefly summarized."}
{"DOCID": "2092", "TEXT": "A Deductive Question-Answer for Natural Language Inference: The question-answering aspects of the Protosynthex III pro totype language processing system are described and exemplified in detail.  The system is written in LISP 1.5 and operates on the Q-32 time-sharing system.  The system's data structures and their semantic organization, the deductive question-answering formalism of relational properties and complex-relation-forming operators, and the question-answering procedures which employ these features in their operation are all described and illustrated.  Examples of the system's performance and of the limitations of its question-answering capability are presented and discussed.  It is shown that the use of semantic information in deductive question answering greatly facilitates the process, and that a top-down procedure which works from question to answer enables effective use to be made of this information.  It is concluded that the development of Protosynthex III into a practically useful system to work with large data bases is possible but will require changes in both the data structures and the algorithms used for question answering."}
{"DOCID": "2093", "TEXT": "A Comparison of Error Improvement Estimates for Adaptive Trapezoid Integration: Various simple choices of error improvement estimates for the trapezoid rule are studied to demonstrate a comparison procedure which is relatively independent of the profusion of adaptive search and stopping strategies.  Comparisons are based on x^r, `; the inclusion of the noninteger powers makes this more realistic than the usual polynomial based comparison.  Behavior near the singularity was found to be the dominant factor, and a new estimate, based on a constant curvature assumption and parametric differences, was considered slightly better than the other choices considered."}
{"DOCID": "2094", "TEXT": "On an Algorithm for Nonlinear Minimax Approximation: Certain nonlinear minimax approximation problems are characterize d by properties which permit the application of special algorithms, mainly based on the exchange algorithms of Remes (1934, 1935), for their solution.  In this paper the application to problems of this type of a general nonlinear algorithm due to Osborne and Watson (1969) is considered.  Examples are given to illustrate that this algorithm can give satisfactory results and, in particular, can successfully solve problems which lead to difficulties with the more conventional specialist method."}
{"DOCID": "2095", "TEXT": "Measurements of Segment Size: Distributions of segment sizes measured under routine operating con ditions on a computer system which utilizes variable sized segments (the Burroughs B5500) are discussed.  The most striking feature of the measurements is the large number of small segments-about 60 percent of the segments in use contain less than 40 words.  Although the results are certainly not installation independent, and although they are particularly influenced by features of the B5500 ALGOL system, they should be relevant to the design of new computer systems, especially with respect to the organization of paging schemes."}
{"DOCID": "2096", "TEXT": "Experiments with the M & N Tree-Searching Program: The M & N procedure is an improvement to the mini-max backing-up procedure widely used in computer program for game-playing and other purposes.  It is based on the principle that it is desirable to have many options when making decisions in the face of uncertainty.  The mini-max procedure assigns to a MAX (MIN) node the value of the highest (lowest) valued successor to that node. The M & N procedure assigns to a MAX (MIN) node some function of the M (N) highest (lowest) valued successors.  An M & N procedure was written in LISP to play the game of kalah, and it was demonstrated that the M & N procedure is significantly superior to the mini-max procedure.  The statistical significance of important conclusions is given. Since information on statistical significance has often been lacking in papers on computer experiments in the artificial intelligence field, these experiments can perhaps serve as a model for future work."}
{"DOCID": "2097", "TEXT": "A Program to Teach Programming: The TEACH system was developed at MIT to ease the cost and improve the results of elementary instruction in programming.  To the student, TEACH offers loosely guided experience with a  conversational language which was designed with teaching in mind.  Faculty involvement is minimal.  A term of experience with TEACH is discussed.  Pedagogically, the system appears to be successful; straightforward reimplementation will make it economically successful as well. Similar programs of profound tutorial skill will appear only as the results of extended research.  The outlines of his research are beginning to become clear."}
{"DOCID": "2098", "TEXT": "t-Test Probabilities (Algorithm 321); Student's t-Distribution (Algorithm 344)"}
{"DOCID": "2099", "TEXT": "Eigenvalues and Eigen vectors of a Real General Matrix (Algorithm 343 $F))"}
{"DOCID": "2100", "TEXT": "Ortho (Algorithm 127 $F5))"}
{"DOCID": "2101", "TEXT": "Least Squares Fit By f(x) = Acos(Bx+C) (Algorithm 376 $E2))"}
{"DOCID": "2102", "TEXT": "Fitting Data To One Exponential (Algorithm 375 $E2))"}
{"DOCID": "2103", "TEXT": "Restricted Partition Generator (Algorithm 374 $A1))"}
{"DOCID": "2104", "TEXT": "Number of Doubly Restricted Partitions (Algorithm 373 $A1))"}
{"DOCID": "2105", "TEXT": "An Interactive Computer System Using Graphical Flowchart Input: An interactive computer system operational on a graphical computer terminal is described.  This system was designed to demonstrate a method of programming by computer interpretation of a flowchart.  The user draws a description of a sampled-data system and specifies description is transmitted to a large scale computer.  The design is simulated, and a graphic representation of the processed signal is returned to the scope.  A successful design may require numerous modifications of the original design.  A graphical interactive system provides an environment to perform this iterative process efficiently and effectively."}
{"DOCID": "2106", "TEXT": "Computer Education in a Graduate School of Management: Several years of experience have led to the belief that the creative design and evaluation of management information systems requires a thorough understanding of the related computer technology.  Concepts such as paging and priority interrupt systems can best be explained at the machine language level.  Any machine used for exposition should fulfill several criteria.  It should: (1) raise as few spurious issues as possible; (2) allow, without undue effort, the solution of interesting problems; (3) be capable of exposing all outstanding issues of significance, capable of exposing all outstanding issues of significance, within the chosen machine; (4) be seful for pursuing issues in great depth when appropriate; (5) not be committed to the equipment provided by any manufacturer; (6) be able to provide the student with diagnostic aids to a great depth; (7) allow the student ready access to the machine; (8) be capable of extension to expose new issues as they come along.  We have constructed a simulated machine and its associated software which meets these criteria.  This system, called the PRISM system, is documented by a primer and a reference manual."}
{"DOCID": "2107", "TEXT": "The Quadratic Quotient Method: A Hash Code Eliminating Secondary Clustering: Secondary clustering as a cause of hash code inefficiency is discussed, and a new hashing method based on its elimination is presented.  Comparisons with previous methods are made both analytically and empirically."}
{"DOCID": "2108", "TEXT": "A Variation on Sorting by Address Calculation: The principles of address calculation and merging are combined to yield an efficient sorting technique. Detailed flowcharts of the most important program steps are included. The characteristics of the proposed sort are discussed."}
{"DOCID": "2109", "TEXT": "The Use of Quadratic Residue Research: A quadratic residue search method has previously been suggested to avoid the clustering usually encountered when hash address collisions occur and linear search methods are used.  The search size, because of the property of quadratic residues, is limited to one half of the storage table.  It is shown that for some classes of prime numbers the complement of the set of quadratic residues can easily be determined and hence the entire table of size p, where p is that prime number, can be searched."}
{"DOCID": "2110", "TEXT": "An Efficient Context-free Parsing Algorithm: A parsing algorithm which seems to be the most efficient general context-free algorithm known is described.  It is similar to both Knuth's LR(k) algorithm and the familiar top-down algorithm.  It has a time bound proportional to n^3 (where n is the length of the string being parsed) in general; it has a n^2 bound for unambiguous grammars; and it runs in linear time on a large class of grammars, which seems to include most practical context-free programming language grammars.  In an empirical comparison it appears to be superior to the top-down and bottom-up algorithms studied by Griffiths and Petrick."}
{"DOCID": "2111", "TEXT": "Spelling Correction in Systems Programs: Several specialized techniques are shown for efficiently incorporating spelling correction algorithms in to compilers and operating systems.  These include the use of syntax and semantics information, the organization of restricted keyword and symbol tables, and the consideration of a limited class of spelling errors.  Sample 360 coding for performing spelling correction is presented.  By using systems which perform spelling correction, the number of debugging runs per program has been decreased, saving both programmer and machine time."}
{"DOCID": "2112", "TEXT": "Translation Equations: Input limited transduction expressions, or translation equations, are used to describe the syntax and left-context sensitive semantics for context-free languages.  A formal procedure is given for deriving from a set of translation equations the specifications for a pushdown translator. The translator consists of Mealy form finite-state automata interacting by means of a pushdown stack.  Within the framework described string recognition and parsing may be treated as special cases of the translation problem."}
{"DOCID": "2113", "TEXT": "The Multistore Parser for Hierarchical Syntactic Structures: A syntactic parser is described for hierarchical concatenation patterns that are presented to the analyzer in the form of linear strings.  Particular emphasis is given to the system of \"significant addresses\" by means of which processing times for large-scale matching procedures can be substantially reduced.  The description makes frequent use of examples taken from the fully operational implementation of the parser in an experimental English sentence analyzer.  By structuring an area of the computer's central core storage in such a way that the individual locations of bytes and bits come to represent the data involved in the matching procedure, the shifting of information is reduced to a minimum, and the searching of lists is eliminated altogether.  The matches are traced by means of binary masks and the state of single bits determines the operational flow of the procedure.  The method could be implemented with any interpretive grammar, provided it can be expressed by the functional classification of the items composing the input hierarchical structures."}
{"DOCID": "2114", "TEXT": "A Formal System for Information Retrieval from Files: A generalized file structure is provided by which the concepts of keyword, index, record, file, directory, file structure, directory decoding, and record retrieval are defined and from which some of the frequently used file structures such as inverted files, index-sequential files, and multilist files are derived.  Two algorithms which retrieve records from the generalized file structure are presented."}
{"DOCID": "2115", "TEXT": "Fortran Tausworthe Pseudorandom Number Generator"}
{"DOCID": "2116", "TEXT": "Interchange Rolls of Perforated Tape for Information Interchange* (Proposed American National Standard)"}
{"DOCID": "2117", "TEXT": "Representation for Calen dar Date for Machine-to-Machine Data Interchange* (Proposed American National Standard)"}
{"DOCID": "2118", "TEXT": "An Efficient Algorithm for Sorting with Minimal Storage (Algorithm 347 $M1))"}
{"DOCID": "2119", "TEXT": "Derivatives (Algorithm 282 $S22))"}
{"DOCID": "2120", "TEXT": "An Algorithm to Produce Complex Primes, Csieve (Algorithm 372 $A1))"}
{"DOCID": "2121", "TEXT": "Partitions in Natural Order (Algorithm 371 $A1))"}
{"DOCID": "2122", "TEXT": "General Random Number Generator (Algorithm 370 $G5))"}
{"DOCID": "2123", "TEXT": "Generator of Random Numbers Satisfying the Poisson Distribution (Algorithm 369 $G5))"}
{"DOCID": "2124", "TEXT": "Numerical Inversion of Laplace Transforms (Algorithm 368 $D5))"}
{"DOCID": "2125", "TEXT": "A Note on Minimal Length Polygonal Approximation to a Digitized Contour: A method for extracting a smooth polygonal contour from a digitized image is illustrated. The ordered sequence of contour points and the connection graph of the image are first obtained by a modified Ledley algorithm in one image scan.  A minimal perimeter polygon subjected to specified constraints is then chosen as the approximating contour.  The determination of the minimal polygon can be reduced to a nonlinear programming problem, solved by an algorithm which takes into account the weak bonds between variables.  Some examples are presented, and the corresponding computing times are listed."}
{"DOCID": "2126", "TEXT": "Experience with an Extensible Language: An operational extensible language system is described. The system and its base language are appraised with respect to efficiency, flexibility, and utility for different categories of users."}
{"DOCID": "2127", "TEXT": "Natural Language Question-Answering Systems: 1969: Recent experiments in programming natural language question-answering systems are reviewed to summarize the methods that have been developed for syntactic, semantic, and logical analysis of English strings.  It is concluded that at least minimally effective techniques have been devised for answering questions from natural language subsets in small scale experimental systems and that a useful paradigm has evolved to guide research efforts in the field.  Current approaches to semantic analysis and logical inference are seen to be effective beginnings but of questionable generality with respect either to subtle aspects of meaning or to applications over large subsets of English. Generalizing from current small-scale experiments to language-processing systems based on dictionaries with thousands of entries-with correspondingly large grammars and semantic systems-may entail a new order of complexity and require the invention and development of entirely different approaches to semantic analysis and questions answering."}
{"DOCID": "2128", "TEXT": "A Processor Allocation Method for Time-Sharing: A scheduling algorithm is proposed which is intended to minimize changes of tasks on processors and thereby reduce over-head.  The algorithm also has application to more general resource allocation problems.  It is implemented by means of a method for efficiently handling dynamically changing segmented lists."}
{"DOCID": "2129", "TEXT": "Recursive Computation of Certain Derivatives-A Study of Error Propagation: A brief study is made of the propagation of errors in linear first-order difference equations.  The recursive computation of successive derivatives of (e^x)/x and (cos x)/x is considered as an illustration."}
{"DOCID": "2130", "TEXT": "Automatic Segmentation of Cyclic Program Structures Based on Connectivity and Processor Timing: Time-shared, multiprogrammed, and overlayed batch systems frequently require segmentation of computer programs into discrete portions. These program portions are transferred between executable and peripheral storage whenever necessary; segmentation of program s in a manner that  reduces the frequency of such transfers is the subject of this paper.  Segmentation techniques proposed by C. V. Ramamoorthy are subject to limitations that arise when the preferred segment size is not compatible with the physical restrictions imposed by the available computing equipment.  A generalization of Ramamoorthy's suggestions is made in order to allow their application when circumstances are other than ideal."}
{"DOCID": "2131", "TEXT": "Rapid Computation of Weights of Interpolatory Quadrature Rules [D1] (Algorithm 417)"}
{"DOCID": "2132", "TEXT": "Rapid Computation of Coefficients of Interpolation Formulas [E1] (Algorithm 416)"}
{"DOCID": "2133", "TEXT": "Algorithm for the Assignment Problem (Rectangular Matrices) [H] (Algorithm 415)"}
{"DOCID": "2134", "TEXT": "An Extension of the Munkres Algorithm for the Assignment Problem to Rectangular Matrices: The assignment problem, together with Munkres proposed algorithm for its solution in square matrices, is presented first.  Then the authors develop an extension of this algorithm which permits a solution for rectangular matrices.  Timing results obtained by using an adapted version of Silver's Algol procedure are discussed, and a relation between solution time and problem size is given."}
{"DOCID": "2135", "TEXT": "Rapid Computation of General Interpolation Formulas and Mechanical Quadrature Rules: Let f have n continuous on a closed interval [a,b] and let L be a linear functional.  The attempt is made to approximate L (f) with L (Q) where Q is a polynomial, approximating f.  Algorithms are developed for rapid computation of L (Q) for a wide class of selections of Q which includes the Lagrangian and Hermitian rules as special cases."}
{"DOCID": "2136", "TEXT": "A Note on \"A Modification of Nordsieck's Method Using an 'Off-Step' Point\""}
{"DOCID": "2137", "TEXT": "New LISP Techniques for a Paging Environment: The system described herein employs the block concept, and that of global and local variables, in addition to the methods applied in most LISP systems. Also, a new means of list representation is used: \"local sequential\" for lists created during compilation, and \"block level sequential\" for those created dynamically.  A new garbage collection algorithm has been introduced to make lists as compact as possible; partial garbage collection is performed after each block exit instead of total garbage collection when storage is exhausted.  The algorithm does not use the customary flagging procedure.  This combination of features has eliminated the need for a free list, and effectively minimizes the number of pages used at any moment."}
{"DOCID": "2138", "TEXT": "BLISS: A Language for Systems Programming: A language, BLISS, is described.  This language is designed so as to be especially suitable for use in writing production software systems for a specific machine (the PDP-10): compilers, operating systems, etc.  Prime design goals of the design are the ability to produce highly efficient object code, to allow access to all relevant hardware features of the host machine, and to provide a rational means by which to cope with the evolutionary nature of systems programs.  A major feature which contributes to the realization of these goals is a mechanism permitting the definition of the representation of all data structures in terms of the access algorithm for elements of the structure."}
{"DOCID": "2139", "TEXT": "Implementation of the Substring Test by Hashing: A technique is described for implementing the test which determines if one string is a substring of another.  When there is low probability that the test will be satisfied, it is shown how the operation can be speeded up considerably if it is preceded by a test on appropriately chosen hash codes of the strings."}
{"DOCID": "2140", "TEXT": "Retrieval-Update Speed Tradeoffs Using Combined Indices: In a paper in the November 1970 Communications of the ACM, V. Y. Lum introduced a technique of file indexing named combined indices.  This technique permitted decreased retrieval time at the cost of increased storage space.  This paper examines combined indices under conditions of file usage with different fractions of retrieval and update.  Tradeoff curves are developed to show minimal cost of file usage by grouping various partially combined indices."}
{"DOCID": "2141", "TEXT": "Algorithmic Selection of the Best Method for Compressing Map Data Strings: The best of a dozen different methods for compressing map data is illustrated.  The choices are generated by encoding data strings-sequence of like codes-by three methods and in four directions. Relationships are developed between compression alternatives to avoid comparing all of them.  The technique has been used to compress data from forest resource maps, but is widely applicable to map and photographic data reduction."}
{"DOCID": "2142", "TEXT": "Reconstruction of Pictures from Their Projections: There are situations in the natural sciences and medicine (e.g. in electron microscopy and X-ray photography) in which it is desirable to estimate the gray levels of a digital picture at the individual points from the sums of the gray levels along straight lines (projections) at a few angles.  Usually, in such situations, the picture is far from determined and the problem is to find the \"most representative\" picture.  Three algorithms are described (all using Monte Carlo methods) which were designed to solve this problem.  The algorithms are applicable in a large and varied number of fields.  The most important uses may be the reconstruction of possibly asymmetric particles from electron micrographs and three-dimensional X-ray analysis."}
{"DOCID": "2143", "TEXT": "Chebyshev Approximation of Continuous Functions by a Chebyshev System of Functions [E2] (Algorithm 414)"}
{"DOCID": "2144", "TEXT": "On Accurate Floating-Point Summation: The accumulation of floating-point sums is considered on a computer which performs t-digit base B floating-point addition with exponents in the range -m to M.  An algorithm is given for accurately summing N t-digit floating-point numbers.  Each of these N numbers is split into q parts, forming qN t-digit floating-point numbers.  Each of these is then added to the appropriate one of n auxiliary t-digit accumulators.  Finally, the accumulators are added together to yield the computed sum.  In all, qN+n-1 t-digit floating-point additions are performed.  Under usual conditions, the relative error in the computed sum is at most [(t+1)/v]B^(1-t) for some v.  Further, with an additional q+n-1 t-digit additions, the computed sum can be corrected to full t-digit accuracy. For example, for the IBM/360 (B=16, t=14, M=63, m=64), typical values for q and n are q=2 and n=32. In this case, (*) becomes N <= 32,768, and we have [(t+1)/v]B^(1-t) = 4x16^-13."}
{"DOCID": "2145", "TEXT": "Automation of Etching-Pattern Layout: HELP (Heuristic Etching-Pattern Layout Program) is an application program developed to computerize the tedious and error-prone although vitally important wiring design of printed circuit boards.  HELP helps automate a design stage one step closer to production than logical design.  It can be used to design wiring patterns of two-layer circuit boards on which ICs in dual-in-line packages as well as discrete components such as transistors and resistors have been placed.  HELP employs two methods of wiring. One is the heuristic method, which simulates human approaches to wiring design, and the other is the theoretically interesting but time-consuming method of maze-running, based on the Lee's algorithm.  HELP performs more than 90 percent of required wiring by the heuristic path with respect to a performance function for each point-to-point, and point-to-line connection.  It can bring the number of successful wiring connections very close to 100 percent."}
{"DOCID": "2146", "TEXT": "Optimizing the Polyphase Sort: Various dispersion algorithms for the polyphase sorting procedure are examined.The optimum algorithm based on minimizing the total number of unit strings read is displayed.  The logic of this algorithm is rather complicated; hence, several other new dispersion algorithms with more straightforward logic are presented.  Of the simple dispersion algorithms discussed, the  Horizontal is best.  It does approximately one-fourth to one and one-half percent less reading and writing than most algorithms in use today.  An additional two and one-fourth to three percent improvement can be achieved by utilizing the Modified Optimum Algorithm.  This algorithm is relatively straightforward, but it requires a fairly close estimate of the total number of unit strings before the dispersion begins."}
{"DOCID": "2147", "TEXT": "Using Computers in Higher Education: Past Recommendations, Status, and Needs: Data from a survey conducted with National Science foundation support, which was published in December 1970, is reviewed, and it is pointed out that, with regard to computers in higher education, national goals stated in the Rosser and Pierce Reports have not been attained.  Quality was lacking in hardware or courses in nearly half of the associate and bachelor's degree programs in data processing, computer science, etc., offered in 1966-67.  A plea is made for continuing studies on status and goals for computing in higher education, improvement of degree programs, and a national testing laboratory for educational technology."}
{"DOCID": "2148", "TEXT": "The Composition of Semantics in Algol 68: The main features of Algol 68 are explained from a semantic point of view.  It is shown how the language permits the composition of values and actions, i.e. ultimately programs, from a minimum set of primitives with a few fundamental recursive rules of composition.  The associated syntax is briefly reviewed.  An attempt has been made to obtain a structured and simple introduction to both Algol 68 and its orthogonal design."}
{"DOCID": "2149", "TEXT": "ENTCAF and ENTCRE: Evaluation of Normalized Taylor Coefficients of an Analytic Function [C5] (Algorithm 413)"}
{"DOCID": "2150", "TEXT": "Concurrent Control with \"Readers\" and \"Writers\": The problem of the mutual exclusion of several independent processes from simultaneous access to a \"critical section\" is discussed for the case where there are two distinct classes of processes known as \"readers\" and \"writers.\"  The \"readers\" may share the section with each other, but the \"writers\" must have exclusive access.  Two solutions are presented: one of the case where we wish minimum delay for the readers; the other for the case where we wish writing to take place as early as possible."}
{"DOCID": "2151", "TEXT": "User Program Measurement in a Time-Shared Environment: A general discussion of the measurement of software systems is followed by a description of a hardware and software scheme for measuring user programs in a time-shared environment.  The TX-2 computer at MIT Lincoln Laboratory was used for the implementation of such a system and the characteristics of this implementation are reported.  A scenario showing the system in use is presented.  Finally, it is shown how other time-sharing systems may provide similar measuring facilities."}
{"DOCID": "2152", "TEXT": "Display Procedures: Although the use of structured display files is widespread in interactive computer graphics, these structures present a number of problems which tend to restrict their generality and usefulness. This paper discusses some of these problems, and suggests an alternative approach to display system design which avoids the use of structured display files. This technique employs display procedures to generate information for display.  By including transformations within calls to these procedures it is possible both to simplify the specification of pictures and to speed up their generation.  Display procedures permit picture elements to be defined conditionally and also facilitate the processing of inputs from pointing devices.  The paper is illustrated by examples from aversion of the EULER language in which display procedures were implemented."}
{"DOCID": "2153", "TEXT": "Experiments with an Automated Instructional System for Numerical Methods: A computer system was developed at Purdue University to teach portions of an undergraduate course in numerical methods.  Each instructional unit or lesson is divided into three modes of instruction which allow the student to press from a computer-controlled presentation to a student-controlled investigation. The system is designed as a classroom-independent course of study, and has been used for two semesters by students in lieu of conventional classroom instruction. Initial measures of effectiveness, student acceptance, and operational cost are the result of testing the system independent of instructor intervention. The system is operational on a CDC 6500 with teletype terminals."}
{"DOCID": "2154", "TEXT": "Clarification of Fortran Standards-Second Report: In 1966, after four years of effort, Fortran became the first programming language standardized in the United States.  Since that initial achievement study and application of the standard specifications have revealed the need for maintenance of the standards. As the result of work initiated in 1967, an initial set of clarifying interpretations was prepared and this clarification was published in Communications of the ACM in May 1969.  That work has continued and has resulted in the preparation of this second set of clarifying interpretations.  The nature of the maintenance and the new set of corrections to and interpretations of the standard specifications are reported."}
{"DOCID": "2155", "TEXT": "Toward an Understanding of Data Structures: This paper presents a notation and formalism for describing the semantics of data structures. This is based on directed graphs with named edges and transformations on these graphs.  In addition, an implementation facility is described which could be part of a programming language, which allows a programmer who has expressed the semantics of an algorithm in terms of the graphs to then specify the implementation of some of his data structures in order to gain efficiency."}
{"DOCID": "2156", "TEXT": "Comment on Cheney's List-Compaction Algorithm"}
{"DOCID": "2157", "TEXT": "Average Binary Search Length for Dense Ordered Lists"}
{"DOCID": "2158", "TEXT": "A Stopping Criterion for the Newton-Raphson Method in Implicit Multistep Integration Algorithms for Nonlinear Systems of Ordinary Differential Equations"}
{"DOCID": "2159", "TEXT": "A Note on Best One-Sided Approximations"}
{"DOCID": "2160", "TEXT": "Canonical Structure in Attribute Based File Organization: A new file structure for attribute based retrieval is proposed in this paper.  It allows queries involving arbitrary Boolean functions of the attribute-value pairs to be processed without taking intersections of lists.  The structure is highly dependent on the way in which the file is to be used and is uniquely determined by the specification of the allowed queries. Thus, for example, the structure for retrieval on the basis of ranges of values of a given attribute would be very different from one where only retrieval on the basis of a single value is permitted.  The file organization being proposed is based on the atoms of a Boolean algebra generated by the queries.  The desirable properties claimed for this structure are proved, and file maintenance questions are discussed."}
{"DOCID": "2161", "TEXT": "An Algorithm for the Blocks and Cutnodes of a Graph (Corrigendum)"}
{"DOCID": "2162", "TEXT": "An Efficient Bit Table Technique for Dynamic Storage Allocation of 2^n-word Blocks: An efficient bit table technique for dynamic storage allocation of 2^n-word blocks, which requires a minimized amount of memory for bookkeeping purposes, is described. The technique has been tested in an implementation of the list processing language L^6. A number of ideas incorporated in the processor are also described."}
{"DOCID": "2163", "TEXT": "Education Related to the Use of Computers in Organizations: The ACM Curriculum Committee on Computer Education for Management has been carrying out a study on \"Curriculum Development in Management Information Systems Education in Colleges and Universities\" under a grant from the National Science Foundation. This position paper provides a framework for the study.  Preliminary conclusions are presented on the need for education in administrative information systems, and appropriate college curricula and courses are suggested.  Also, the role of professional societies and organizations using computers is discussed, and the plans of the Committee are outlined. The initial approach of the Committee has been to describe the education necessary for the effective use of computers in organizations, to classify the positions for which education is required, and to survey educational programs now available."}
{"DOCID": "2164", "TEXT": "Symbolic Integration: The Stormy Decade: Three approaches to symbolic integration in the 1960's are described.  The first, from artificial intelligence, led to Slagle's SAINT and to a large degree to Moses' SIN.  The second, from algebraic manipulation, led to Manove's implementation and to Horowitz' and Tobey's reexamination of the Hermite algorithm for integrating rational functions.  The third, from mathematics, led to Richardson's proof of the unsolvability of the problem for a class of functions and for Risch's decision procedure for the elementary functions.Generalizations of Risch's algorithm to a class of special functions and programs for solving differential equations and for finding the definite integral are also described."}
{"DOCID": "2165", "TEXT": "General Relativity and the Application of Algebraic Manipulative Systems: The paper describes some applications of symbolic algebra systems to problems of general relativity including the derivation of the field equations, the Petrov classification of a metric, and the solution of the field equations in the presence of matter in a simple case.  Attention is drawn to the strictly algebraic difficulties encountered in this work."}
{"DOCID": "2166", "TEXT": "Automated Algebraic Manipulation in Celestial Mechanics: In this paper we consider some of the applications of automated algebraic manipulation which have been made in celestial mechanics.  Particular attention is paid to the use of Poisson series, and a typical problem in perturbation theory is described. The requirements of processors for use in celestial mechanics are considered and compared with those for general manipulation packages.  Some future directions for research using these systems are briefly outlined. To illustrate the relative simplicity of the algorithm required in celestial mechanics, a typical integration problem is considered in an appendix."}
{"DOCID": "2167", "TEXT": "Algebraic Simplification: A Guide for the Perplexed: Algebraic simplification is examined first from the point of view of a user who needs to comprehend a large expression, and second from the point of view of a designer who wants to construct a useful and efficient system.  First we describe various techniques akin to substitution.  These techniques can be used to decrease thesize of an expression and make it more intelligible to a user.  Then we delineate the spectrum of approaches to the design of automatic simplification capabilities in an algebraic manipulation system.  Systems are divided into five types.  Each type provides different facilities for the manipulation and simplification of expressions. Finally we discuss some of the theoretical results related to algebraic simplification.  We describe several positive results about the existence of powerful simplification algorithms and the number-theoretic conjectures on which they rely.  Results about the nonexistence of algorithms for certain classes of expressions are included."}
{"DOCID": "2168", "TEXT": "List Tracing in Systems Allowing Multiple Cell-Types: List-processing systems have each allowed the use of only a single size and configuration of list cell.  In this paper a system is described which allows the use of arbitrarily many different sizes and configurations of list cells, possibly not specified until run time."}
{"DOCID": "2169", "TEXT": "The Altran System for Rational Function Manipulation-A Survey: Altran is a complete system for symbolic computation with rational functions in several variables with integer coefficients.  It has been designed and implemented to handle large problems with ease and efficiency.  Considerable effort has been spent to ensure a minimum amount of machine dependence in the implementation, thus permitting the system to be installed quickly and easily on a variety of computing machines.  In this paper a brief description of the language, run time data structures, and implementation is given."}
{"DOCID": "2170", "TEXT": "Applications of Symbol Manipulation in Theoretical Physics: This paper surveys the applications of symbolic computation techniques to problems in theoretical physics.  Particular emphasis is placed on applications in quantum electrodynamics where the most activity has occurred."}
{"DOCID": "2171", "TEXT": "Solution of Simultaneous Nonlinear Equations"}
{"DOCID": "2172", "TEXT": "Graph Plotter [J6] (Algorithm 412)"}
{"DOCID": "2173", "TEXT": "Three Procedures for the Stable Marriage Problem [H] (Algorithm 411)"}
{"DOCID": "2174", "TEXT": "The Stable Marriage Problem: The original work of Gale and Shapley on an assignment method using the stable marriage criterion has been extended to find all the stable marriage assignments. The algorithm derived for finding all the stable marriage assignments is proved to satisfy all the conditions of the problem.  Algorithm 411 applies to this paper."}
{"DOCID": "2175", "TEXT": "Subexpression Ordering in the Execution of Arithmetic Expressions: An arithmetic expression can often be broken down into its component subexpressions.  Depending on the hardware environment in which the expression is to be executed, these subexpressions can be evaluated in serials, in parallel, or in a combination of these modes.  This paper shows that expression execution time can be minimized only if consideration is given to the ordering of the subexpressions.  In particular, subexpressions should be executed in order of decreasing memory and processor time requirements.  This observation is valid for configurations ranging from a uniprocessor with an unbuffered main memory to multiprocessor with a \"cache\" buffer memory.  If the number of subexpressions which can be executed in parallel exceeds the number of available processors, then execution of some of these subexpressions must be postponed.  A procedure is given which combines this requirement with the earlier ordering considerations to provide an optimal execution sequence."}
{"DOCID": "2176", "TEXT": "Buffer Allocation in Merge-Sorting: A fixed buffer allocation for merge-sorting is presented here which minimizes the number of input-output operations for a given order of merge. When sorting on movable arm disks, the number of seeks is equal to the number of input-output operations, and the seek time usually controls the sort time.  First some standard terminology is introduced. Then the input buffer allocation method is described, followed by an analysis of the improvement to be expected over more conventional allocation.  This analysis makes use of a particular distribution function.  An analysis of a completely different distribution is given which yields similar results.  This suggests that the results do not depend on a particular distribution function.  An optimum output buffer size is also determined.  It is concluded that this buffering allocation can significantly reduce the time of merge sorting on movable arm disks when the input data are not random, and that this output buffer allocation should be used whether the data is random or not."}
{"DOCID": "2177", "TEXT": "An Algorithm for the Blocks and Cutnodes of a Graph: An efficient method is presented for finding blocks and cutnodes of an arbitrary undirected graph.  The graph may be represented either (i) as an ordered list of edges or (ii) as a packed adjacency matrix.  If w denotes the word length of the machine employed, the storage (in machine words) required for a graph with n nodes and m edges increases essentially as 2(m+n) in case (i), or (n^2)/win case (ii).  A spanning tree with labeled edges is grown, two edges finally bearing different labels if and only if they belong to different blocks.  For both representations the time required to analyze a graph on n nodes increases as n^G where G depends on the type of graph, 1 <= G <= 2, and both bounds are attained. Values of G are derived for each of several suitable families of test graphs, generated by an extension of the web grammar approach.  The algorithm is compared in detail with that proposed by Read for which 1 <= G <= 3."}
{"DOCID": "2178", "TEXT": "A Language Extension for Graph Processing and Its Formal Semantics: A simple programming language \"extension,\" Graspe, for processing directed graphs is defined. Graspe consists of a type of directed graph data structure and a set of primitive operations for manipulating these structures.  Graspe may be most easily implemented by embedding it in a host language.  Emphasis is placed both on Graspe itself and on its method of definition.  Commonly, the definition of a language involves definition of the syntactic elements and explanation of the meaning to be assigned them (the semantics).  The definition of Graspe here is solely in terms of its semantics; that is, the data structures and operations are defined precisely but without assignment of a particular syntactic representation. Only when the language is implemented is assignment of an explicit syntax necessary.  An example of an implementation of Graspe embedded in Lisp is given as an illustration.  The advantages and disadvantages of the definition of a language in terms of its semantics are discussed."}
{"DOCID": "2179", "TEXT": "Simple LR(k) Grammars: A class of context-free grammars, called the \"Simple LR(k)\" or SLR(k) grammars is defined. This class has been shown to include weak precedence and simple precedence grammars as proper subsets. How to construct parsers for the SLR(k) grammars is also shown.  These parser-construction techniques are extendible to cover all of the LR(k) grammars of Knuth; they have been implemented and by direct comparison proved to be superior to precedence techniques, not only in the range of grammars covered, but also in the speed of parser construction and in the size and speed of the resulting parsers."}
{"DOCID": "2180", "TEXT": "A Programmer Training Project: A project is described whose purpose is to train selected black residents of the Albany-Schenectady area in computer programming and arrange for jobs for them in the computer field. Both the organization and curriculum of the course are discussed."}
{"DOCID": "2181", "TEXT": "The State of Computer Oriented Curricula in Business Schools 1970: The ACM Committee on Computer Education for Management, supported by a National Science Foundation Grant, is established to appraise the state of the art and to develop a series of recommendations for improving computer education for management.  To provide the Committee with material for its study of curricular needs, five regional meetings in the United States were held in 1970, at each of which a broad cross section of invited academicians and practitioners considered the state of curricula in business schools.  Three topics were covered: curricula for the general manager; computer-related material in required and functional courses; and curricula for students concentrating on computer-based information systems.  An analysis of the minutes of the meetings revealed a common set of experiences which raised similar pedagogic and economic issues.  This presentation gives a summary of the discussions; a condensation of the pedagogic and substantive concerns raised; and consideration of the resource allocation issues involved.  Preliminary to the Committee's recommendations for improving computer education for management, this report has been prepared to provide the participants and the administrators of their institutions with background information for the ongoing task of course development.  Chairman of the ten-man Committee is Daniel Teichroew (The University of Michigan)."}
{"DOCID": "2182", "TEXT": "Interrupt Driven Programming"}
{"DOCID": "2183", "TEXT": "Binary Summation"}
{"DOCID": "2184", "TEXT": "On the Meaning of Names in Programming Systems: It is assumed that there is a similarity of function between the data names of a programming language and the file names of an operating system. The two functions are discussed in terms of the same basic concepts in order to identify the extent to which they overlap. It is suggested that there is some similarity between the idea of a file directory and a storable object of type context.  Manipulations with contexts are then discussed at length.  It is noted that there is a simple extension of Church's Lambda notation that deals nicely with these ideas of context manipulation.  Whereas a function can be regarded as the abstraction based upon the first two terms of the expression Lambda(name list)(expression)(value list), it is found that a context can be viewed as an abstraction based upon the first two terms in the equivalent expression Mu(name list)(value list)(expression)."}
{"DOCID": "2185", "TEXT": "A Note on Compiling Fixed Point Binary Multiplications: An algorithm is developed for compiling, as a sequence of shifts, additions,and subtractions, many fixed point binary multiplications involving a constant.  The most significant characteristics of the algorithm are the simplicity of the test which determines if the algorithm should be applied and the degree to which it \"suggests\" efficient object code."}
{"DOCID": "2186", "TEXT": "Numerical Properties of the Ritz-Trefftz Algorithm for Optimal Control: In this paper the Ritz-Trefftz algorithm is applied to the computer solution of the state regulator problem.  The algorithm represents a modification of the Ritz direct method and is designed to improve the speed of solution and the storage requirements to the point where real-time implementation becomes feasible.  The modification is shown to be more stable computationally than the traditional Ritz approach. The first concern of the paper is to describe the algorithm and establish its properties as a valid and useful numerical technique.  In particular such useful properties as definiteness and reasonableness of condition are established for the method.  The second part of the paper is devoted to a comparison of the new techniques with the standard procedure of numerically integrating a matrix Riccati equation to determine a feedback matrix.  The new technique is shown to be significantly faster for comparable accuracy."}
{"DOCID": "2187", "TEXT": "Computer Science: A Conceptual Framework for Curriculum Planning: Two views of computer science are considered: a global view which attempts to capture broad characteristics of the field and its relationships to other fields, and a local view which focuses on the inner structure of the field.  This structure is presented in terms of the kinds of knowledge, problems, and activities that exist within the discipline, as well as the relations between them.  An approach to curriculum planning in computer science is presented which is guided by the structure of the field, by the fact that change is an important feature of the situation, and by the expectation that computer science will continue to increase its working contacts with other disciplines."}
{"DOCID": "2188", "TEXT": "An Approach to the Optimum Design of Computer Graphics Systems: Display system designers are faced with the difficult task of selecting major subsystems in an intelligent way.  Each subsystem is chosen from large numbers of alternatives; the selection is based on considerations such as system response time, system cost, and the distribution of data storage and processing between the graphics processor and its supporting data processing system.  The work reported here develops an objective, quantitative design procedure and helps give a better understanding of now to  configure display systems.  This is accomplished by means of a mathematical model of a computer driven graphics system.  The parameters of the model are functions of the capabilities of the graphics hardware and of the computational requirements of the graphics application. The model can be analyzed using numerical queueing analysis or simulation to obtain an average response time prediction.  By combining the model with an optimization, the best graphics system configuration, subject to a cost constraint, is found for several applications.  The optimum configurations are in turn used to find general display system design guidelines."}
{"DOCID": "2189", "TEXT": "Generation of Rosary Permutations Expressed in Hamiltonian Circuits: Systematic generation of a specific class of permutations fundamental to scheduling problems is described.  In a nonoriented complete graph with n vertices, Hamitonian circuits equivalent to .5(n - 1)! specific permutations of n elements, termed rosary permutations, can be defined.  Each of them corresponds to two circular permutations which mirror-image each other, and is generated successively by a number system covering 3*4*...*(n-1) sets of edges. Every set of edges {E[k]}, 1 <= E[k] <= k, 3 <= k <= (n-1) is determined recursively by constructing a Hamiltonian circuit with k vertices from a Hamiltonian circuit with k-1 vertices, starting with the Hamiltonian circuit of 3 vertices.  The basic operation consists of transposition of a pair of adjacent vertices where the position of the pair in the permutation is determined by {E[k]}.  Two algorithms treating the same example for five vertices are presented.  It is very easy to derive all possible n! permutations  from the .5(n - 1 )! rosary permutations be cycling the permutations and by taking them in the reverse order-procedures which can be performed fairly efficiently by computer."}
{"DOCID": "2190", "TEXT": "Function Minimization"}
{"DOCID": "2191", "TEXT": "ALGORITHM 410 Partial Sorting [M1]"}
{"DOCID": "2192", "TEXT": "Another Recursion Induction Principle: An inductive method for proving things about recursively defined functions is described.  It is shown to be useful for proving partial functions equivalent and thus applicable in proofs about interpreters for programming languages."}
{"DOCID": "2193", "TEXT": "On Implementation of Label Variables: Variables of label mode are conventionally implemented with a technique which fails to trap certain programming errors.  Fine-grained calendar clocks have recently become available; these allow implementation of label variables via a new technique which traps all programming errors of this variety."}
{"DOCID": "2194", "TEXT": "How To Keep the Addresses Short: An algorithm is presented for minimizing the sum of the lengths of the blocks of coding produced by an assembler or compiler when (1) the length of each computer instruction is assumed to be either \"long\" or \"short\" (\"long,\" if the memory location addressed is more than a predetermined distance from the current location; \"short,\" otherwise), and (2) there are blocks of instructions whose beginnings (origins) are separated by prespecified amounts. For example, some computers permit either 8-bit addressing (interpreted relative to the location counter) or full 16-bit addressing of all of memory.  When assembling or compiling two or more blocks of instructions which have many mutual references in such a computer, there is no simple iterative procedure for keeping as many of the addresses short as possible.  This paper demonstrates that a wide class of problems of this type can be formulated as covering problems solvable by means of elementary arithmetic operations on the column vectors of a ternary matrix."}
{"DOCID": "2195", "TEXT": "On the Optimal Detection of Curves in Noisy Pictures: A technique for recognizing systems of lines is presented.  In this technique the heuristic of the problem is not embedded in the recognition algorithm but is expressed in a figure of merit. A multistage decision process is then able to recognize in the input picture the optimal system of lines according to the given figure of merit.  Due to the global approach, greater flexibility and adequacy in the particular problem is achieved.  The relation between the structure of the figure of merit and the complexity of the optimization process is then discussed. The method described is suitable for parallel processing because the operations relative to each state can be computed in parallel, and the number of stages is equal to the length N of the curves (or to log2 N if the approximate method is used)."}
{"DOCID": "2196", "TEXT": "A Man-Machine Approach Toward Solving the Traveling Salesman Problem: The traveling salesman problem belongs to an important class of scheduling and routing problems. It is also a subproblem in solving others, such as the warehouse distribution problem.  It has been attacked by many mathematical methods with but meager success.  Only for special forms of the problem or for problems with a moderate number of points can it be solved exactly, even if very large amounts of computer time are used.  Heuristic procedures have been proposed and tested with only slightly better results.  This paper describes a computer aided heuristic technique which uses only a modest amount of computer time in real-time to solve large (100-200) point problems.  This technique takes advantage of both the computer's and the human's problem-solving abilities.  The computer is not asked to solve the problem in a brute force way as in many of today's heuristics, but it is asked to organize the data for the human so that the human can solve the problem easily. The technique used in this paper seems to point to new directions in the field of man-machine interaction and in the field of artificial intelligence."}
{"DOCID": "2197", "TEXT": "The Merit of Regional Computing Networks: One of the suggested means for stimulating the spread of computing capabilities in institutions of higher learning is through the construction of regional computing networks.  One such network has been constructed in the San Francisco Bay Area by Stanford University.  This paper reports upon the lessons learned from the operation of the network over the past two years.  A major impact of the network was not so much the computer power delivered to the schools as the awakening of computing awareness and the fostering of capability development at these schools. The expertise and assistance from the central facility as well as the sharing of ideas among the participants were other important benefits.  Both the quality and variety of services provided by the central facility were found to play a key role in the effectiveness of the network.  A regional network brings many benefits and should not be judged as a purveyor of raw computer power alone."}
{"DOCID": "2198", "TEXT": "Introduction to \"Feature Analysis of Generalized Data Base Management Systems\": This paper is a separately published introduction to a main report which analyzes the features of generalized data base management systems.  This introduction gives a review of the current state of the art in these systems and discusses the differences and similarities between capabilities found in host language systems and those found in self-contained systems.  After some discussion of the problems of data independence and binding,the four user levels are identified and described.  Technical problems facing future designers are described.  The first of these is that of handling existing stored data and the next is that of providing more complex data structures than those already available in conventional programming languages.  The problem of high level interrogation and update functions acting on network structures is mentioned, followed by a discussion of the problem of catering to a high volume of transactions initiated from terminals by parametric users-the lowest level of user.  The use of Cobol as a basis for further development work is considered at some length with respect to data structures, host language capabilities, and self-contained capabilities.  This section also assesses the effect of the Data Base Task Group proposals.  The final section outlines the ten major topics in the main body of the full report."}
{"DOCID": "2199", "TEXT": "A Sparse Matrix Package (Part I) [F4] (Algorithm 408)"}
{"DOCID": "2200", "TEXT": "On Complement Division: The division algorithm theorem is expressed in a form that permits it to serve as the basis for devising division operations that produce both quotient and remainder in complement form.  Algorithms for division yielding complement results are derived for numbers represented in any base greater than one.  Both radix and radix-less-one complementation schemes are considered.  The binary form of the algorithms thus includes both two's and one's complement implementation. The problem of quotient overflow for complement results is dealt with as is that of selecting an appropriate form of the remainder condition for complement division."}
{"DOCID": "2201", "TEXT": "Animator: An On-Line Two-dimensional Film Animation System: Animator is a computer animation system which was designed to overcome some of the inherent disadvantages associated with conventional computer animation techniques.The DEC-338 serves as an input terminal for movie making, allowing the trial and error design of picture sequences in a conversational mode.  During all stages on the system input elements (light pen, pushbuttons, and teletype) is maintained. At the user's request, this record is sent to the IBM 360/75 where the S-D 4020 instructions necessary to produce the same sequence of pictures can be generated. It is anticipated that one of the primary contributions of Animator will be the provision of a facility which will allow any professor to produce his own expository film strips."}
{"DOCID": "2202", "TEXT": "Dynamic Microprogramming: Processor Organization and Programming: A dynamically microprogrammed processor is characterized by a small (4^k 64-bit word) read-write \"micro\" storage.  The access time of this storage is similar to the cycle time of the machine (50-100 nsec).  This microstorage is used to contain both data and subroutines.  The (micro) instructions in such a processor differ from the conventional in that they perform only purely combinatorial operations; sequencing is under the control of the microinstruction. The presence of the read-write microstorage permits a more flexible assignment of resources than the read-only storage.  In particular, the processor developed in this paper stresses the simultaneous operation (within the microinstruction) of the adder, shifter, masker, and testing facilities of the processor. A microassembly language is developed and the overhead involved in subroutine linkages is analyzed. The efficiency of a flexible software linkage scheme is examined as to its overhead for various subroutine characteristics.  Finally, three examples of problem-oriented programming are considered and the resulting coding is compared against a System/360 assembly language version, with the technology normalized."}
{"DOCID": "2203", "TEXT": "Key-to-Address Transform Techniques: A Fundamental Performance Study on Large Existing Formatted Files: The results of a study of eight different key-to-address transformation methods applied to a set of existing files are presented.  As each method is applied to a particular file, load factor and bucket size are varied over a wide range.  In addition, appropriate variables pertinent only to a specific method take on different values.  The performance of each method is summarized in terms of the number of accesses required to get to a record and the number of overflow records created by a transformation. Peculiarities of each method are discussed.  Practical guidelines obtained from the results are stated. Finally, a proposal for further quantitative fundamental study is outlined."}
{"DOCID": "2204", "TEXT": "Program Development by Stepwise Refinement: The creative activity of programming-to be distinguished from coding-is usually taught by examples serving to exhibit certain techniques.  It is here considered as a sequence of design decisions concerning the decomposition of tasks into subtasks and of data into data structures.  The process of successive refinement of specifications is illustrated by a short but nontrivial example, from which a number of conclusions are drawn regarding the art and the instruction of programming."}
{"DOCID": "2205", "TEXT": "DIFSUB for Solution of Ordinary Differential Equations [D2] (Algorithm 407)"}
{"DOCID": "2206", "TEXT": "Exact Solution of Linear Equations Using Residue Arithmetic [F4] (Algorithm 406)"}
{"DOCID": "2207", "TEXT": "The Automatic Integration of Ordinary Differential Equations: An integration technique for the automatic solution of an initial value problem for a set of ordinary differential equations is described.  A criterion for the selection of the order of approximation is proposed.  The objective of the criterion is to increase the step size so as to reduce solution time. An option permits the solution of \"stiff\" differential equations.  A program embodying the techniques discussed appears in Algorithm 407."}
{"DOCID": "2208", "TEXT": "Storage Utilization in a Memory Hierarchy When Storage Assignment Is Performed by a Hashing Algorithm: The utilization of storage is studied in a two-level memory hierarchy.  The first storage level, which is the fast store, is divided into a number of storage areas.  When an entry is to be filed in the hierarchy, a hashing algorithm will attempt to place the entry into one of these areas.  If this particular area is full, then the entry will be placed into the slower second-level store, even though other areas in the first-level store may have space available.  Given the N entries have been filed in the entire hierarchy, an expression is derived for the expected number of entries filed in the first-level store.This expression gives a measure of how effectively the first-level store is being used.  By means of examples, storage utilization is then studied as a function of the hashing algorithm, the number of storage areas into which the first-level store is divided and the total size of the first-level store."}
{"DOCID": "2209", "TEXT": "A Scheduling Algorithm for a Computer Assisted Registration System: This paper presents the scheduling algorithm used in the Computer Assisted Registration System at the University of Tennessee.  Notation is defined and the logic of the algorithm necessary to implement educational policy is described.  Results from the first term's implementation are presented."}
{"DOCID": "2210", "TEXT": "Toward Automatic Program Synthesis: An elementary outline of the theorem-proving approach to automatic program synthesis is given, without dwelling on technical details.  The method is illustrated by the automatic construction of both recursive and iterative programs operating on natural numbers,lists, and trees,  In order to construct a program satisfying certain specifications a theorem induced by those specifications is proved, and the desired program is extracted from the proof.  The same technique is applied to transform recursively defined functions into iterative programs, frequently with a major gain inefficiency.  It is emphasized that in order to construct a program with loops or with recursion, the principle of mathematical induction must be applied. The relation between the version of the induction rule used and the form of the program constructed is explored in some detail."}
{"DOCID": "2211", "TEXT": "Scanned-Display Computer Graphics: A television-like scanned-display system has been successfully implemented on a Honeywell DDP-224 computer installation.  The scanned image is stored in the core memory of the computer, and software scan conversion is used to convert the rectangular coordinates of a point to the appropriate word and bit in an output display array in core storage.  Results thus far indicate that flicker-free displays of large amounts of data are possible with reasonably fast graphical interaction.  A scanned image of size 240 X 254 points is displayed at a 30 frame-per-second rate."}
{"DOCID": "2212", "TEXT": "F-DISTRIBUTION"}
{"DOCID": "2213", "TEXT": "Roots of Matrix Pencils: The Generalized Eigenvalue Problem [F2] (Algorithm 405)"}
{"DOCID": "2214", "TEXT": "Complex Interval Arithmetic: Complex interval arithmetic is defined using real interval arithmetic.  Complex interval division is defined so as to assure smallest possible resulting intervals."}
{"DOCID": "2215", "TEXT": "Application of Game Tree Searching Techniques to Sequential Pattern Recognition: A sequential pattern recognition (SPR) procedure does not test all the features of a pattern at once.  Instead, it selects a feature to be tested.  After receiving the result of that test, the procedure either classifies the unknown pattern or selects another feature to be tested, etc.  Medical diagnosis is an example of SPR.  In this paper the authors suggest that SPR be viewed as a one-person game played against nature (chance).  Virtually all the powerful techniques developed for searching two-person, strictly competitive game trees can easily be incorporated either directly or by analogy into SPR procedures. In particular, one can incorporate the \"mini average backing-up procedure\" and the \"gamma procedure,\" which are the analogues of the \"minimax backing-up procedure\" and the \"alpha-beta procedure,\" respectively. Some computer simulated experiments in character recognition are presented.  The results indicate that the approach is promising."}
{"DOCID": "2216", "TEXT": "On the Probability Distribution of the Values of Binary Trees: An integral equation is derived for the generating function for binary tree values, the values reflecting sorting effort. The analysis does not assume uniformly distributed branching ratios, and therefore is applicable to a family of sorting algorithms discussed by Hoare, Singleton, and van Emden. The solution to the integral equation indicates that using more advanced algorithms in the family makes only minor reductions in the expected sorting effort, but substantially reduces the variance in sorting effort.  Statistical tests of the values of several thousand trees containing up to 10,000 points have given first, second, and third moments of the value distribution function in satisfactory agreement with the moments computed from the generating function.  The empirical tests, as well as the analytical results, are in agreement with previously published results for the first moment in the cases of uniform and nonuniform distribution of branching ratio, and for the second moment in the case of uniform distribution of branching ratio."}
{"DOCID": "2217", "TEXT": "Experiments in Automatic Learning for a Multipurpose Heuristic Program: An automatic learning capability has been developed and implemented for use with the MULTIPLE (MULTIpurpose Program that LEarns) heuristic tree-searching program, which is presently being applied to resolution theorem-proving in predicate calculus. MULTIPLE's proving program (PP) uses two evaluation functions to guide its search for a proof of whether or not a particular goal is achievable.  Thirteen general features of predicate calculus clauses were created for use in the automatic learning of better evaluation functions for PP.  A multiple regression program was used to produce optimal coefficients for linear polynomial functions in terms of the features. Also, automatic data-handling routines were written for passing data between the learning program and the proving program, and for analyzing and summarizing results.  Data was generally collected for learning (regression analysis) from the experience of PP.  A number of experiments were performed to test the effectiveness and generality of the learning program. Results showed that the learning produced dramatic improvements in the solutions to problems which were in the same domain as those used for collection learning data.  Learning was also shown to generalize successfully to domains other than those used for data collection.  Another experiment demonstrated that the learning program could simultaneously improve performance on problems in a specific domain and on problems in a variety of domains.  Some variations of the learning program were also tested."}
{"DOCID": "2218", "TEXT": "An Analysis of Some Time-Sharing Techniques: The effectiveness of certain time-sharing techniques such as program, relocation, disk rotational delay minimization, and swap volume minimization is investigated.  Summary data is presented, and the findings are discussed.  The vehicle for this investigation was a SIMULA based simulation model reflecting an early framework for a planned Burroughs B6500 time-sharing system.  Inasmuch as the B6500 system is based upon the use of variable sized segments and a dynamic overlay procedure, data is also presented which provides some indication of the effectiveness of this type of organization in a time-sharing environment. The design characteristics and operational capabilities of the simulation model are also described."}
{"DOCID": "2219", "TEXT": "A Policy-Driven Scheduler for a Time-Sharing System: The service received by a process from a time-sharing operating system can be characterized by a resource count SUM{w[i]R[ij]} where R[ij] is the number of units of service received by process i from resource i and w[i] is the cost per unit of the service.  Each class of users can be characterized by a policy function which specifies the amount of service a user who belongs to this class should receive as a function of time.  Priority changes dynamically as a function of the difference between the service promised to the user by the policy function and the service he actually receives.  A scheduling and swapping algorithm which keeps the resource count of each process above its policy function will provide the specified level of service.  Overhead can be reduced by avoiding swaps of process which have received at least his level of service.  The algorithm has been implemented in a general purpose operating system, and it has provided significantly better service to interactive and to batch jobs than the previous scheduler."}
{"DOCID": "2220", "TEXT": "Conversion of Limited-Entry Decision Tables to Computer Programs-A Proposed Modification to Pollack's Algorithm: Pollack has proposed an algorithm for converting decision tables into flowcharts which minimize subsequent execution time when compiled into a computer program.  Two modifications of this algorithm are proposed.  The first relies on Shannon's noiseless coding theorem and the communications concept of entropy but does not completely test the ELSE Rule. The second modification completely tests the ELSE Rule but results in more executions than the first modification. Both modifications result in modification guarantees a globally optimal solution."}
{"DOCID": "2221", "TEXT": "Comment on the Conversion of Decision Tables to Computer Programs"}
{"DOCID": "2222", "TEXT": "Comment on London's Certification of Algorithm 245"}
{"DOCID": "2223", "TEXT": "Minit Algorithm For Linear Programming (Algorithm 222 [H])"}
{"DOCID": "2224", "TEXT": "Complex Gamma Function [S14] (Algorithm 404)"}
{"DOCID": "2225", "TEXT": "Circular Integer Partitioning [A1] (Algorithm 403)"}
{"DOCID": "2226", "TEXT": "Further Evidence for the Analysis of Algorithms for the Zero-One Programming Problem: The purpose of this note is to report computational experience additional to that recently summarized by Gue et al, with two algorithms for the zero-one linear programming problem.  An error in Gue's paper is corrected.  The utility of one of the algorithms as a suboptimizer is indicated."}
{"DOCID": "2227", "TEXT": "Proof of a Program: FIND: A proof is given of the correctness of the algorithm \"Find.\"  First, a informal description is given of the purpose of the program and the method used.  A systematic technique is described for constructing the program proof during the process of coding it, in such a way as to prevent the intrusion of logical errors.  The proof of termination is treated as a separate exercise.  Finally, some conclusions relating to general programming methodology are drawn."}
{"DOCID": "2228", "TEXT": "Comments on Prevention of System Deadlocks: Habermann's method of deadlock prevention is discussed, where deadlock is defined as a system state from which resource allocations to certain processes are not possible.  It is shown that the scheduler may introduce \"artificial\" deadlocks which Habermann's method does not prevent.  Permanent blocking is the situation where certain processes never receive their resource requests.  It is shown that deadlock prevention does not necessarily eliminate permanent blocking. A method of preventing permanent blocking is given."}
{"DOCID": "2229", "TEXT": "Construction of Rational and Negative Powers of a Formal Series: Some methods are described for the generation of fractional and negative powers of any formal series, such as Poisson series or Chebyshev series.  It is shown that, with the use of the three elementary operations of addition, subtraction, and multiplication, all rational (positive and negative) powers of a series can be constructed.  There are basically two approaches: the binomial theorem and the iteration methods.  Both methods are described here, and the relationship between them is pointed out.  Some well-known classical formulas are obtained as particular cases, and it is shown how the convergence properties of these formulas can be improved with very little additional computations.  Finally, at the end of the article, some numerical experiments are described with Chebyshev series and with Fourier series."}
{"DOCID": "2230", "TEXT": "A Language for Treating Geometric Patterns in a Two-dimensional space: In this paper CADEP, a problem-oriented language for positioning geometric patterns in a two-dimensional space, is presented.  Although the language has been specifically designed for the automatic generation of integrated circuit masks, it turns out to be well suited also for such other placement problems as architecture design, urban planning, logical and block diagram representation.  The design criteria, the structure, and the specific features of CADEP are illustrated."}
{"DOCID": "2231", "TEXT": "The Reconstruction of Binary Patterns from Their Projections: Given the horizontal and vertical projections of a finite binary pattern f, can we construct the original pattern f?  In this paper we give a characterization of patterns that are reconstructable from their projection.  Three algorithms are developed to reconstruct both unambiguous and ambiguous patterns.  It is shown that an unambiguous pattern can be perfectly reconstructed in time m X n and that a pattern similar to an ambiguous pattern can also be constructed in time m X n, where m, n are the dimensions of the pattern frame."}
{"DOCID": "2232", "TEXT": "Pattern Width at a Given Angle: That the pattern feature \"width as a function of angle\" possesses several possible interpretations is demonstrated in this paper, which is a review of the width concept in pattern recognition and the geometrical concept itself.  The object of the work is to clarify how the word description can be made precise so that computer algorithms for feature extraction may be obtained; the focus is on the theoretical subject matter.  The results consist of a set-theoretic definition of width-at-angle, a theorem relating it to the pattern boundary radius vector, and descriptions of alternate widths.  All widths are calculated for an illustrative example; graphical and tabular comparisons are given.  Substantial variation in width-at-angle magnitude is found.  The principal conclusion is that the set-theoretic width-at-angle is a useful pattern feature when it can be easily computed.  Further investigation of the information contained in only part of a width function is recommended for cases where computation of width-at-angle is difficult."}
{"DOCID": "2233", "TEXT": "Signature Simulation and Certain Cryptographic Codes: Three cyphers allegedly authored by Thomas Jefferson Beale in 1822 have been the subject of intensive study for over 100 years.  Generations of cryptanalysts have expended untold man-years, thus far without success, attempting tode code them; vast armies of fortune hunters and treasure seekers have devoted Herculean labors to digging up the rolling hills of Virginia trying to locate the promised bonanza. The history of pertinent activities would fill volumes, yet serious students of cryptography have always had nagging doubts about the cyphers' authenticity. It has been alleged that the \"known solution\" to Cypher Number Two: 115, 73, 24, 818, 37, 52, 49,...(\"I have deposited in the County of Bedford about four miles from Buford's in an excavation or vault...\") with the aid of an unsanitized version of the Declaration of Independence was merely a superb, imaginative, and grandiose hoax perpetrated ages ago for whatever reasons.  Modern computer technology could obviously perform signature analyses the process of encoding itself so as to yield new clues and deeper insights into their construction.  For the benefit of the uninitiated, the encoding method used in the second cypher employs a specified document whose words are simply numbered consecutively, and first letters of these words are sought out at random to match the letters of these words are sought out at random to match the letters of the clear text or message. The sequence of numbers corresponding to these matches is then written down as the final code.  While primitive, the process has the advantage of relative security until the source document becomes known; at that moment the cypher can be decoded even by second graders.  The work now completed with the help of our UNIVAC 1108 includes numerous analytical studies of the Beale cyphers and various types of simulations. For example, we have turned the entire process of simulated encoding by various schemes over to the machine and analyzed the signatures of these synthetic codes; we have also encoded various messages by hand, using different texts and a variety of methods to obtain their signatures. These simulations provide convincing evidence that the signatures are both process and data dependent; they indicate also very strongly that Mr. Beale's cyphers are for real and that it is merely a matter of time before someone finds the correct source document and locates the right vault in the common-wealth of Virginia."}
{"DOCID": "2234", "TEXT": "Roots of Matrix Pencils (Algorithm R405)"}
{"DOCID": "2235", "TEXT": "Decision Table Translation (Algorithm R394)"}
{"DOCID": "2236", "TEXT": "Remarks on Characteristic Values and Associated Solutions of Mathieus Differential Equation, Exponential Integral, and Systems of Hyperbolic P.D.E. (Algorithms R352, R385, R392)"}
{"DOCID": "2237", "TEXT": "BANDSOLVE (Algorithm R195)"}
{"DOCID": "2238", "TEXT": "Least Squares Surface Fit (Algorithm R176)"}
{"DOCID": "2239", "TEXT": "Squank (Algorithm C379)"}
{"DOCID": "2240", "TEXT": "Pseudo-Random Numbers [G5] (Algorithm C266)"}
{"DOCID": "2241", "TEXT": "Product Type Three-point Gauss-Legendre-Simpson's Integration [D1] (Algorithm A439)"}
{"DOCID": "2242", "TEXT": "Product Type Two-Point Gauss-Legendre-Simpson's Integration [D1] (Algorithm A438)"}
{"DOCID": "2243", "TEXT": "Product Type Simpson's Integration [D1] (Algorithm A437)"}
{"DOCID": "2244", "TEXT": "Product Type Trapezoidal Integration (Algorithm A436)"}
{"DOCID": "2245", "TEXT": "Trace-Driven Modeling and Analysis of CPU Scheduling in Multiprogramming System: Microscopic level job stream data obtained in a production environment by an event-driven software probe is used to drive a model of a multiprogramming computer system.  The CPU scheduling algorithm of the model is systematically varied.  This technique, called trace-driven modeling, provides an accurate replica of a production environment for the testing of variations in the system.  At the same time alterations in scheduling methods can be easily carried out in a controlled way with cause and effects relationships being isolated.  The scheduling methods tested included the best possible and worst possible methods, the traditional methods of multiprogramming theory, round-robin, first-come-first-served, etc., and dynamic predictors.  The relative and absolute performances of these scheduling methods are given.  It is concluded that a successful CPU scheduling method must be preemptive and must prevent a given job from holding the CPU for too long a period."}
{"DOCID": "2246", "TEXT": "Levels of Language for Portable Software: An increasing amount of software is being implemented in a portable form.  A popular way of accomplishing this is to encode the software in a specially designed machine-independent language and then to map this language, often using a macro processor, into the assembly language of each desired object machine.  The design of the machine-independent language is the key factor in this operation. This paper discusses the relative merits of pitching this language at a high level or a low level, and presents some comparative results."}
{"DOCID": "2247", "TEXT": "On the Criteria To Be Used in Decomposing Systems into Modules: This paper discusses modularization as a mechanism for improving the flexibility ad comprehensibility of a system while allowing the shortening of its development time.  The effectiveness of a \"modularization\" is dependent upon the criteria used in dividing the system into modules.  A system design problem is presented and both a conventional and unconventional decomposition are described.  It is shown that the unconventional decompositions have distinct advantages for the goals outlined.  The criteria used in arriving at the decompositions are discussed.  The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases.  An alternative approach to implementation which does not have this effect is sketched."}
{"DOCID": "2248", "TEXT": "A New Method for the Solution of the Cauchy Problem for Parabolic Equations: An integral equation representation is given for parabolic partial differential equations. When the equations are defined in unbounded domains, as in the initial value (Cauchy) problem, the solution of the integral equation by the method of successive approximation has inherent advantages over other methods.  Error bounds for the methods are of order h^(3/2) and h^(7/2) (his the increment size) depending on the finite difference approximations involved."}
{"DOCID": "2249", "TEXT": "A Comparison of Multivariate Normal Generators: Three methods for generating outcomes on multivariate normal random vectors with a specified variance-covariance matrix are presented.  A comparison is made to determine which method requires the least computer execution time and memory space when utilizing the IBM 360/67.  All methods use as a basis a standard Gaussian random number generator.  Results of the comparison indicate that the method based on triangular factorization of the covariance matrix generally requires less memory space and computer time than the other two methods."}
{"DOCID": "2250", "TEXT": "Computer Methods for Sampling from the Exponential and Normal Distributions (Corrigendum)"}
{"DOCID": "2251", "TEXT": "Weighted Increment Linear Search for Scatter Tables: A new linear search for hash tables whose increment step is a function of the key being addressed is presented.  Comparisons with known methods are given, in terms of efficiency and computation complexity. In particular, the new method applies to tables of size n = 2^r.  It allows full table searching, and practically eliminates primary clustering at a very low cost."}
{"DOCID": "2252", "TEXT": "A Method for Incrementally Compiling Languages with Nested Statement Structure: A method of incremental compilation is presented which applies especially to programming languages in which statements can be nested (such as Algol and PL/I).  The method permits editing of the source language using a general purpose text editor, and incremental processing of changes without frequent recompilation of entire routines.  The essential points of the method are: (1) the syntax of the language is restricted insof ar as which constructs may occur on lines; (2) an internal data structure (called the skeleton) is maintained to represent the statement structure; (3) the recompilation is partially batched in the sense that recompilation of modified lines does not occur until the last of a set of editing commands has been received; and (4) the parsing and compilation are factored into two parts, that done on individual lines and that done globally to handle the relationships between the lines."}
{"DOCID": "2253", "TEXT": "Index Ranges for Matrix Calculi: The paper describes a scheme for symbolic manipulation of index expressions which arise as a by-product of the symbolic manipulation of expressions in the matrix calculi described by the authors in a previous paper.  This scheme attempts program optimization by transforming the original algorithm rather than the machine code.  The goal is to automatically generate code for handling the tedious address calculations necessitated by complicated data structures. The paper is therefore preoccupied with \"indexing by position.\"  The relationship of \"indexing by name\" and \"indexing by position\" is discussed."}
{"DOCID": "2254", "TEXT": "Dynamic Partitioning for Array Languages: The classical process of partitioning an array into subarrays is extended to a more useful array language operation.  Various modes of partitioning are defined for different types of arrays, so that subarrays may vary over the original array in a nearly arbitrary manner.  These definitions are motivated with several realistic examples to illustrate the value of partitioning for array languages. Of general interest is the data structure for partitioning. This consists of dynamic tree structures which are used to derive and maintain the array control information.  These are described in sufficient detail to be of value in the design of other array languages. The description presented in this paper is implemented in a new array language, OL/2, currently under development at the University of Illinois."}
{"DOCID": "2255", "TEXT": "Comments on Moorer's Music and Computer Composition"}
{"DOCID": "2256", "TEXT": "Further Comments on Dijkstra's Concurrent Programming Control Problem"}
{"DOCID": "2257", "TEXT": "A Note on Optimal Doubly-Chained Trees"}
{"DOCID": "2258", "TEXT": "Additional Results on Key-to-Address Transform Techniques: A Fundamental Performance Study on Large Existing Formatted Files"}
{"DOCID": "2259", "TEXT": "Modified Incomplete Gamma Function [S14] (Algorithm A435)"}
{"DOCID": "2260", "TEXT": "Exact Probabilities for R x C Contingency Tables [G2] (Algorithm A434)"}
{"DOCID": "2261", "TEXT": "An Approximate Method for Generating Symmetric Random Variables: A method for generating values of continuous symmetric random variables that is relatively fast, requires essentially no computer memory, and is easy to use is developed.  The method, which uses a uniform zero-one random number source, is based on the inverse function of the lambda distribution of Turkey.  Since it approximates many of the continuous theoretical distributions and empirical distributions frequently used in simulations, the method should be useful to simulation practitioners."}
{"DOCID": "2262", "TEXT": "Garbage Collection for Virtual Memory Computer Systems: In list processing there is typically a growing demand for space during program execution. This paper examines the practical implications of this growth within a virtual memory computer system, proposes two new garbage collection techniques for virtual memory systems, and compares them with traditional methods by discussion and by simulation."}
{"DOCID": "2263", "TEXT": "The Conversion of Limited-Entry Decision Tables to Optimal and Near-Optimal Flowcharts: Two New Algorithms: Two new algorithms for deriving optimal and near-optimal flowcharts from limited entry decision tables are presented.  Both take into account rule frequencies and the time needed to test conditions. One of the algorithms, called the optimum-finding algorithm, leads to a flowchart which truly minimizes execution time for a decision table in which simple rules are already contracted to complex rules.  The other one, called the optimum-approaching algorithm, requires many fewer calculations but does not necessarily produce the optimum flowchart.  The algorithms are first derived for treating decision tables not containing an ELSE-rule, but the optimum-approaching algorithm is shown to be equally valid for tables including such a rule.  Both algorithms are compared with existing ones and are applied to a somewhat large decision table derived from a real case.  From this comparison two conclusions are drawn.  (1) The optimum-approaching algorithm will usually lead to better results than comparable existing ones and will not require more, but usually less, computation time.(2) In general, the greater computation effort needed for applying the optimum-finding algorithm will not be justified by the small reduction in execution time obtained."}
{"DOCID": "2264", "TEXT": "Derived Semantics for Some Programming Language Constructs: The constructs of a simple programming language are introduced and described informally in terms of values and side-effects.  A translator is defined which translates the language into flowcharts for a simple machine.  The action of the machine in executing a flowchart is defined.  A proof is constructed that the effect of translating and executing any program can be expressed solely in terms of the value and side-effect of the program.  During the course of constructing the proof, formal definitions of the concepts of value and side-effect are derived in order to make the proof rigorous.  Correctness of the implementation involves checking that the definitions derived in the step above are an acceptable formalization of the informal description given in the first step."}
{"DOCID": "2265", "TEXT": "A Model for Type Checking: Most current programming languages treat computation over different classes of objects (e.g. numbers, strings, labels and functions).  For correct compilation and execution, the following question then arises: is a program properly constructed so that its operations and operands are compatible?  The activity of answering this question is usually called type checking.  This paper attempts to isolate the notion of type checking and presents a partial solution to the type checking problem based on the notions of abstraction and application of functions. In particular, a program is mapped into an expression within a decidable subset of the Lambda calculus, which characterizes the type relations within the program and eliminates all other information.  The determination of the type-wise correctness or incorrectness of the program is resolved by reducing its corresponding Lambda calculus expression to one of two normal forms, the constant \"correct\" for a type-wise correct program or the constant \"error\".  An application to type checking in Algol 60 is made, and the attendant problems faced for any notion of type checking are discussed."}
{"DOCID": "2266", "TEXT": "A Highly Parallel Algorithm for Approximating All Zeros of a Polynomial with Only Real Zeros: An algorithm is described based on Newton's method which simultaneously approximates all zeros of a polynomial with only real zeros.  The algorithm, which is conceptually suitable for parallel computation, determines its own starting values so that convergence to the zeros is guaranteed.  Multiple zeros and their multiplicity are readily determined.  At no point in the method is polynomial deflation used."}
{"DOCID": "2267", "TEXT": "Algorithms To Reveal Properties of Floating-Point Arithmetic: Two algorithms are presented in the form of Fortran subroutines.  Each subroutine computes the radix and number of digits of the floating-point numbers and whether rounding or chopping is done by the machine on which it is run.  The methods are shown to work on any \"reasonable\" floating-point computer."}
{"DOCID": "2268", "TEXT": "A Comparative Study of Computer Programs for Integrating Differential Equations: A study comparing the performance of several computer programs for integrating systems of ordinary differential equations is reported.  The integration methods represented include multistep methods (predictor-correctors), single-step methods (Runge-Kutta) and extrapolation methods (both polynomial and rational).  The testing procedure is described together with the evaluation criteria applied.  A set of test problems on which the programs were tested is included in an appendix. For the particular problems and criteria used in the investigation it was found that a program based on rational extrapolation showed the best performance."}
{"DOCID": "2269", "TEXT": "Tableless Date Conversion (Algorithm R398)"}
{"DOCID": "2270", "TEXT": "Interpolation and Smooth Curve Fitting Based on Local Procedures [E2] (Algorithm A433)"}
{"DOCID": "2271", "TEXT": "Aesthetics and the Human Factor in Programming (Corrigendum)"}
{"DOCID": "2272", "TEXT": "Sorting by Natural Selection: A family of sorting algorithms is proposed, the members of which make fuller use of the memory space and thus yield longer sorted strings.  Extensive simulation results are presented, and various implications and further applications are discussed."}
{"DOCID": "2273", "TEXT": "Conversion of Decision Tables By Rule Mask Method Without Rule Mask: Two algorithms for generating computer programs from decision tables are described.  The algorithms allow handling limited entry, extended entry, and mixed entry tables.  The algorithms are based on the rule mask method but need not have the masks at execution time.  They perform the logical operations immediately rather than at the end of the interpreting process.  Execution time can be considerably reduced by instantly marking rules which are not applicable (Algorithms 1 and 2) or conditions which are already tested (Algorithm 2).  The new algorithms combine to a certain degree the advantages of mask methods with those of tree methods."}
{"DOCID": "2274", "TEXT": "Generating English Discourse from Semantic Networks: A system is described for generating English sentences from a form of semantic nets in which the nodes are word-sense meanings and the paths are primarily deep case relations.  The grammar used by the system is in the form of a network that imposes an ordering on a set of syntactic transformations that are expressed as LISP functions.  The generation algorithm uses the information in the semantic network to select appropriate generation paths through the grammar.  The system is designed for use as a computational tool that allows a linguist to develop and study methods for generating surface strings from an underlying semantic structure.  Initial finding with regard to form determiners such as voice, form, tense, and mood, some rules for embedding sentences, and some attention to pronominal substitution are reported.  The system is programmed in LISP 1.5 and is available from the authors."}
{"DOCID": "2275", "TEXT": "Integral Equations of Immunology: The inversion of a particular integral equation of the first (Fredholm) kind is the basic problem considered.  The strategy which yielded success consisted of three essential points: (1) fit the known experimental data by a curve with properties which derive from properties of the (as yet unknown) function; (2) stabilize the computation for the unknown function by using singular value decomposition; (3) constrain the unknown function approximation (since it represents a probability distribution) to be nonnegative. A number of test cases are presented. One set of actual experimental data is analyzed with the procedures presented."}
{"DOCID": "2276", "TEXT": "Computer Methods for Sampling from the Exponential and Normal Distributions: Various methods are known for transforming uniformly distributed random numbers into exponentially and normally distributed quantities.  The most efficient ones are compared, in terms of memory requirements and speed, with some new algorithms.  A number of procedures convert Taylor series expansions directly into sampling steps, an approach which may be used for sampling from any continuous distribution.  For the exponential distribution a definite recommendation can be made, whereas in the case of the normal distribution there remains a choice between slower and shorter algorithms and faster but space consuming methods."}
{"DOCID": "2277", "TEXT": "Demand Paging Through Utilization of Working Sets on the MANIAC II: A hardware implementation on the Maniac II computer of the working set model for demand paging, as introduced by Denning, is discussed.  Characteristics of the Maniac II are given, along with a description of the basic demand paging scheme and the associate memory which has been added to the Maniac II hardware. Finally, a description of the hardware design for implementation of the working set model is discussed and a specification of the actions taken under various conditions which may arise during the operation of the full working set model, demand paging system is given."}
{"DOCID": "2278", "TEXT": "On Foster's Information Storage and Retrieval Using AVL Trees"}
{"DOCID": "2279", "TEXT": "A Controller for a Braille Terminal"}
{"DOCID": "2280", "TEXT": "Comment on Deadlock Prevention Method"}
{"DOCID": "2281", "TEXT": "The Eigen problem of Block Tridiagonal Matrices"}
{"DOCID": "2282", "TEXT": "A Comparison of Floating Point Summation Methods"}
{"DOCID": "2283", "TEXT": "Thinning Algorithms on Rectangular, Hexagonal, and Triangular Arrays: In this report three thinning algorithms are developed: one each for use with rectangular, hexagonal, and triangular arrays.  The approach to the development of each algorithm is the same.  Pictorial results produced by each of the algorithms are presented and the relative performances of the algorithms are compared.  It is found that the algorithm operating with the triangular array is the most sensitive to image irregularities and noise, yet it will yield a thinned image with an overall reduced number of points.  It is concluded that the algorithm operating in conjunction with the hexagonal array has features which strike a balance between those of the other two arrays."}
{"DOCID": "2284", "TEXT": "Solution of the Matrix Equation AX+XB=C [F4] (Algorithm A432)"}
{"DOCID": "2285", "TEXT": "Computer Routine for Quadratic and Linear Programming Problems [H] (Algorithm A431): A computer program based on Lemke's complementary pivot algorithm is presented.  This can be used to solve linear and quadratic programming problems. The program has been extensively tested on a wide range of problems and the results have been extremely satisfactory."}
{"DOCID": "2286", "TEXT": "Automatic Error Analysis for Determining Precision: The problem considered is that of evaluating a rational expression to within any desired tolerance on a computer which performs variable-precision floating-point arithmetic operations.    An automatic error analysis technique is given for determining, directly from the results of a trial low-precision interval arithmetic calculation, just how much precision and data accuracy are required to achieve a desired final accuracy.  The technique given generalize easily to the evaluation of many nonrational expressions."}
{"DOCID": "2287", "TEXT": "A New Approach to Automatic Scanning of Contour Maps: The problem of automatic digitizing of contour maps is discussed.  The structure of a general contour map is analyzed, and its topological properties are utilized in developing a new scanning algorithm. The problem of detection and recognition of contour lines is solved by a two color labeling method. It is shown that for maps containing normal contour lines only, it suffices to distinguish between so-called \"even\" and \"odd\" lines.  The \"tangency problem\" involved in practical scanning is discussed, and a solution based on minimizing computer memory space and simplifying control program is suggested."}
{"DOCID": "2288", "TEXT": "File Organization: The Consecutive Retrieval Property: The consecutive retrieval property is an important relation between a query set and record set.  Its existence enables the design of an information retrieval system with a minimal search time and no redundant storage.  Some important theorems on the consecutive retrieval property are proved in this paper.  Conditions under which the consecutive retrieval property exists and remain invariant have been established.  An outline for designing an information retrieval system based on the consecutive retrieval property is also discussed."}
{"DOCID": "2289", "TEXT": "Cellular Arrays for the Solution of Graph Problems: A cellular array is a two-dimensional, checkerboard type interconnection of identical modules (or cells), where each cell contains a few bits of memory and a small amount of combinational logic, and communicates mainly with its immediate neighbors in the array.  The chief computational advantage offered by cellular arrays is the improvement in speed achieved by virtue of the possibilities for parallel processing.  In this paper it is shown that cellular arrays are inherently well suited for the solution of many graph problems.  For example, the adjacency matrix of a graph is easily mapped onto an array; each matrix element is stored in one cell of the array, and typical row and column operations are readily implemented by simple cell logic.  A major challenge in the effective use of cellular arrays for the solution of graph problems is the determination of algorithms that exploit the possibilities for parallelism, especially for problems whose solutions appear to be inherently serial.  In particular, several parallelized algorithms are presented for the solution of certain spanning tree, distance, and path problems, with direct applications to wire routing, PERT chart analysis, and the analysis of many types of networks. These algorithms exhibit a computation time that in many cases grows at a rate not exceeding log2 n, where n is the number of nodes in the graph.  Straightforward cellular implementations of the well-known serial algorithms for these problems require about n steps, and noncellular implementations require from n^2 to n^3 steps."}
{"DOCID": "2290", "TEXT": "Immediate Predominators in a Directed Graph [H] (Algorithm A430)"}
{"DOCID": "2291", "TEXT": "Localization of the Roots of a Polynomial [C2] (Algorithm A429)"}
{"DOCID": "2292", "TEXT": "A Note on the Generation of Rosary Permutations"}
{"DOCID": "2293", "TEXT": "Comment on Average Binary Search Length"}
{"DOCID": "2294", "TEXT": "A Bonus from van Wijngaarden's Device"}
{"DOCID": "2295", "TEXT": "Comment on the Composition of Semantics in Algol 68"}
{"DOCID": "2296", "TEXT": "Compiling Fixed-Point Multiplications"}
{"DOCID": "2297", "TEXT": "A Model of Memory Contention in a Paging Machine: This paper is concerned with certain aspects of contention for main memory resources in a multiprogrammed computer system operating under demand paging.  In the model presented, the number of page-frames of main memory allocated to a problem program varies in time.  These changes in memory configuration are represented explicitly in the model, CPU requirements and page exception characteristics of program material being described statistically.  Expressions for the distribution of the number of page-frames allocated to an executing program, the long run expected fraction of a program's execution time in a given number of page-frames, and the average execution interval of the multiprogrammed load are obtained.  It is pointed out heuristically and demonstrated numerically that an increase is obtain able in the average execution interval of the multiprogrammed load over that resulting from equal fixed partitioning of main memory."}
{"DOCID": "2298", "TEXT": "An Environment for Research in Microprogramming and Emulation: The development of the research project in microprogramming and emulation at State University of New York at Buffalo consisted of three phases: the evaluation of various possible machines to support this research; the decision to purchase one such machine, which appears to be superior to the others considered; and the organization and definition of goals for each group in the project.  Each of these phases is reported, with emphasis placed on the early results achieved in this research."}
{"DOCID": "2299", "TEXT": "An Extensible Editor for a Small Machine with Disk Storage: A design philosophy for developing a sophisticated utility program is illustrated by the actual design and implementation of a text editor.  A versatile data structure is employed so that only a small number of programmed subroutines are necessary for all types of data manipulation.  Such a data structure is described, and its merits are illustrated by the ease with which powerful extensions can be implemented in terms of a few basic editing function."}
{"DOCID": "2300", "TEXT": "Political Redistricting by Computer: The problems of political redistricting are considered and a computer method for redistricting is presented.  Criteria for acceptable redistricting are discussed, including population equality, compactness, contiguity, and preservation of natural and/or political boundaries.  Only nonpartisan criteria are considered. Using 1970 Bureau of Census population data, specific results are given for the ten Congressional Districts in the state of Missouri and for the seven St. Louis County Council seats.  Results from the use of the algorithm indicate the feasibility of political redistricting with the aid of a computer."}
{"DOCID": "2301", "TEXT": "Generating Parsers for Affix Grammars: Affix grammars are two-level grammars which are similar to van Wijngaarden's two-level grammars used in the definition of Algol 68.  Affix grammars are shown by Koster to be equal in power to van Wijngaarden grammars.  They are much more suited to parsing than are the latter, however.  Koster, the inventor of affix based on recursive procedures.  This paper presents a bottom-up scheme for parsing them, based on an extension of Floyd Production Language (FPL).  Included is an algorithm, similar to that of DeRemer's, for converting a large class of affix grammars into FPL. The paper concludes by discussing briefly the applicabilities of the conversion algorithm and affix grammars in general, and some possible extensions to Koster's definition of affix grammars."}
{"DOCID": "2302", "TEXT": "Computers and Employment: The relationship of computers and automation to employment is part of the more general relation of technological change to employment.  The most obvious effect is that increase in productivity due to technology can eliminate jobs.  Technology affects the individual worker, in the nature and amount of his work, and in his attitudes toward that work.  Technological change affects the occupational structure of the entire labor force.  Because of the central importance of these effects, the impact of technology has been the subject of extensive study by economists, sociologists, political scientists, and psychologists. Even within a single discipline, studies are often contradictory, and conclusions are colored by political overtones.  We wish to delineate some of the issues, and present arguments given to support different viewpoints."}
{"DOCID": "2303", "TEXT": "Archaeology of Computers - Reminiscences, 1945-1947: The period preceding the founding of ACM was dominated by the first large computer ENIAC. Its characteristics, described here, foreshadow later developments."}
{"DOCID": "2304", "TEXT": "A Western View of Computer History: Many U.S. histories of the digital computer field have tended to be impersonal, with heavy emphasis on eastern universities and commercial developments. This article records the events of the early years in a personal way.  The people, organizations, technologies, and computers of the 1945-55 period in the western part of the United Statesare described as they happened."}
{"DOCID": "2305", "TEXT": "The \"Plankalkul\" of Konrad Zuse: A Forerunner of Today's Programming Languages: Plankalkul was an attempt by Korrad Zuse in the 1940's to devise a notational and conceptual system for writing what today is termed a program.  Although this early approach to a programming language did not lead to practical use, the plan is described here because it contains features that are standard in today's programming languages.  The investigation is of historical interest; also, it may provide insights that would lead to advancements in the state of the art.  Using modern programming terminology, the Plankalkul is presented to the extent it has possible to reconstruct it from the published literature."}
{"DOCID": "2306", "TEXT": "Ancient Babylonian Algorithms: The early origins of mathematics are discussed, emphasizing those aspects which seem to be of greatest interest from the standpoint of computer science.  A number of old Babylonian tablets, many of which have never before been translated into English, are quoted."}
{"DOCID": "2307", "TEXT": "Dynamic Document Processing: The current role of computers in automatic document processing is briefly outlined, and some reasons are given why the early promise of library automation and of the mechanization of documentation processes has not been fulfilled.  A new dynamic document environment is then outlined in which clustered files are searched and information is retrieved following an interactive user-controlled search process. Methods are described for an automatic query modification based on user needs, and for a continuous reorganization of the stored information as a function of earlier file processing and of normal collection growth.  The proposed procedures provide powerful tools for information retrieval and for the control of dynamic library collections in which new items are continually added and old ones are retired."}
{"DOCID": "2308", "TEXT": "Computers and Urban Society: This brief survey of the use of computers in urban society covers the broad range of activities found in any city.  The future scope of applications is limited only by the imagination and inventiveness of future system designers, programmers, analysts, and decision makers.  The computer can be, if properly used, with respect for human dignity and civil liberty, a significant factor in improving the efficiency of the urban process.  It is expected that the benefits of such computer usage will outweigh the costs and that we may look forward to an expansion of such usage."}
{"DOCID": "2309", "TEXT": "Computers in the Instructional Process: Directions for Research and Development: A survey is given of computer applications to the instructional process which suggests how the computer professional can contribute to effective educational systems."}
{"DOCID": "2310", "TEXT": "Language Analysis in the Humanities: The use of the computer in the language-oriented humanities for exhaustive listing of detail (as in indices and concordances) is widespread and accepted as desirable.  The implications of the computer for a \"science\" of the humanities-a science entailing gathering data for the construction and testing of models-are neither widely recognized nor accepted. This paper argues that the computer's  major role as to language analysis in the humanities will be the establishing of such a science.Thus, for those areas of the humanities for which rigor and precision are necessary (e.g. analyzing literature or teaching a student to write a composition) the computer can be a critically important facilitator."}
{"DOCID": "2311", "TEXT": "A Generational Perspective of Information System Development: System development is categorized from a generational point of view that parallels the commonly described computing system generations.  For each generation, the scope of development projects and the technological world view of the system developer are examined."}
{"DOCID": "2312", "TEXT": "On the Present and Future of Scientific Computation: A pessimistic forecast is given of what can be expected to happen in the application of computers to the physical sciences."}
{"DOCID": "2313", "TEXT": "The Evolution of Storage Structures: Data base management systems have grown rapidly in their power and complexity over the 15-year history of data processing on commercially available computers.  The original concepts have split, and new terms have been adopted to name and refer to these concepts.  The Data Structure Diagram graphic technique is used to illustrate the splitting of the concepts and the structural relations which exist between these concepts at each point in the evolution."}
{"DOCID": "2314", "TEXT": "Requirements for Advanced Programming Systems for List Processing: List processing systems should be designed to facilitate production of large programs to manipulate large complex symbolic data stores.  This paper presents an overview of a number of system features which the author feels are important to improve the productivity of programmers working in such domains.  A system view it taken, rather than focusing just on language features, since algorithms must be not only coded in a language form, but debugged, modified, made efficient, and run on data.  Because of this general framework,the requirements specified are applicable to the design of advanced programming systems for a wide range of applications.  Three aspects of programming systems are highlighted: good interactive facilities, programmable control structures, and sophisticated data communication mechanisms.  Interactive features are described to facilitate program composition, entry, testing, debugging, editing, optimization, and packaging.  Implementation of a generalized environment structure model specified would allow programming of various control regimes including multiprocesses, coroutines and backtracking.  Alternative methods of procedure invocation required include invocation by pattern and by monitoring condition.  The  need for extended data forms, storage management, and extensibility are stressed, as is the duality of data retrieval and function evaluation.  Syntax directed input and output of data would facilitate use of complex data stores."}
{"DOCID": "2315", "TEXT": "The Production of Better Mathematical Software: Some observations are made on steps to be taken toward the creation of better mathematical software.  These steps suggest the need for a coordinated effort and the creation of a center to focus activities in this area."}
{"DOCID": "2316", "TEXT": "Programming Languages: History and Future: This paper discusses both the history and future of programming languages (= higher level languages). Some of the difficulties in writing such a history are indicated.  A key part of the paper is a tree showing the chronological development of languages and their interrelationships.  Reasons for the proliferation of languages are given.  The major languages are listed with the reasons for their importance.  A section on chronology indicates the happenings of the significant previous time periods and the major topics of 1972.  Key concepts other than specific languages are discussed."}
{"DOCID": "2317", "TEXT": "Programming Systems and Languages 1965-1975: In spite of impressive gains by PL/I, Fortran and Cobol remain the languages in which most of the world's production programs are written and will remain so into the foreseeable future.  There is a great deal of theoretical interest in Algol 68 and in extensible languages, but so far at least they have had little practical impact.  Problem-oriented languages may very well become the most important language development area in the next five to ten years. In the operating system area all major computer manufacturers set out to produce very ambitious multiprogramming systems, and they all ran into similar problems.  A number of university projects,though not directly comparable to those of the manufacturers, have contributed greatly to a better understanding of operating system principles.  Important trends include the increased interest in the development of system measurement and evaluation techniques,and increased use of microprogramming for some programming system functions."}
{"DOCID": "2318", "TEXT": "The Role of Computer System Models in Performance Evaluation: Models constitute a useful means of investigating computer system performance.  This paper examines the interrelationships between models and other methods for evaluating the performance of computer systems and establishes circumstances under which the use of a model is appropriate."}
{"DOCID": "2319", "TEXT": "Operating System Performance: An overview of the current and future positions with respect to operating system performance is given.  While a great deal of information and a large number of models for subsystems have been developed, gaps still exist in out knowledge.  Because of the severe interactions between the various subsystems of an operating system, an overall model of the total system must be developed to be able to analyze and design the performance aspects of an operating system although such total system designs are exceptional today, it is projected that they will become increasingly more common and necessary in the near future. Such a design philosophy will clearly have a severe impact on the way we go about modularizing operating and computer systems."}
{"DOCID": "2320", "TEXT": "Structured Multiprogramming: This paper presents a proposal for structured representation of multiprogramming in a high level language.  The notation used explicitly associates a data structure shared by concurrent processes with operations defined on it.  This clarifies the meaning of programs and permits a large class of time-dependent errors to be caught at compile time.  A combination of critical regions and event variables enables the programmer to control scheduling of resources among competing processes to any degree desired.  These concepts are sufficiently safe to use not only within operating systems but also within user programs."}
{"DOCID": "2321", "TEXT": "On the Interface Between Computers and Data Communications Systems: Future systems that combine computers, digital terminals, and communications equipment present design optimization problems that require reconsideration of the traditional functional responsibilities of the respective subsystems.  Several \"standard\" interfaces, by means of which computers and digital terminals connect to the communications systems will be required.  When specifying these interfaces, consideration must be given to problems of coordination, synchronization, error control, signaling, stream multiplexing, and switch control, in addition to minimizing the technological interdependence of specific subsystem designs.  A focus on some of the problems is obtained in a discussion of a detailed specification for a particular computer-communications system interface."}
{"DOCID": "2322", "TEXT": "A View of computer Architecture: An attempt is made to predict the developments of the next 25 years in the field of computer architecture.  Standardized, inexpensive microcomputers on a single chip are predicted.  These will be used extensively to provide logical functions for noncomputational devices and incidentally for the design of superscale computers."}
{"DOCID": "2323", "TEXT": "Toward a General Theory of Special Functions: A list of a number of natural developments for the field of algebraic manipulation is given. Then the prospects for a general theory of functions defined by ordinary differential equations are discussed.  The claim is made that recent developments in mathematics indicate that it should be possible to algorithmically generate many properties of solutions to differential equations.  Such a theory is preferable to a less general effort to make algebraic manipulation systems knowledgeable about the usual special functions (e.g. exponential, hypergeometric)."}
{"DOCID": "2324", "TEXT": "Management Science: A View from Nonlinear Programming: A brief history of integer and continuous nonlinear programming is presented as well as the current obstacles to practical use of these mathematical programming techniques.  It is forecast that the useful contributions to nonlinear programming actually made in the next few years are more likely to be consolidations than theoretical breakthroughs.  These contributions are likely to be the documentation of standard test problems, construction of user oriented software, and comparisons of currently known algorithms to demonstrate which techniques are best for specific problems."}
{"DOCID": "2325", "TEXT": "Numerical Mathematics and Computer Science: Numerical mathematics is viewed as the analysis of continuous algorithms.  Four of the components of numerical mathematics are discussed.  These are: foundations (finite precision number systems, computational complexity), synthesis and analysis of algorithms, analysis of error, programs and program libraries."}
{"DOCID": "2326", "TEXT": "Fix point Approach to the Theory of Computation: Following the fix point theory of Scott, the semantics of computer programs are defined in terms of the least fix points of recursive programs.  This allows not only the justification of all existing verification techniques, but also their extension to the handling, in a uniform manner of various properties of computer programs, including correctness, termination, and equivalence."}
{"DOCID": "2327", "TEXT": "Toward an Automata Theory of Brains: A source of ideas for automata theory-the study of the brain-has been pushed aside in mathematical development of the theory.  This paper suggests the ways in which automata theory might evolve over the next 25 years if it is to contribute to an understanding of how the brain processes information."}
{"DOCID": "2328", "TEXT": "Individualizing Instruction in a Generative CAI Tutor"}
{"DOCID": "2329", "TEXT": "Computer Science-A Vicious Circle"}
{"DOCID": "2330", "TEXT": "Calculation of Fourier Integrals (Algorithm R418)"}
{"DOCID": "2331", "TEXT": "An Integer Programming Problem (Algorithm R397)"}
{"DOCID": "2332", "TEXT": "Special Series Summation with Arbitrary Precision (Algorithm R393)"}
{"DOCID": "2333", "TEXT": "Random Vectors Uniform is Solid Angle (Algorithm R381)"}
{"DOCID": "2334", "TEXT": "General Random Number Generator (Algorithm R370)"}
{"DOCID": "2335", "TEXT": "Eigenvalues and Eigenvectors of a Real General matrix (Algorithm R343)"}
{"DOCID": "2336", "TEXT": "Complex Error Function (Algorithm C363)"}
{"DOCID": "2337", "TEXT": "A Sorting Problem and Its Complexity: A technique for proving min-max norms of sorting algorithms is given.  One new algorithm for finding the minimum and maximum elements of a set with fewest comparisons is proved optimal with this technique."}
{"DOCID": "2338", "TEXT": "A Starting Method for Solving Nonlinear Volterra Integral Equations of the Second Kind: A fourth-order starting method is given for Volterra integral equations of the second kind and numerical examples are presented."}
{"DOCID": "2339", "TEXT": "Computer-Assigned Codes from Verbal Responses: It is often desirable to convert verbal responses to multidigit codes. This conversion is generally accomplished by clerk-coders.  A study was conducted to test the feasibility of translating verbal descriptions to numerical codes in a computer program.  Primary emphasis was placed on computerized construction of a reference file of verbal descriptions for use by the program.  The results of the study clearly show that such procedures are feasible."}
{"DOCID": "2340", "TEXT": "A Boolean Matrix Method for the Computation of Linear Precedence Functions: A modified version of Bell's Boolean matrix method for the computation of linear precedence functions associated with a conflict-free matrix of precedence relations is given.  This algorithm not only detects when the precedence functions do not  exist, but also provides an indication of why they do not exist, so that corrective action can be taken if possible.  Necessary and sufficient conditions for the existence of precedence functions are given. The use of Boolean matrices to prove the existence of precedence functions associated with classes of conflict-free grammars is illustrated through an example."}
{"DOCID": "2341", "TEXT": "Blocks-A New Data type for SNOBOL4: A new data type, called a block, has been implemented for SNOBOL4.  A block is a three-dimensional aggregate of characters in the form of a right parallelepiped, best thought of as a three-dimensional extension to a string.  (The third dimension is used for overstriking.)  Blocks may be printed, concatenated in any of three dimensions, and merged on the basis of program-defined connection points.  Some blocks adapt in size and shape to their environment.  Blocks and their operations are mainly used for composing printable output.  A variety of graphical problems (including flowcharting, bargraphs, logic diagrams, mathematical-equation formation, and text justification and preparation) have been programmed on a printer in what appears to be an easy and natural way.  In addition to these somewhat specialized applications, blocks appear to be a good general purpose device-independent output formation mechanism especially suitable for nonnumerical work.  The concept of a block is largely language independent.That is, blocks require little in the way of specialized syntax and could readily be absorbed into the external structure of most programming languages."}
{"DOCID": "2342", "TEXT": "Interference Between Communicating Parallel Processes: Various kinds of interference between communicating parallel processes have been examined by Dijkstra, Knuth, and others.  Solutions have been given for the mutual exclusion problem and associated subproblems, in the form of parallel programs, and informal proofs of correctness have been given for these solutions.  In this paper a system of parallel processes is regarded as a machine which proceeds from one state S (i.e. a collection of pertinent data values and process configurations) to a next state S' in accordance with a transition rule S --> S'.  A set of such rules yields sequences of states, which dictate the system's behavior.  The mutual exclusion problem and the associated subproblems are formulated as questions of inclusion between sets of states, or of the existence of certain sequences.  A mechanical proof procedure is shown, which will either verify (prove the correctness of ) or discredit (prove the incorrectness of) an attempted solution, with respect to any of the interference properties.  It is shown how to calculate transition rules from the \"partial rules\" by which the individual processes operate. The formation of partial rules and the calculation of transition rules are both applicable to hardware processes as well as to software processes, and symmetry between processes is not required."}
{"DOCID": "2343", "TEXT": "A Proposal To Establish a Pseudo Virtual Memory via Writable Overlays: Many computer systems solve executable storage size problems for large programs by using overlays. However, it appears that no one overlay scheme contains a well-balanced combination of the most useful capabilities which are found in various existing techniques. A proposal is presented which utilizes several of the best capabilities from existing schemes and is complemented by several additional features, e.g. writable overlays.  The writable overlay capability provides a virtual memory effect, although the programmer may still be required to design the overlay configuration.  Since overlay structuring is a complex task, several tools (including a graphic display) are included in the proposal in order to aid the programmer in the design.  The content of overlays is briefly discussed, and it is noted that many of the details of the final overlay configuration may be decided after the fact."}
{"DOCID": "2344", "TEXT": "On the Optimization of Performance of Time-Sharing Systems by Simulation: A simulation model of a time-sharing system with a finite noncontiguous store and an infinite auxiliary store is used to study the variation of system parameters such as store size, number of jobs allowed to execute simultaneously, job-scheduling algorithm, etc.  The effects of these variations on a measure of system performance is used to ascertain which of the parameters controllable by the job-scheduling algorithm, including the scheduling itself, require optimization, and which of the parameters not normally controllable by the scheduling algorithm have a marked effect on system performance.  System performance is based upon the mean cost of delay to all jobs processed. It is shown that significant improvements in the measure of system performance can be obtained by using variable time-slice techniques and by selecting the optimum round-robin cycle time.  It appears that these features would benefit from optimization whereas other parameters controllable by the scheduling algorithm affect system performance in a predictable manner and would not benefit from optimization.  Features not normally under the control of the scheduling algorithm can also have a marked effect on the measure of performance; in particular, supervisor overheads, the size of the store, and the speed of the CPU.  A comparison is made between the results of the simulation model and two analytical equations for quantum-oriented nonpreemptive time-sharing systems.  The comparison is found to be very favorable."}
{"DOCID": "2345", "TEXT": "Curriculum Recommendations for Graduate Professional Programs in Information Systems: The need for education related to information systems in organizations is discussed, and a curriculum is proposed for graduate professional programs in universities, at the Master's level.  Material necessary for such programs is identified, and courses incorporating it are specified.  Detailed course descriptions are presented, program organization discussed, and implementation questions considered."}
{"DOCID": "2346", "TEXT": "Hu-Tucker Minimum Redundancy Alphabetic Coding Method [Z] (Algorithm A428)"}
{"DOCID": "2347", "TEXT": "Fourier Cosine Integral [D1] (Algorithm A427)"}
{"DOCID": "2348", "TEXT": "Merge Sort Algorithm [M1] (Algorithm A426)"}
{"DOCID": "2349", "TEXT": "Generation of Random Correlated Normal Variables [G5] (Algorithm A425)"}
{"DOCID": "2350", "TEXT": "Clenshaw-Curtis Quadrature [D1] (Algorithm A424)"}
{"DOCID": "2351", "TEXT": "The Optimality of Winograd's Formula"}
{"DOCID": "2352", "TEXT": "Minimax Nonlinear Approximation by Approximation on Subsets"}
{"DOCID": "2353", "TEXT": "Fast Finite-Difference Solution of Biharmonic Problems: Setting the Reynolds number equal to zero, in a method for solving the Navier-Strokes equations numerically, results in a fast numerical method for biharmonic problems.  The equation is treated as a system of two second order equations and a simple smoothing process is essential for convergence. An application is made to a crack-type problem."}
{"DOCID": "2354", "TEXT": "Implementing Clenshaw-Curtis Quadrature, II Computing the Cosine Transformation: In a companion paper to this, \"I Methodology and Experiences,\" the automatic Clenshaw-Curtis quadrature scheme was described and how each quadrature formula used in the scheme requires a cosine transformation of the integrand values was shown. The high cost of these cosine transformations has been a serious drawback in using Clenshaw-Curtis quadrature. Two other problems related to the cosine transformation have also been trouble some.  First, the conventional computation of the cosine transformation by recurrence relation is numerically unstable, particularly at the low frequencies which have the largest effect upon the integral.  Second, in case the automatic scheme should require refinement of the sampling, storage is required to save the integrand values after the cosine transformation is computed.  This second part of the paper shows how the cosine transformation can be computed by a modification of the fast Fourier transform and all three problems overcome.  The modification is also applicable in other circumstances requiring cosine or sine transformations, such as polynomial interpolation through the Chebyshev points."}
{"DOCID": "2355", "TEXT": "Implementing Clenshaw-Curtis quadrature, I Methodology and Experience: Clenshaw-Curtis quadrature is a particularly important automatic quadrature scheme for a variety of reasons, especially the high accuracy obtained from relatively few integrand values.  However, it has received little use because it requires the computation of a cosine transformation and the arithmetic cost of this has been prohibitive.  This paper is in two parts; a companion paper, \"II Computing the Cosine Transformation,\" shows that this objection can be overcome by computing the cosine transformation by a modification of the fast Fourier transform algorithm. This first part discusses the strategy and various error estimates, and summarizes experience with a particular implementation of the scheme."}
{"DOCID": "2356", "TEXT": "A Technique for Software Module Specification with Examples: This paper presents an approach to writing specifications for parts of software systems.  The main goal is to provide specifications sufficiently precise and complete that other pieces of software can be written to interact with the piece specified without additional information.  The secondary goal is to include in the specification no more information than necessary to meet the first goal.  The technique is illustrated by means of a variety of examples from a tutorial system."}
{"DOCID": "2357", "TEXT": "MUX, a Simple Approach to On-Line Computing: An on-line system operating as part of a normal batch system for the CDC 6600 computer is described. The system, which required one man-year for initial software implementation, although basically simple, provides the necessary elements to input and modify files, submit them for batch execution, and provide results at the user's terminal.  A multiplexer designed and developed as part of the project cost one man-year for design and checkout, and $16,000 for parts and fabrication.  All aspects of the system are described, including design criteria, implementation, cost, overhead, and user reactions."}
{"DOCID": "2358", "TEXT": "The Multics Virtual Memory: Concepts and Design: As experience with use of on-line operating systems has grown, the need to share information among system users has become increasingly apparent. Many contemporary systems permit some degree of sharing.  Usually, sharing is accomplished by allowing several users to share data via input and output of information stored in files kept in secondary storage. Through the use of segmentation, however, Multics provides direct hardware addressing by user and system programs of all information, independent of its physical storage location.  Information is stored in segments each of which is potentially sharable and carries its own independent attributes of size and access privilege.  Here, the design and implementation considerations of segmentation and sharing in Multics are first discussed under the assumption that all information resides in large, segmented main memory. Since the size of main memory on contemporary systems is rather limited, it is then shown how the Multics software achieves the effect of a large segmented main memory through the use of the Honeywell 645 segmentation and paging hardware."}
{"DOCID": "2359", "TEXT": "An Improved Index Sequential Access Method Using Hashed Overflow: The Index Sequential Access Method (ISAM) is one of the most important file management systems used with moveable head disk devices.  This study investigates the use of an unconventional method of treating overflow records.  The method is to use hashing techniques to allocate space for such records. If certain conditions are satisfied, this is superior to the conventional ISAM method of chaining the overflow records via linked list techniques.  These conditions are: long overflow chains with significant overflow; lack of tight disk space constraints; record keys which are small compared to the total record size; and significant use of the file in the index as opposed to the sequential mode.  Using hashed overflow, the time to locate a record is dependent not on the total volume of overflow records as in conventional ISAM, but on the percentage use of space dedicated to overflow records."}
{"DOCID": "2360", "TEXT": "A Comment on the Double-Chained Tree"}
{"DOCID": "2361", "TEXT": "A Note on Cheney's Nonrecursive List-Compacting Algorithm"}
{"DOCID": "2362", "TEXT": "Linear Equation Solver [F4] (Algorithm A423)"}
{"DOCID": "2363", "TEXT": "Minimal Spanning Tree [H] (Algorithm A422)"}
{"DOCID": "2364", "TEXT": "Complex Gamma Function with Error Control [S14] (Algorithm A421)"}
{"DOCID": "2365", "TEXT": "Matrix Computations with Fortran and Paging: The efficiency of conventional Fortran programs for matrix computations can often be improved by reversing the order of nested loops.  Such modifications produce modest savings in many common situations and very significant savings for large problems run under an operating system which uses paging."}
{"DOCID": "2366", "TEXT": "Complex Gamma Function with Error Control: An algorithm to compute the gamma function and log gamma function of a complex variable is presented. The standard algorithm is modified in several respects to insure the continuity of the function value and to reduce accumulation of round-off errors.  In addition to computation of function values, this algorithm includes an object-time estimation of round-off errors.  Experimental data with regard to the effectiveness of this error control are presented. A Fortran program for the algorithm appears in the algorithms section of this issue."}
{"DOCID": "2367", "TEXT": "Computers and Society: A Proposed Course for Computer Scientists: The purpose of this paper is to describe a course concerned with both the effects of computers on society and the responsibilities of computer scientists to society.  The impact of computers is divided into five components: political, economic, cultural, social, and moral; the main part of the paper defines each component and presents examples of the relevant issues.  In the remaining portions the possible formats for such a course are discussed, a topic by topic outline is given, and a selected set of references is listed.  It is hoped that the proposal will make it easier to initiate courses on this subject."}
{"DOCID": "2368", "TEXT": "An Implemented Graph Algorithm for Winning Shannon Switching games: In this tutorial paper a computer program which wins Shannon Switching Games is described. Since these games are played on graphs, the program is a good example of the implementation of graph algorithms.  The two players in a Shannon Switching Game, CONNECT and CUT, have nonsimilar goals.  Either CONNECT, CUT, or the player moving first is guaranteed the existence of a winning strategy.  The simple strategy explained in this paper is valid in all three cases.  In fact, the major routines never need to know whether the computer is CONNECT or CUT."}
{"DOCID": "2369", "TEXT": "Hidden Lines Elimination for a Rotating Object: A method is presented of determining which parts of three-dimensional objects are visible and which are invisible when the objects are rotated about some axis.  This paper describes a polygon comparison scheme in which the relationships of two polygons can be classified into tree types, and also discusses how the relationship is changed for each pair of polygons under rotation about some axis.  A rotation table is defined for each pair of polygons, which remains fixed as long as rotation is about one axis and provides a means of rapidly determining the visible and hidden line relationship between two polygons. Additional work must be done to extend this approach to simultaneous rotation about several axes."}
{"DOCID": "2370", "TEXT": "An Experimental Laboratory for Pattern Recognition and Signal Processing: An interactive computer-controlled scanning and display system has been in operation at the IBM Thomas J. Watson Research Center for three years. The system includes two flying-spot scanners and a TV camera specially interfaced to a process control digital computer, dot-mode and vector displays, analog input and output facilities, and a variety of other experimental equipment.  The system design and programming support are described and typical applications in scanner control, optical character recognition,and image processing are presented."}
{"DOCID": "2371", "TEXT": "A System for Interprocess Communication in a Resource Sharing Computer Network: A system of communication between processes in a time-sharing system is described and the communication system is extended so that it may be used between processes distributed throughout a computer network. The hypothetical application of the system to an existing network is discussed."}
{"DOCID": "2372", "TEXT": "On the Implementation of Security Measures in Information Systems: The security of an information system may be represented by a model matrix whose elements are decision rules and whose row and column indices are users and data items respectively.  A set of four functions is used to access this matrix at translation and execution time.  Distinguishing between data dependent and data independent decision rules enables one to perform much of the checking of security only once at translation time rather than repeatedly at execution time.  The model is used to explain security features of several existing systems, and serves as a framework for a proposal for general security system implementation within today's languages and operating systems."}
{"DOCID": "2373", "TEXT": "Properties of the Working-Set Model: A program's working set W(t,T) at time t is the set of distinct pages among the T most recently referenced pages.  Relations between the average working-set size, the missing-page rate, and the interreference-interval distribution may be derived both from time-average definitions and from ensemble-average (statistical) definitions. An efficient algorithm for estimating these quantities is given.  The relation to LRU (least recently used) paging is characterized.  The independent-reference model, in which page references are statistically independent, is used to assess the effects to interpage dependencies on working-set size observations. Under general assumptions, working-set size is shown to be normally distributed."}
{"DOCID": "2374", "TEXT": "A Study of Storage Partitioning Using a Mathematical Model of Locality: Both fixed and dynamic storage partitioning procedures are examined for use in multiprogramming systems.  The storage requirement of programs is modeled as a stationary Gaussian process.  Experiments justifying this model are described.  By means of this model dynamic storage partitioning is shown to provide substantial increases in storage utilization and operating efficiency over fixed partitioning."}
{"DOCID": "2375", "TEXT": "A Comparative Analysis of Disk Scheduling Policies: Five well-known scheduling policies for movable head disks are compared using the performance criteria of expected seek time (system oriented)and expected waiting time (individual I/O request oriented). Both analytical and simulation results are obtained. The variance of waiting time is introduced as another meaningful measure of performance, showing possible discrimination against individual requests. Then the choice of a utility function to measure total performance including system oriented and individual request oriented measures is described.  Such a function allows one to differentiate among the scheduling policies over a wide range of input loading conditions. The selection and implementation of a maximum performance two-policy algorithm are discussed."}
{"DOCID": "2376", "TEXT": "Synchronization of Communicating Processes: Formalization of a well-defined synchronization mechanism can be used to prove that concurrently running processes of a system communicate correctly. This is demonstrated for a system consisting of many sending processes which deposit messages in a buffer and many receiving processes which remove messages from that buffer.  The formal description of the synchronization mechanism makes it very easy to prove that the buffer will neither overflow nor underflow, that senders and receivers will never operate on the same message frame in the buffer nor will they run into a deadlock."}
{"DOCID": "2377", "TEXT": "A Hardware Architecture for Implementing Protection Rings: Protection of computations and information is an important aspect of a computer utility.  In a system which uses segmentation as a memory addressing scheme, protection can be achieved in part by associating concentric rings of decreasing access privilege with a computation.  This paper describes hardware processor mechanisms for implementing these rings of protection.  The mechanisms for implementing these rings of protection. The mechanisms allow cross-ring calls and subsequent returns to occur without trapping to the supervisor.  Automatic hardware validation of references across ring boundaries is also performed.  Thus, a call by a user procedure to a protected subsystem (including the supervisor) is identical to a call to a companion user procedure.  The mechanisms of passing and referencing arguments are the same in both cases as well."}
{"DOCID": "2378", "TEXT": "An Operating System Based on the Concept of a Supervisory Computer: An operating system which is organized as a small supervisor and a set of independent processes are described.  The supervisor handles I/O with external devices-the file and directory system-schedules active processes and manages memory, handle errors, and provides a small set of primitive functions which it will execute for a process.  A process is able to specify a request for a complicated action on the part of the supervisor (usually a wait on the occurrence of a compound event in the system) by combining these primitives into a \"supervisory computer program.\" The part of the supervisor which executes these programs may be viewed as a software implemented \"supervisory computer.\"  The paper develops these concepts in detail, outlines the remainder of the supervisor, and discusses some of the advantages of this approach."}
{"DOCID": "2379", "TEXT": "The Design of the Venus Operating System: The Venus Operating System is an experimental multiprogramming system which supports five or six concurrent users on a small computer.  The system was produced to test the effect of machine architecture on complexity of software.  The system is defined by a combination of microprograms and software.  The microprogram defines a machine with some unusual architectural feature; the software exploits these features to define the operating system as simply as possible. In this paper the development of the system is described, with particular emphasis on the principles which guided the design."}
{"DOCID": "2380", "TEXT": "TENEX, a Paged Time Sharing System for the PDP-10: TENEX is a new time sharing system implemented on DEC PDP-10 augmented by special paging hardware developed at BBN.  This report specifies a set of goals which are important for any time sharing system. It describes how the TENEX design and implementation achieve these goals.  These include specifications for a powerful multiprocess large memory virtual machine, intimate terminal interaction, comprehensive uniform file and I/O capabilities, and clean flexible system structure.  Although the implementation described here required some compromise to achieve a system operational within six months of hardware checkout, TENEX has met its major goals and provided reliable service at several sites and through the ARPA network."}
{"DOCID": "2381", "TEXT": "Average Binary Search Length for Dense Ordered Lists (Corrigendum)"}
{"DOCID": "2382", "TEXT": "Reconstruction of Pictures from Their Projections (Corrigendum)"}
{"DOCID": "2383", "TEXT": "Music and Computer Composition: The problem discussed is that of simulating human composition of Western popular music by computer and some relevant theories of music and harmony are given. Problems with this kind of program and several schemes that are known not to work are discussed.  Several previous computer compositions are discussed, including the ILLIAC Suite.  A program to generate short melody fragments was written to simulate some of the aspects of human composition.  Five samples of its output are presented and discussed.  It was discovered that although the fragments show many of the characteristics of popular melodies, they have a strangely alien sound.  It is theorized that this is because the relevant probabilities which would discriminate against unfamiliar sequences were not used."}
{"DOCID": "2384", "TEXT": "Hidden-Line Plotting Program [J6] (Algorithm A420)"}
{"DOCID": "2385", "TEXT": "Zeros of a Complex Polynomial [C2] (Algorithm A419)"}
{"DOCID": "2386", "TEXT": "Dynamic Microprogramming: Processor Organization and Programming (Corrigendum)"}
{"DOCID": "2387", "TEXT": "Maximum Computing Power and Cost Factors in the Centralization Problem: A simple analysis of some computer-economic factors involved in comparing multimachine installations versus large single machine installations is given, and a mathematical model is derived to assist policy decisions."}
{"DOCID": "2388", "TEXT": "Optimizing Binary Trees Grown With a Sorting Algorithm: Items can be retrieved from binary trees grown with a form of the Algorithm Quicksort in an average time proportional to log n, where n is the number of items in the tree.  The binary trees grown by this algorithm sometimes have some branches longer than others; therefore, it is possible to reduce the average retrieval time by restructuring the tree to make the branches as uniform in length as possible. An algorithm to do this is presented.  The use of this algorithm is discussed, and it is compared with another which restructures the tree after each new item is added."}
{"DOCID": "2389", "TEXT": "Preliminary Report on a System for General Space Planning: A computer language and a set of programs within that language are described which allow the formulating and solving of a class of space planning problems.  The language is an extension of Algol and includes means to represent spaces and objects, to manipulate them, and to test the resulting arrangements according to a variety of constraints.  The algorithms used to solve problems expressed in this language rely on heuristic programming.  Both the language and the search algorithms are detailed."}
{"DOCID": "2390", "TEXT": "A Proposal for a Computer-Based Interactive Scientific Community: Because of the problems created by the explosion of papers in the mathematical sciences and the drawbacks that this places on research, it is suggested that a tree of all mathematical results and terminology be maintained in a multiterminal computer system. Users of the system can store in the computer an updated file of their current knowledge, and on selecting a paper to read, they can obtain from the computer the minimum subtree of theorems required to bring them from what they already know to the background knowledge which the paper assumes.  Under certain conditions, means are also provided for the contribution of useful comments by the readers of a work and for interaction between commentators and with the author. This paper describes how the system can be organized and the role required of readers, writers, and commentators."}
{"DOCID": "2391", "TEXT": "Unitary Symmetric Polynomials [Z] (Algorithm R391)"}
{"DOCID": "2392", "TEXT": "In-Situ Transposition of a Rectangular Matrix [F1] (Algorithm C380)"}
{"DOCID": "2393", "TEXT": "Calculation of Fourier Integrals [D1] (Algorithm A418)"}
{"DOCID": "2394", "TEXT": "Ordering +-f(+-f(+-f(...+-f(x)..))) When f(x) Is Positive Monotonic"}
{"DOCID": "2395", "TEXT": "Quadratic Programming for Nonlinear Regression: A quadratic programming algorithm is described for use with the magnified diagonal method of nonlinear regression with linear constraints.  The regression method is published in JACM, July 1970."}
{"DOCID": "2396", "TEXT": "MUSE: A Model To Understand Simple English: MUSE is a computer model for natural language processing, based on a semantic memory network like that of Quillian's TLC.  MUSE, from a Model to Understand Simple English, processes English sentences of unrestricted content but somewhat restricted format. The model first applies syntactic analysis to eliminate some interpretations and then employs a simplified semantic intersection procedure to find a valid interpretation of the input.  While the semantic processing is similar to TLC's, the syntactic component includes the early use of parse trees and special purpose rules.  The \"relational triple\" notation used during interpretation of input is compatible with MUSE's memory structures, allowing direct verification of familiar concepts and the addition of new ones. MUSE also has a repertoire of actions, which range from editing and reporting the contents of its own memory to an indirect form of question answering. Examples are presented to demonstrate how the model interprets text, resolves ambiguities, adds information to memory, generalizes from examples and performs various actions."}
{"DOCID": "2397", "TEXT": "Optimizing the Polyphase Sort (Corrigendum)"}
{"DOCID": "2398", "TEXT": "Teacher/Student Authored CAI Using the NEWBASIC System: The pedagogical advantages of a general purpose interactive system called NEWBASIC/CATALYST are discussed.  NEWBASIC/CATALYSTincorporates an advanced implementation of BASIC, system-level interactive features, and a general capability for extension through user oriented function attachment,  Application of this last feature to provide a flexible CAI scan capability is illustrated.  An example of interaction at the system level shows how students can mix the advantages of independent or \"solo\" mode computing with those of guided or \"dual\" mode interaction.  Preliminary experience with the system in an urban secondary school setting is discussed."}
{"DOCID": "2399", "TEXT": "A CRT Editing System: A test-editing and manipulation program is described. The program operates from low-cost cathode-ray tube entry and display stations with keyboard and 13 function buttons. Applications, potential economy of operation, and some aspects of implementation are discussed."}
{"DOCID": "2400", "TEXT": "Use of the Hough Transformation ToDetect Lines and Curves in Pictures: Hough has proposed an interesting and computationally efficient procedure for detecting lines in pictures.  This paper points out that the use of angle-radius rather than slope-intercept parameters simplifies the computation further.  It also shows how the method can be used for more general curve fitting, and gives alternative interpretations that explain the source of its efficiency."}
{"DOCID": "2401", "TEXT": "On Shrinking Binary Picture Patterns: A parallel processing algorithm for shrinking binary patterns to obtain single isolated elements, one for each pattern, is presented.  This procedure may be used for counting patterns on a matrix, and a hardware implementation of the algorithm using large scale integrated technology is envisioned.  The principal features of this method are the very small window employed (two-by-two elements), the parallel nature of the process, and the possibility of shrinking any pattern, regardless of the complexity of its configuration.  Problems regarding merging and disconnection of patterns during the process as well as the determination of the maximum number of steps necessary to obtain a single isolated element from a pattern, are reviewed and discussed.  An analogy with a neural network description, in terms of McCulloch-Pitts \"neurons\" is presented."}
{"DOCID": "2402", "TEXT": "Pictorial Pattern Recognition and the Phase Problem of X-ray Crystallography: The availability of interactive, three-dimensional, computer graphics systems coupled to powerful digital computers encourages the development of algorithms adapted to this environment.  Pictorial pattern recognition techniques make possible a number of approaches to X-ray structure determination based on molecular model building, i.e. the use of chemical information to frame \"structural hypotheses\" which can computationally be tested and refined by reference to the experimental data.  Application of standard pattern recognition algorithms is hindered by the fact that the cross-correlation between a model and the correct structure cannot be computed because of a fundamental incompleteness in the measured data. However, it is possible to compute an upper bound to such a cross-correlation.  A simple example demonstrates that this information can be the basis of a technique for structure determination that can make effective use of an interactive graphics system. Model building by cross-correlations has intrinsic advantages over usual crystallographic techniques based on the autocorrelation or Patterson function, especially for large structures.  This is significant, for crystallography of biological macromolecules hasbeen and will continue to be a field of intense interest."}
{"DOCID": "2403", "TEXT": "Procedures for Natural Spline Interpolation [E1] (Algorithm A472)"}
{"DOCID": "2404", "TEXT": "Exponential Integrals [S13] (Algorithm A471)"}
{"DOCID": "2405", "TEXT": "Linear Systems with Almost Tridiagonal Matrix [F4] (Algorithm A470)"}
{"DOCID": "2406", "TEXT": "A Data Definition and Mapping Language: A data definition language i sa declarative computer language for specifying data structures. Most data definition languages concentrate on the declaration of logical data structures with little concern for how these structures are physically realized on a computer system.  However, the need for data definition languages which describe both the logical and physical aspects of data is increasingly apparent.  Such languages will be a key systems, as well as in advanced data management systems and distributed data bases.  This paper reviews past work in the data definition language for describing both logical and physical aspects of data.  Applications of these \"generalized\" data definition languages are also discussed."}
{"DOCID": "2407", "TEXT": "Curriculum Recommendations for Undergraduate Programs in Information Systems: The need for education related to information systems in organizations is discussed, and a curriculum is proposed for an undergraduate program. Material necessary for such programs is identified, and courses incorporating it are specified.Detailed course descriptions are presented.  Program organization and a problems of implementation are discussed."}
{"DOCID": "2408", "TEXT": "Solving the Biharmonic Equation in a Square: A Direct Versus a Semidirect Method: Two methods for solving the biharmonic equation are compared.  One method is direct, using eigenvalue-eigenvector decomposition.  The other method is iterative, solving a Poisson equation directly at each iteration."}
{"DOCID": "2409", "TEXT": "An Algorithm for the Approximate Solution of Wiener-Hopf Integral Equations: An explicit approximate solution is given for an equation.  Where it is assumed that the classical Wiener-Hopf technique may be applied.  It is furthermore assumed that Fourier transforms are known explicitly. The approximate solution depends on two positive parameters."}
{"DOCID": "2410", "TEXT": "A Recurrence Scheme for Converting from One Orthogonal Expansion into Another: A generalization of a scheme of Hamming for converting a polynomial Pn(x) into a Chebyshev series is combined with a recurrence scheme of Clenshaw for summing any finite series whose terms satisfy a three-term recurrence formula."}
{"DOCID": "2411", "TEXT": "Tree-Structured Programs"}
{"DOCID": "2412", "TEXT": "Comment on Brent's Scatter Storage Algorithm"}
{"DOCID": "2413", "TEXT": "A Note on Subexpression Ordering in the Execution of Arithmetic Expressions: A counterexample to the supposed optimality of an algorithm for generating schedules for trees of tasks with unequal execution times is presented. A comparison with the \"critical path\" heuristic is discussed."}
{"DOCID": "2414", "TEXT": "Arithmetic Overa Finite Field [A1] (Algorithm A469)"}
{"DOCID": "2415", "TEXT": "Algorithm for Automatic Numerical Integration Over a Finite Interval [D1] (Algorithm A468)"}
{"DOCID": "2416", "TEXT": "Matrix Transposition in Place [F1] (Algorithm A467)"}
{"DOCID": "2417", "TEXT": "Four Combinatorial Algorithms [G6] (Algorithm A466)"}
{"DOCID": "2418", "TEXT": "Student's t Frequency [S14] (Algorithm A465)"}
{"DOCID": "2419", "TEXT": "Eigenvalues of a Real, Symmetric, Tridiagonal Matrix [F2] (Algorithm A464)"}
{"DOCID": "2420", "TEXT": "Experiments with an Automatic Theorem-Prover HavingPartial Ordering Inference Rules: Automatic theorem-provers need to be made much more efficient.  With this in mind, Slagle has shown how the axioms for partial ordering can be replaced by built-in inference rules when using a particular theorem-proving algorithm based upon hyper-resolution and paramodulation.  The new rules embody the transitivity of partial orderings and the close relationship between predicates.  A program has been developed using a modified version of these rules.  This new theorem-prover has been found to be very powerful for solving problems involving partial orderings.  This paper presents a detailed description of the program and a comprehensive account of the experiments that have been performed with it."}
{"DOCID": "2421", "TEXT": "A Scan Conversion Algorithm with Reduced Storage Requirements: Most graphics systems using a raster scan output device (CRT or hardcopy) maintain a display file in the XY or random scan format.  Scan converters, hardware or software, must be provided to translate the picture description from the XY format to the raster format.  Published scan conversion algorithms which are fast will reserve a buffer area large enough to accommodate the entire screen.  On the other hand, those which use a small buffer area are slow because they require multiple passes through the XY display file.  The scan conversion algorithm described here uses a linked list data structure to process the lines of the drawing in strips corresponding to groups of scan lines.  A relatively small primary memory buffer area is used to accumulate the binary image for a group of scan lines.  When this portion of the drawing has been plotted, the buffer is reused for the next portion.  Because of the list processing procedures used, only a single pass through the XY display file is required when generating the binary image and only a slight increase in execution time over the fully buffered core results.  Results slow that storage requirements can be reduced by more than 80 percent while causing less than a 10 percent increase in execution time."}
{"DOCID": "2422", "TEXT": "Adaptive Correction of Program Statements (Corrigendum)"}
{"DOCID": "2423", "TEXT": "A Parser-Generating System for Constructing Compressed Compilers: This paper describes a parser-generating system (PGS) currently in use on the CDC-6500 computer at Purdue University.  The PGS is a Fortran-coded compiler. In the input translation grammar, each BNF syntactic rule corresponds to a (possibly empty) \"code generator\" realizable as an assembly language, Fortran or Algol, subroutine that is called whenever that syntactic rule is applied in the parse of a program.  Typical one-pass compilers constructed by the PGS translate source programs at speeds approaching 14,000 cards per minute.  For an XPL compiler, the parser program and its tables currently occupy 288 words of 60-bit core memory of which 140 words are parsing table entries and 82 words are links to code generators."}
{"DOCID": "2424", "TEXT": "Dynamic Verification of Operating System Decisions: Dynamic verification of a decision implies that every time the decision is made there is a consistency check performed on the decision using independent hardware and software.  The dynamic verification of operating system decisions is used on the PRIME system being designed and constructed at the University of California, Berkeley.  PRIME is an experimental time-sharing which is to have the properties of continuous availability, data privacy, and cost effectiveness. The technique of dynamic verification allows the construction of an operating system which does not make certain decisions improperly even in the presence of a single hardware or software fault.  Furthermore, multiple faults lead to unreliable operation only if the faults happen to reinforce each other.  On PRIME, dynamic verification is used to ensure that one user's information cannot become available to another user gratuitously even in the presence of a single hardware or software fault.the amount of additional hardware and software required for dynamic verification can be modest."}
{"DOCID": "2425", "TEXT": "The Programmer as Navigator"}
{"DOCID": "2426", "TEXT": "Algorithms SCALE1, SCALE2, and SCALE3 for Determination of Scales on Computer Generated Plots [J6] (Algorithm A463)"}
{"DOCID": "2427", "TEXT": "Bivariate Normal Distribution [S15] (Algorithm A462)"}
{"DOCID": "2428", "TEXT": "Cubic Spline Solutions to a Class of Functional Differential Equations [D2] (Algorithm A461)"}
{"DOCID": "2429", "TEXT": "Calculation of Optimum Parameters for Alternating Direction Implicit Procedures [D3] (Algorithm A460)"}
{"DOCID": "2430", "TEXT": "The Elementary Circuits of a Graph [H] (Algorithm A459)"}
{"DOCID": "2431", "TEXT": "Discrete Linear L1 Approximation by interval Linear Programming [E2] (Algorithm A458)"}
{"DOCID": "2432", "TEXT": "Addendum to a Multiple-Precision Division Algorithm"}
{"DOCID": "2433", "TEXT": "Control Structures in Illiac IV Fortran: As part of an effort to design and implement a Fortran compiler on the ILLIAC IV, an extended Fortran, called IVTRAN, has been developed.  This language provides a means of expressing data and control structures suitable for exploiting ILLIAC IV parallelism. This paper reviews the hardware characteristics of the ILLIAC and singles out unconventional features which could be expected to influence language (and compiler) design.  The implications of these features for data layout and algorithm structure are discussed, and the conclusion is drawn that data allocation rather than code structuring is the crucial ILLIAC optimization problem.  A satisfactory method of data allocation is then presented.  Language structures to utilize this storage method and express parallel algorithms are described."}
{"DOCID": "2434", "TEXT": "Using Page Residency To Select the Working Set Parameter: Denning's method for selecting the working set parameter, which uses interreference intervals, is examined.  Several omissions in his model are noted, and new assumptions are introduced to overcome these omissions.  Using this modified model, Dening's results on page residency are rederived and reconsidered for selecting the working set parameter."}
{"DOCID": "2435", "TEXT": "A Class of Dynamic Memory Allocation Algorithms: Anew dynamic memory allocation algorithm, the Fibonacci system, is introduced.  This algorithm is similar to, but seems to have certain advantages over, the \"buddy\" system.  A generalization is mentioned which includes both of these systems as special cases."}
{"DOCID": "2436", "TEXT": "A Note on the Confinement Problem: This note explores the problem of confining a program during its execution so that it cannot transmit information to any other program except its caller.  A set of examples attempts to stake out the boundaries of the problem.  Necessary conditions for a solution are stated and informally justified."}
{"DOCID": "2437", "TEXT": "General Performance Analysis of Key-to-Address Transformation Methods Using an Abstract File Concept: This paper presents a new approach to the analysis of performance of the various key-to-address transformation methods.  In this approach the keys in a file are assumed to have been selected from the key space according to a certain probabilistic selection algorithm.  All files with the same number of keys selected from this key space will be suitably weighted in accordance with the algorithm, and the average performance of the transformation methods on these files will be used as the potential of these methods.  Using this analysis, methods with the same overall performance can be classified and key distributions partial to certain transformations can be identified. All this can be done analytically.  The approach is applied to a group of transformation methods using files whose keys are selected randomly."}
{"DOCID": "2438", "TEXT": "A Model and Stack Implementation of Multiple Environments: Many control and access environment structures require that storage for a procedure activation exist at times when control is not nested within the procedure activated.  This is straightforward to implement by dynamic storage allocation with linked blocks for each activation, but rather expensive in both time and space.  This paper presents an implementation technique using a single stack to hold procedure activation storage which allows retention of that storage for durations not necessarily tied to control flow.  The technique has the property that, in the simple case,it runs identically to the usual automatic stack allocation and deallocation procedure. Applications of this technique to multitasking, coroutines, backtracking, label-valued variables, and functional arguments are discussed.  In the initial model, a single real processor is assumed, and the implementation assumes multiple-processes coordinate by passing control explicitly to one another.  A multiprocessor implementation requires only a few changes to the basic technique, as described."}
{"DOCID": "2439", "TEXT": "Multiple Terminals Under User Program Control in a Time-Sharing Environment: User-written programs on the Dartmouth Time-Sharing system can communicate with many remote terminals simultaneously and can control the interactions between these terminals.  Such programs can be written using standard input and output instructions in any language available on the system.  This paper describes how this multiple-terminal facility was implemented without requiring any changes in the system executive or in any of the system's compilers or interpreters."}
{"DOCID": "2440", "TEXT": "Localization of the Roots of a Polynomial (Algorithm R429)"}
{"DOCID": "2441", "TEXT": "Hidden-Line Plotting Program (Algorithm R420)"}
{"DOCID": "2442", "TEXT": "A Sparse Matrix Package (Algorithm R408)"}
{"DOCID": "2443", "TEXT": "Generation of Permutations in Lexicographic Order (Algorithm R323)"}
{"DOCID": "2444", "TEXT": "Finding All Cliques of an Undirected Graph (Algorithm A457)"}
{"DOCID": "2445", "TEXT": "Routing Problem (Algorithm A456)"}
{"DOCID": "2446", "TEXT": "Analysis of Skew Representations of the Symmetric Group (Algorithm A455)"}
{"DOCID": "2447", "TEXT": "Sard Kernels for Certain Bivariate Cubatures: An error analysis for some bivariate cubatures is given.  The remainders are obtained by the use of Sard kernels.  Numerical results and computer graphs are given for some of the kernel functions."}
{"DOCID": "2448", "TEXT": "Reversible Execution"}
{"DOCID": "2449", "TEXT": "A Simple Technique for Structured Variable Lookup: A simple technique for the symbol-table lookup of structured variables based on simple automata theory is presented. The technique offers a deterministic solution to a problem which is currently handled in a nondeterministic manner in PL/I and COBOL compilers."}
{"DOCID": "2450", "TEXT": "Empirical Working Set Behavior: The working set model for program behavior has been proposed in recent years as a basis for the design of scheduling and paging algorithms.  Although the words \"working set\" are now commonly encountered in the literature dealing with resource allocation, there is a dearth of published data on program measurements, in the hope that workers in the field might find experimental evidence upon which to substantiate and base theoretical work."}
{"DOCID": "2451", "TEXT": "Design of Tree Structures for Efficient Querying: A standard information retrieval operation is to determine which records in a data collection satisfy a given query expressed in terms of data values. The process of locating the desired responses can be represented by a tree search model.  This paper poses an optimization problem in the design of such trees to serve a well-specified application. The problem is academic in the sense that ordinarily the optimal tree cannot be implemented by means of practical techniques.  On the other hand, it is potentially useful for the comparison it affords between observed performance and that of an intuitively attractive ideal search procedure.  As a practical application of such a model this paper considers the design of a novel tree search scheme based on a bit vector representation of data and shows that essentially the same algorithm can be used to design either an ideal search tree or a bit-vector tree.  An experimental study of a small formatted file illustrates the concepts."}
{"DOCID": "2452", "TEXT": "Evaluation and Selection of File Organization-A Model and System: This work first discusses the factors that affect file (data base) organization performance, an elusive subject, and then presents a methodology, a model and a programmed system to estimate primarily total storage costs and average access time of several file organizations, given a specific data base, query characterization and device-related specifications. Based on these estimates, an appropriate file structure may be selected for the specific situation. The system is a convenient tool to study file structures and to facilitate as much as possible the process of data base structure design and evaluation."}
{"DOCID": "2453", "TEXT": "Information Theory Applied to the Conversion of Decision Tables to Computer Programs: Using ideas from information theory, this paper develops a heuristic algorithm that converts a limited entry decision table to a tree structured computer program with near minimum average processing time.  The method is applicable to any limited entry decision table and does not require that actions have single rules or that the cost of testing conditions be equal.  It is thus more general than the previously published heuristic algorithms.  Compared to the optimal algorithm of Reinwald and Soland, this algorithm is easy to code and takes a much smaller translation time; it is thus felt that it is more useful in practice.  The algorithm is well suited for manual conversion of decision tables to flowcharts."}
{"DOCID": "2454", "TEXT": "Computational Algorithms for Closed Queueing Networks with Exponential Servers: Methods are presented for computing the equilibrium distribution of customers in closed queueing networks with exponential servers.  Expressions for various marginal distributions are also derived. The computational algorithms are based on two-dimensional iterative techniques which are highly efficient and quite simple to implement.  Implementation considerations such as storage allocation strategies and order of evaluation are examined in some detail."}
{"DOCID": "2455", "TEXT": "A Generalization of AVL Trees: A generalization of AVL trees is proposed in which imbalances up to (triangle shape) is a small integer.  An experiment is performed to compare these trees with standard AVL trees and with balanced trees on the basis of mean retrieval time, of amount of restructuring expected, and on the worst case of retrieval time.  It is shown that, by permitting imbalances of up to five units, the retrieval time is increased a small amount while the amount of restructuring required is decreased by a factor of ten. A few theoretical results are derived, including the correction of an earlier paper, and are duly compared with the experimental data.  Reasonably good correspondence is found."}
{"DOCID": "2456", "TEXT": "On the Capabilities of While, Repeat, and Exit Statements: A well-formed program is defined as a program in which loops and if statements are properly nested and can be entered only at their beginning.  A corresponding definition is given for a well-formed flowchart.  It is shown that a program is well formed if and only if it can be written with if, repeat, and multi-level exit statements for sequence control. It is also shown that if,while, and repeat statements with single-level exit do not suffice.  It is also shown that any flowcharts can be converted to a well-formed flowchart by node splitting.  Practical implications are discussed."}
{"DOCID": "2457", "TEXT": "Inductive Methodsfor Proving Properties of Programs: There are two main purposes in this paper: first, clarification and extension of known results about computation of recursive programs, with emphasis on the difference between the theoretical and practical approaches; second, presentation and examination of various known methods for proving properties of recursive programs.  Discussed in detail are two powerful inductive methods computational induction and structural induction, including examples of their applications."}
{"DOCID": "2458", "TEXT": "Localization of the Roots of a Polynomial (Algorithm R429)"}
{"DOCID": "2459", "TEXT": "Hu-Tucker Minimum Redundancy Alphabetic Coding Method (Algorithm R428)"}
{"DOCID": "2460", "TEXT": "Clenshaw-Curtis Quadrature (Algorithm R424)"}
{"DOCID": "2461", "TEXT": "Graph Plotter (Algorithm R412)"}
{"DOCID": "2462", "TEXT": "An Efficient Prime Number Generator (Algorithm R357)"}
{"DOCID": "2463", "TEXT": "Complex Gamma Function (Algorithm R404,C404)"}
{"DOCID": "2464", "TEXT": "The Complex Method for Constrained Optimization [E4] (Algorithm A454)"}
{"DOCID": "2465", "TEXT": "Gaussian Quadrature formulas for Bromwich's Integral [D1] (Algorithm A453)"}
{"DOCID": "2466", "TEXT": "Enumerating Combinations of m Out of n Objects [G6] (Algorithm A452)"}
{"DOCID": "2467", "TEXT": "Chi-Square quantiles [G1] (Algorithm A451)"}
{"DOCID": "2468", "TEXT": "Rosenbrock Function Minimization [E4] (Algorithm A450)"}
{"DOCID": "2469", "TEXT": "Petri Nets and Speed Independent design: Petri nets are investigated as one method of modeling speed independent asynchronous circuits. A study of circuit realizations of Petri nets leads to a demonstration of their usefulness in modeling speed independent operation.  This usefulness is emphasized by the design of a speed independent processor from modules developed in the investigation of Petri net implementation."}
{"DOCID": "2470", "TEXT": "Fen-An Axiomatic Basis for Program Semantics: A formal system is presented which abstracts the notions of data item, function, and relation. It is argued that the system is more suitable than set theory (or its derivatives) for the concise and accurate description of program semantics.  It is shown how the system can be used to build composite data types out of simper ones with the operations of rowing, structuring, and uniting.  It is also demonstrated that completely new primitive types can be introduced into languages through the mechanism of singleton data types.  Both deterministic and nondeterministic functions are shown to be definable in the system. It is described how the local environment can be modeled as a data item and how imperative statements can be considered functions on the environment.  The nature of recursive functions is briefly discussed, and a technique is presented by which they can be introduced into the system.  The technique is contrasted with the use of the paradoxical combinator, Y.  The questions of local and global environments and of various modes of function calling and parameter passing are touched upon. The theory is applied to the proof of several elementary theorems concerning the semantics of the assignment, conditional, and iterative statements.  An appendix is included which presents in detail the formal system governing webs and fen, the abstractions used informally in the body of the paper."}
{"DOCID": "2471", "TEXT": "A Learning Program Which Plays Partnership Dominoes: A learning program has been written is BASIC to play four-player partnership dominoes.  Because dominoes is a game of incomplete information, the program uses somewhat different principles of artificial intelligence from those used in programs for games of complete information, such as checkers, chess, and go.  The program was constructed to use a \"strategy signature table\" which classifies board situations through the interactions of game parameters. Each entry in the table contains adaptively determined weights indicating the advi sability of various strategies. Once chosen, a strategy then employs probability analysis and linear polynomial evaluation to choose a move.  Our program wins approximately two-thirds of its games in tournament situations, and has defeated championship players."}
{"DOCID": "2472", "TEXT": "Minimal spanning Tree (Algorithm R422)"}
{"DOCID": "2473", "TEXT": "Hidden-Line Plotting Program (Algorithm R420)"}
{"DOCID": "2474", "TEXT": "DIFSUB for Solution of Ordinary Differential Equations (Algorithm C407)"}
{"DOCID": "2475", "TEXT": "Solution of Linear Programming Problems in 0-1 Variables [H1] (Algorithm A449)"}
{"DOCID": "2476", "TEXT": "Equivalence Between AND/OR Graphs and Context-Free Grammars"}
{"DOCID": "2477", "TEXT": "Multiple Exits from a Loop Without the GOTO"}
{"DOCID": "2478", "TEXT": "Computer Science-Seminars for Undergraduates"}
{"DOCID": "2479", "TEXT": "Curriculum Recommendations for Graduate Professional Programs in Information Systems: Recommended Addendum on Information Systems Administration: An addendum to the Report of the ACM Curriculum Committee on Computer Education for Management is proposed. The proposed addendum is to include in the curriculum a course on Information Systems administration. It is important for two reasons: (1) the systems designer must understand the administrative framework in which he must operate to work effectively, and (2) an important objective of the curriculum recommendations is to prepare the future manager of the computer activity. It is felt that the importance of these two reasons justifies the addition of the recommended course. The course is outlined in the format of the original report."}
{"DOCID": "2480", "TEXT": "Teaching \"About Programming\": This paper presents the goals and organization of a course about programming designed to provide entering students in a graduate program with a cultural enrichment in their professional lives.  The students are expected to have taken at least two programming courses prior to this one and, therefore, to be familiar with at least two programming languages, both as students and users.  Teaching someone how to program is similar to teaching him to play a musical instrument: neither skill can be taught-they must be learned.  However, the teacher still serves several vital purposes: to present a set of rules for producing well-formed utterances; to offer numerous demonstrations of his own skill; and to function as an involved critic.  Finally, the teacher is the source of information about the process in which the student is involved."}
{"DOCID": "2481", "TEXT": "The Distribution of a Program in Primary and Fast Buffer Storage: A virtual memory computer system with a fast buffer (cache) memory between primary memory and the central processing unit is considered.  The optimal distribution of a program between the buffer and primary memory is studied using the program's lifetime function.  Expressions for the distribution of a program which maximizes the useful fraction of the cost-time integral of primary and fast buffer storage are obtained for swapping and nonswapping buffer management policies."}
{"DOCID": "2482", "TEXT": "Mixed Solutions for the Deadlock Problem: Mixtures of detection, avoidance, and prevention provide more effective and practical solutions to the deadlock problem than any one of these alone.  The individual techniques can be tailored for subproblems of resource allocation and still operate together to prevent deadlocks.  This paper presents a method, based on the concept of the hierarchical operating system, for constructing appropriate mixtures and suggests appropriate subsystems for the most frequently occurring resource allocation problems"}
{"DOCID": "2483", "TEXT": "COKO III: The Cooper-Koz Chess Program: COKO III is a chess player written entirely in Fortran.  On the IBM 360-65, COKO III plays a minimal chess game at the rate of .2 sec cpu time per move, with a level close to lower chess club play.  A selective tree searching procedure controlled by tactical chess logistics allows a deployment of multiple minimal game calculations to achieve some optimal move selection.  The tree searching algorithms are the heart of COKO's effectiveness, yet they are conceptually simple.  In addition, an interesting phenomenon called a tree searching catastrophe has plagued COKO's entire development just as it troubles a human player.  Standard exponential growth is curbed to a large extent by the definition and trimming of the Fischer set.  A clear distinction between tree pruning and selective tree searching is also made. Representation of the chess environment is described along with a strategical preanalysis procedure that maps the Lasker regions.  Specific chess algorithms are described which could be used as a command structure by anyone desiring to do some chess program experimentation.  A comparison is made of some mysterious actions of human players and COKO III."}
{"DOCID": "2484", "TEXT": "A Note on Information Organization and Storage: Since the logical structure of a data base can be represented by a tree or graph, it is quite natural for us to view the process of designing a data base as that of constructing a tree or a graph. A general method for constructing such a tree or a graph is provided.  There are three important elements in this general construction method; namely, a set of binary relations, an algorithm for constructing subsets of a set, and an algorithm for selecting an element from the given set of objects.  The use of different relations and algorithms results in different information structures, as list, tree, ring, etc.  Thus the problem of information organization and storage is reduced to that of defining relations and formulating algorithms under a given set of constraints. The results presented may be valuable to designers as useful design concepts, and may serve as a basis for developing a formal theory on the subject."}
{"DOCID": "2485", "TEXT": "Managing the Computer Resource: A Stage Hypothesis: Based on the study of expenditures for data processing, a descriptive stage hypothesis is presented. It is suggested that the planning, organizing, and controlling activities associated with managing the computer resource will change in character over a period of time, and will evolve in patterns roughly correlated to four stages of the computer budget: Stage I (computer acquisition), Stage II (intense system development), Stage III (proliferation of controls), and Stage IV (user/service orientation).  Each stage is described and related to individual tasks for managing the computer resource."}
{"DOCID": "2486", "TEXT": "Computer Photocomposition of Technical Text: In computer assisted typesetting by means of photocomposition, special problems arise in highly technical material such as mathematical formulas.  New solutions to several of these problems have been devised in the information system of the American Institute of Physics.  They include: the representation of special characters (foreign alphabets, mathematical symbols, etc.) not available on input keyboards or on the photocomposer; the generation of such symbols, e.g. by overprinting; the precise positioning of accent marks (floating diacritics); line breaks, i.e. words or formulas placed partly at the end of one line and partly at the beginning of the next; and certain aspects of error correction."}
{"DOCID": "2487", "TEXT": "Cubic Spline solutions to Fourth-order Boundary Value Problems: The cubic spline approximation to the fourth-order differential equation y''''+p(x)y''+q(x)y'+r(x)y=t(x) is shown to reduce to the solution of a five-term recurrence relationship.  For some special cases the approximation is shown to be simply related to a finite difference representation with a local truncation error of order (y/720)delta^8."}
{"DOCID": "2488", "TEXT": "Least Squares Piecewise Cubic Curve Fitting: The matrices involved in a linear least squares formulation are determined for the problem of fitting piecewise cubic functions, those possessing a continuous derivative, to arrays of planar data."}
{"DOCID": "2489", "TEXT": "Number of Multiply-Restricted Partitions [A1] (Algorithm A448)"}
{"DOCID": "2490", "TEXT": "Efficient Algorithms for Graph Manipulation [H] (Algorithm A447): Efficient algorithms are presented for partitioning a graph into connected components, biconnected components and simple paths.  The algorithm for partitioning of a graph into simple paths is iterative and each iteration produces a new path between two vertices already on paths.  (The start vertex can be specified dynamically.)  If V is the number of vertices and E is the number of edges, each algorithm requires time and space proportional to max (V,E) when executed on a random access computer."}
{"DOCID": "2491", "TEXT": "Threaded Code: The concept of \"threaded code\" is presented as an alternative to machine language code.  Hardware and software realizations of it are given.  In software it is realized as interpretive code not needing an interpreter.  Extensions and optimizations are mentioned."}
{"DOCID": "2492", "TEXT": "The Development of Decision Tables via Parsing of Complex Decision Situations: A new parsing technique is proposed which allows parsing based only on syntactical characteristics of the decision problem.  It requires a description of the problem in decision grid chart format and allows the development of decision tables within defined limits by avoiding, or at least minimizing, repetition of conditions and actions in the resulting tables."}
{"DOCID": "2493", "TEXT": "Optimum Data Base Reorganization Points: In certain data base organization schemes the cost per access may increase due to structural inefficiencies caused by updates.  By reorganizing the data base the cost per access may be reduced. However, the high cost of a reorganization prohibits frequent reorganizations.  This paper examines strategies for selecting the optimum reorganization points."}
{"DOCID": "2494", "TEXT": "A Computer Generated Aid for Cluster Analysis: A computer generated graphic method, which can be used in conjunction with any hierarchical scheme of cluster analysis, is described and illustrated. The graphic principle used is the representation of the elements of a data matrix of similarities or dissimilarities by computer printed symbols (of character overstrikes) of various shades of darkness, where a dark symbol corresponds to a small dissimilarity. The plots, applied to a data matrix before clustering and to the rearranged matrix after clustering, show at a glance whether clustering brought forth any distinctive clusters.  A well-known set of data consisting of the correlations of 24 psychological tests is used to illustrate the comparison of groupings by four methods of factor analysis and two methods of cluster analysis."}
{"DOCID": "2495", "TEXT": "Adapting Optimal Code Generation for Arithmetic Expressions to the Instruction Sets Available on Present-Day Computers"}
{"DOCID": "2496", "TEXT": "On the Near-Optimality of the Shortest-Latency-Time-First Drum Scheduling Discipline: For computer systems in which it is practical to determine the instantaneous drum position, a popular discipline for determining the sequence in which the records are to be accessed is the so-called shortest-latency-time-first, SLTF, discipline.  When a collection of varying-length records is to be accessed from specified drum positions, it is known that the SLTF discipline does not necessarily minimize the drum latency time.  However, we show that the total time to access the entire collection for any SLTF schedule is never as much as a drum revolution longer than a minimum latency schedule."}
{"DOCID": "2497", "TEXT": "Synchronizing Processors with Memory-Content-Generated Interrupts: Implementations of the \"Lock-Unlock\" method of synchronizing processors in a multiprocessor system usually require uninterruptable, memory-pause type instructions. An interlock scheme called read-interlock, which does not require memory-pause instructions, has been developed for a dual DEC PDP-10 system with real-time requirements.  The read-interlock method does require a special\"read-interlock\" instruction in the repertoire of the processors and a special \"read-interlock\" cycle in the repertoire of the memory modules.  When a processor examines a \"lock\" (a memory location) with a read-interlock instruction, it will be interrupted if the lock was already set; examining a lock immediately sets it if it was not already set (this event sequence is a read-interlock cycle). Writing into a lock clears it.  Having the processor interrupted upon encountering a set lock instead of branching is advantageous if the branch would have resulted in an effective interrupt."}
{"DOCID": "2498", "TEXT": "Minimizing Wasted Space in Partitioned Segmentation: A paged virtual memory system using a finite number of page sizes is considered.  Two algorithms for assigning pages to segments are discussed.  Both of these algorithm are simple to implement.  The problem of choosing the page sizes to minimize the expected value of total wasted space in internal fragmentation and in a page table, per segment, is then solved for a probability density function of segment size which may be expressed as a convex combination of Erlang densities."}
{"DOCID": "2499", "TEXT": "Efficient Multiprogramming Resource Allocation and Accounting: Although sometimes thought of as only a component of time-sharing operation, multiprogramming can involve broader questions of resource allocation, since fairness is not required to meet a response criterion.  In a multiprogrammed system, it may serve maximal resource use to be unfair, for example by holding an input/output channel idle for a program while it completes a small amount of processor usage, enabling further use of the channel.  Several applications of this principle are given, and it is suggested that a multiprogramming executive might dynamically adjust its allocation algorithms to gain efficiency.  Allocation of resources is closely connected to accounting for those resources, raising the problems of repeatability, minimal uncharged overhead, and relative weighting of charges for dependent resources.  Since weightings may depend on allocation algorithms, these are not arbitrary accounting parameters.  Often the only repeatable accounting is one which omits an extensive overhead will be paid, and should multiprogramming prove efficient, overcharges will result.  Multiprogramming turns on allocation of the memory resource essential to control of other resources.  The general suggestions for allocation and accounting are applied to this question, and some details provided for the case of a monitor which controls a virtual-memory machine."}
{"DOCID": "2500", "TEXT": "A Practical Approach to Managing Resources and Avoiding Deadlocks: Resource scheduling and allocation can be expensive with regard to time and space in multiprogramming or time-sharing environments involving large numbers of tasks and resources with conflicting requirements. Detection and/or prevention of deadlocks can require massive amounts of additional overhead if efficient usage of resources is to be maintained.  A resource management program is described which uses linked lists along with other techniques to overcome a large portion of this overhead.  The program, which is currently running as part of a large scale general purpose operating system, keeps resources relatively active but does not detect or prevent all deadlocks in its implemented state.  Certain changes, which would permit more comprehensive levels of deadlock prevention/detection at additional cost, have not been incorporated in the running system due to the infrequency of deadlock situations."}
{"DOCID": "2501", "TEXT": "WYLBUR: An Interactive Text Editing and Remote Job Entry System: WYLBUR is a comprehensive system for manipulating all kinds of text, such as computer programs, letters, and manuscripts, using typewriter terminals connected to a computer.  It has facilities for remote job entry and retrieval as well as facilities for text alignment and justification.  A powerful method for addressing text by content is provided.  This paper describes the external appearance of WYLBUR as well as its internal structure.  A short description of the major features of ORVYL, a general purpose time-sharing system which operates in conjunction with WYLBUR, is also included."}
{"DOCID": "2502", "TEXT": "A Comment on the Practical Aspects of Computer Science Education"}
{"DOCID": "2503", "TEXT": "Another Comment on Computer Music"}
{"DOCID": "2504", "TEXT": "Concerning Music and Computer Composition in Computational Linguistics"}
{"DOCID": "2505", "TEXT": "Reflection-Free Permutations, Rosary Permutations, and Adjacent Transposition Algorithms"}
{"DOCID": "2506", "TEXT": "A Sparse Matrix Package (Algorithm R408)"}
{"DOCID": "2507", "TEXT": "Exact Solution of Linear Equations Using Residue Arithmetic (Algorithm R406)"}
{"DOCID": "2508", "TEXT": "Increasing the Efficiency of Quicksort (Algorithm R402)"}
{"DOCID": "2509", "TEXT": "Minit Algorithm for Linear Programming (Algorithm R333)"}
{"DOCID": "2510", "TEXT": "Minit Algorithm for Linear Programming (Algorithm R333)"}
{"DOCID": "2511", "TEXT": "Maxflow (Algorithm R324)"}
{"DOCID": "2512", "TEXT": "Coulomb Wave Functions (Algorithm R300)"}
{"DOCID": "2513", "TEXT": "A Nonrecursive List Moving Algorithm: An efficient, nonrecursive algorithm is given for moving any LISP-type list.  In particular, the algorithm requires no storage other than the new nodes into which the list is to be moved, and no additional bits per node for marking; the algorithm runs in time proportional to the number of nodes in the list.  The original list structure is destroyed as it is moved."}
{"DOCID": "2514", "TEXT": "An Array Grammar Programming System: A package of Fortran programs has been developed that permits a user to interactively design and test array grammars.  The user can control the rule selection procedure in a derivation or parse, using weighted programming matrices; he also has a choice of instance selection schemes (raster,random, parallel).  Examples are given involving array languages consisting of simple geometrical patterns, as well as a language of \"neuron pictures.\""}
{"DOCID": "2515", "TEXT": "Minimal Event-Node Network of Project Precedence Relations: A procedure for constructing a minimal event-node network to represent a set of precedence relations without parallel activities is presented.  A minimal event-node network is an event-node network in which both the number of nodes and the number of arcs are the minima to preserve the given precedence relations Counterexamples are given to show that the algorithm presented by A. C. Fisher, J. S. Liebman, and G. L. Nemhauser (1968) produces event-node networks which are not minimal.  Since our procedure includes the set-covering problem, the time required may grow exponentially with the number of given activities."}
{"DOCID": "2516", "TEXT": "Hierarchical Storage in Information Retrieval: A probabilistic analysis is employed to determine the effect of hierarchical storage organizations on information retrieval operations.  The data storage hardware is assumed to consist on n-levels of linearly connected memory hardware with increasing data access times and increasing data storage capabilities. A system might, for example, consist of fast semiconductor memory, computer core memory, extended core storage, disk memory, and data cells.  Equations are derived to predict the effect of such a system on data access times using sequential files, random access files, and structured files employing multiple-hierarchical linked lists."}
{"DOCID": "2517", "TEXT": "Some Comments on the Use of Ambiguous Decision Tables and Their Conversion to Computer Programs: This paper comments upon recently published work on decision table translation using methods similar to the rule-mask technique.  The applicability of these methods under various possible conventions on overall table meaning is discussed, and it is argued that there is a place both for the multi-rule and the single-rule (or action set) convention in decision tale usage."}
{"DOCID": "2518", "TEXT": "Programming by Questionnaire: An Effective Way To Use Decision Tables: Programming by questionnaire combines aspects of decision table programming and general purpose programming by using decision tables to construct an application program through the selection of certain source statements from a predefined file.  It is proposed that programming by questionnairies a useful compromise between general and special purpose programming for a significant class of large scale problems. The elements of the approach are discussed an existing application is described."}
{"DOCID": "2519", "TEXT": "On the Problem of Communicating Complex Information: The nature of the difficulty involved in communicating mathematical results between scientists using a computer based information retrieval system is examined.  The problem is analyzed in terms of psychological and information-processing processes, and what turns out to be a vicious circle of effects is described.  These include ways of augmenting written natural language by various notational and linguistic devices, the exhibition of the structure inherent in the information we are communicating, and a sophisticated interactive system controlled by computer."}
{"DOCID": "2520", "TEXT": "Greatest Common Divisor of n Integers and Multipliers (Algorithm C386)"}
{"DOCID": "2521", "TEXT": "Ten Subroutines for the Manipulation of Chebyshev Series [C1] (Algorithm A446)"}
{"DOCID": "2522", "TEXT": "The Design, Implementation, and Evaluation of a Working Set Dispatcher: The behavior of a computer system is largely dependent upon the algorithms employed to allocate the system resources to the processes competing for them. Recent research in time-sharing paging systems has developed the working set model for program behavior, and are source allocation strategy based on this model has been proposed.  Two implementations along these principles have been reported, but it seems that in neither case have further results been announced.  This report discusses the design and implementation of a dispatcher based on the working set principle, presents data to permit analysis of its behavior, and indicates future directions of research on methods of controlling a computer system."}
{"DOCID": "2523", "TEXT": "A Region Coloring Technique for Scene Analysis: A method of converting a picture into a \"cartoon\" or \"map\" whose regions correspond to differently textured regions is described.  Texture edges in the picture are detected, and solid regions surrounded by these (usually broken) edges are \"colored in\" using a propagation process.  The resulting map is cleaned by comparing the region colors with the textures of the corresponding regions in the picture, and also by merging some regions with others according to criteria based on topology and size.  The method has been applied to the construction of cloud cover maps from cloud cover pictures obtained by satellites."}
{"DOCID": "2524", "TEXT": "Some Approaches to Best-Match File Searching: The problem of searching the set of keys in a file to find a key which is closest to a given query key is discussed.  After \"closest,\" in terms of a metric on the the key space, is suitably defined, three file structures are presented together with their corresponding search algorithms, which are intended to reduce the number of comparisons required to achieve the desired result. These methods are derived using certain inequalities satisfied by metrics and by graph-theoretic concepts.  Some empirical results are presented which compare the efficiency of the methods."}
{"DOCID": "2525", "TEXT": "A Statistical Study of the Accuracy of Floating Point Number Systems: This paper presents the statistical results of tests of the accuracy of certain arithmetic systems in evaluating sums, products and inner products, and analytic error estimates for some of the computations.  The arithmetic systems studied are 6-digit hexadecimal and 22-digit binary floating point number representations combined with the usual chop and round modes of arithmetic with various numbers of guard digits, and with a modified round mode with guard digits.  In a certain sense, arithmetic systems differing only in their use of binary or hexadecimal number representations are shown to be approximately statistically equivalent inaccuracy.  Further, the usual round mode with guard digits is shown to be statistically superior in accuracy to the usual chop mode in all cases save one.  The modified round mode is found to be superior to the chop mode in all cases."}
{"DOCID": "2526", "TEXT": "Asymmetric Memory Hierarchies: A study is presented of some of the system implications of memory hierarchies in which the backing or secondary store has a very small read time, relative of both the time required for writing and to the read time of conventional backing storage devices. Several analytic models are introduced, and it is shown that such hierarchies may operate in ways which differ from those of more conventional hierarchies.  In particular, it is shown that it may not be necessary to multiprogram in such a situation. In the past, backing storage devices have been roughly symmetric with respect to their read and write times.  This situation may not continue, as several devices are currently under development which may have a very small read-time/write-time ratio.  This study places particular emphasis on one such system-the RCA read/write holographic optical memory."}
{"DOCID": "2527", "TEXT": "Implementation of High Level Language Machine: Computing machines which directly execute the statements of a high level language have been proposed in the past.  This report describes the actual implementation of such a machine: it is a computer whose \"machine language\" is APL.  The machine is fully operational and correctly executes almost all of the APL operations on scalars, vectors, and arrays. The machine automatically allocates memory, executes statements, calls functions, converts numbers from one type to another, checks subscripts, and automatically detects many types of programmer errors."}
{"DOCID": "2528", "TEXT": "Binary Pattern Reconstruction from Projections [Z] (Algorithm R445)"}
{"DOCID": "2529", "TEXT": "Binary Pattern Reconstruction from Projections [Z] (Algorithm A445)"}
{"DOCID": "2530", "TEXT": "An Algorithm for Extracting Phrases in a Space-Optimal Fashion [Z] (Algorithm A444)"}
{"DOCID": "2531", "TEXT": "Graduate Education: The Ph.D. Glut"}
{"DOCID": "2532", "TEXT": "On Harrison's Substring Testing Technique"}
{"DOCID": "2533", "TEXT": "Gray Code and the +- Sign Sequence when +-f (+-f(+-f(...+-f(x)...))) Is Ordered"}
{"DOCID": "2534", "TEXT": "Design and Implementation of a Diagnostic Compiler for PL/I: PL/C is a compiler for a dialect for PL/I.  The design objective was to provide a maximum degree of diagnostic assistance in a batch processing environment. For the most part this assistance is implicit and is provided automatically by the compiler. The most remarkable characteristic of PL/C is its perseverance-it completes translation of every program submitted and continues execution until a user-established error limit is reached. This requires that the compiler repair errors encountered during both translation and execution, and the design of PL/C is dominated by this consideration.  PL/C also introduces several explicit user-controlled facilities for program testing. To accommodate these extensions to PL/I without abandoning compatibility with IBM compiler PL/C permits \"pseudo comments\"-constructions whose contents can optionally be considered either source test or comment. In spite of the diagnostic effort PL/C is a fast and efficient processor.  It effectively demonstrates that compilers can provide better diagnostic assistance than is customarily offered, even when a sophisticated source language is employed, and that this assistance need not be prohibitively costly."}
{"DOCID": "2535", "TEXT": "The Effects of Multiplexing on a Computer-Communications System: A study is made of the way in which asynchronous time division multiplexing changes the stochastic nature of the arrival process from a user to the computer and, consequently, affects the performance of a time-shared computer-communications system.  It is concluded that while, for certain values of system parameters, there is noticeable improvement in the performance of the computer (model), in the sense that time-shared scheduling delays are reduced, these improvements are offset by the transmission delays imposed by multiplexing so that there may be little or no change in the computer-communications system performance.  Analytical and simulation results are based on the model of the computer-communications system being an M/D/1 queue (the multiplexor) in tandem with a single exponential server (the computer). Analytical results include a general description of the output process of an M/D/1 queue and the conditions under which this output process is approximately Poisson."}
{"DOCID": "2536", "TEXT": "Telecommunications Using a Front-End Minicomputer: The use of a front-end minicomputer to provide varied remote terminal access to a large scale computer is considered.  The problems of embedding telecommunications I/O within an operating system are discussed, and it is shown how the decentralization of intelligence acquired by front-end processing vastly simplifies the problem.  A specific implementation is discussed with emphasis on the main processor-minicomputer link, the hardware-software implementation, the effect of the main processor operating system, and an assessment of the advantages over a hard wired line controller."}
{"DOCID": "2537", "TEXT": "Common Phrases and Minimum-Space Text Storage: A method for saving storage space for text strings, such as compiler diagnostic messages, is described.  The method relies on hand selection of a set of text strings which are common to one or more messages.  These phrases are then stored only once. The storage technique gives rise to a mathematical optimization problem: determine how each message should use the available phrases to minimize its storage requirement.  This problem is nontrivial when phrases which overlap exist.  However, a dynamic programming algorithm is presented which solves the problem in time which grows linearly with the number of characters in the text.  Algorithm 444 applies to this paper."}
{"DOCID": "2538", "TEXT": "A Computer Science Course Program for Small Colleges: The ACM Subcommittee on Small College Programs of the Committee on Curriculum in Computer Science (CCCS) was appointed in 1969 to consider the unique problems of small colleges and universities, and to make recommendations regarding computer science programs at such schools.  This report, authorized by both the subcommittee and (CCCS), supplies a set of recommendations for courses and necessary resources. Implementation problems are discussed, specifically within the constraints of limited faculty and for the purposes of satisfying a wide variety of objectives. Detailed description of four courses are given; suggestions are made for more advanced work; and an extensive library list is included."}
{"DOCID": "2539", "TEXT": "Solution of the Transcendental Equation w*exp(w)=x [C5] (Algorithm A443)"}
{"DOCID": "2540", "TEXT": "Properties of the Working Set Model (Corrigendum)"}
{"DOCID": "2541", "TEXT": "An Overview of the ISPL Computer System Design: This paper explores the advantages of the concurrent design of the language, operating system, and machine (via microcode) to create an interactive programming laboratory.  It describes the synergistic effect that the freedom to move and alter features from one of these domains to another has had on the design of this system (which has not been implemented). This freedom simplified both incremental compilation and the system's addressing structure, and centralized the communication mechanisms enabling the construction of hierarchical subsystems.  It also suggested an important new concept for operating systems: separation of the scheduling from the maintenance functions in resource allocation. This separation enables incorporation of new scheduling algorithms (decision of what to do) without endangering the system integration (correctly performing the scheduling decisions)."}
{"DOCID": "2542", "TEXT": "A Software Design and Evaluation System: A critical failure of current software system design and implementation methodology is that the performance of a proposed design is not evaluated before it is actually implemented.  In this paper the reasons for this failure are explored, and a new methodology which overcomes many of the difficulties is proposed.  A system which integrates performance evaluation with design and implementation is described. This system is based on a simple, high level language which is used to describe the evolving system at all stages of its development.  The source language description is used as direct input to performance analysis and simulation routines.  Using the performance information obtained from these routines as feedback, the problems which adversely affect performance are detected early enough so that they can be corrected without costly major reimplementation of the proposed system."}
{"DOCID": "2543", "TEXT": "Reducing the Retrieval Time of Scatter Storage Techniques: A new method for entering and retrieving information in a hash table is described.  The method is intended to be efficient if most entries are looked up several times.  The expected number of probes to look up an entry, predicted theoretically and verified by Monte Carlo experiments, is considerably less than for other comparable methods if the table is nearly full.  An example of a possible Fortran implementation is given."}
{"DOCID": "2544", "TEXT": "Automatic Error bounds for Simple Zeros of Analytic Functions: The Cauchy-Ostrowski theorem on convergence of Newton iterates for an analytic function in one variable is extended to include computational errors using complex interval arithmetic. Several numerical examples are given for polynomials with real and complex roots and one example for the Bessel function of the first kind."}
{"DOCID": "2545", "TEXT": "A Theory of Discrete Patterns and Their Implementation in SNOBOL4: The notion of a discrete pattern is formalized and certain properties deduced.  A pattern is shown to be a generalization of a formal language. Algorithms for implementing the kinds of patterns in SNOBOL4 are given.  The general approach is to create, in-so-far as possible, a bottom-up parse from a top-down specification."}
{"DOCID": "2546", "TEXT": "The Use of Grammatical Inference for Designing Programming Languages: Both in designing a new programming language and in extending an existing language, the designer is faced with the problem of deriving a \"natural\" grammar for the language.  We are proposing an interactive approach to the grammar design problem wherein the designer presents a sample of sentences and structures as input to a grammatical inference algorithm.  The algorithm then constructs a grammar which is a reasonable generalization of the examples submitted by the designer. The implementation is presently restricted to a subclass of operator precedence grammars, but a second algorithm is outlined which applies to a larger class of context-free grammars."}
{"DOCID": "2547", "TEXT": "Representation of Contours ad Regions for Efficient Computer Search: A novel computer-searchable representation for the three basic pictorial features, contour maps, region coverage, and line structures, is described. The representation, which has practical storage requirements, provides a rapid mean of searching large files for data associated with geometric position as well as with attribute value.  An application of this representation to handling terrain information illustrates its utility.  The algebraic properties of the data structure make it computationally easy to determine whether a point lies within a closed boundary; compute the area contained by a closed boundary; generate the closed boundary representing the union or intersection of two closed boundaries; and determine the neighboring boundaries to a point and the minimum distances between them and the point."}
{"DOCID": "2548", "TEXT": "Normal Deviate [S14] (Algorithm A442)"}
{"DOCID": "2549", "TEXT": "Random Deviates from the Dipole Distribution [G5] (Algorithm A441)"}
{"DOCID": "2550", "TEXT": "A Multidimensional Monte Carlo Quadrature with Adaptive Stratified Sampling [D1] (Algorithm A440)"}
{"DOCID": "2551", "TEXT": "Mutual Recursion in Algol 60 Using Restricted Compilers"}
{"DOCID": "2552", "TEXT": "A Note on When To Chain Overflow Items Within a Direct-Access Table"}
{"DOCID": "2553", "TEXT": "The Practical Aspect of Computer Science Education-Discussion"}
{"DOCID": "2554", "TEXT": "Reduction of a Band-Symmetric Generalized Eigenvalue Problem: An algorithm is described for reducing the generalized eigenvalue problem Ax = lambda Bx to an ordinary problem, in case A and B are symmetric band matrices with B positive definite.  If n is the order of the matrix and m the bandwidth, the matrices A and B are partitioned into m-by-m blocks; and the algorithm is described in terms of these blocks. The algorithm reduces the generalized problem to an ordinary eigenvalue problem for a symmetric band matrix C whose bandwidth is the same as A and B. The algorithm is similar to those of Rutishauser and Schwartz for the reduction of symmetric matrices to band form.  The calculation C requires order mn^2 operation.  The round-off error in the calculation of C is of the same order as the sum of the errors at each of the n/m steps of the algorithm, the latter errors being largely determined by the condition of B with respect to inversion."}
{"DOCID": "2555", "TEXT": "Variable-Precision Exponentiation: A previous paper presented an efficient algorithm, called the Recomputation Algorithm, for evaluating a rational expression to within any desired tolerance on a computer which performs variable-precision arithmetic operations.  The Recomputation Algorithm can be applied to expressions involving any variable-precision operations having O(10^(-p) + SUM{|Ei|}) error bounds, where p denotes the operation's precision and Ei denotes the error in the operation's ith argument. This paper presents an efficient variable-precision exponential operation with an error bound of the above order.  Other operations such as log, sin, and cos, which have simple series expansions, can be handled similarly."}
{"DOCID": "2556", "TEXT": "Adaptive Correction of Program Statements: A method of analyzing statements in a programming language which can tolerate a considerable inaccuracy in their specification is proposed. This method involves principles at present mainly confined to studies in the area of artificial intelligence such as feature extraction, approximate tree matching, and strategy improvement by feedback from the matching process. A pilot program incorporating the principles is described and preliminary operating results are presented. A final section surveys further principles which are currently being investigated."}
{"DOCID": "2557", "TEXT": "On the Time Required for a Sequence of Matrix Products: This paper discusses the multiplication of conformable sequences of row vectors, column vectors, and square matrices.  The minimum time required to evaluate such products on ordinary serial computers as well as parallel computers is discussed.  Algorithms are presented which properly parse such matrix sequences subject to the constraints of the machine organization."}
{"DOCID": "2558", "TEXT": "Protection in Programming Languages: Linguistic mechanisms which can be used to protect one subprogram from another's malfunctioning are described.  Function-producing functions and various type-tagging schemes are considered.  An attempt is made to distinguish between access limitation and authentication."}
{"DOCID": "2559", "TEXT": "The Reallocation of Hash-Coded Tables: When the space allocation for a hash-coded table is altered, the table entries must be rescattered over the new space.  A technique for accomplishing this rescattering is presented.  The technique is independent of both the length of the table and the hashing function used, and can be utilized in conjunction with a linear reallocation of the table being rescattered. Moreover, it can be used to eliminate previously flagged deletions from any hash-coded table, or to change from one hashing method to another.  The efficiency of the technique is discussed and theoretical statistics are given."}
{"DOCID": "2560", "TEXT": "A Queuing Model of a Multiprogrammed Computer with a Two-Level Storage System: The results are presented of an analysis of a probabilistic model of a multiprogrammed computer system with a two-level storage system in which there is sequential dependency of accesses between the devices.  Expressions are obtained for the long-run probability that both the CPU and each of the storage devices are busy.  Some numerical results are given which quantify the gains in CPU utilization obtainable by multiprogramming in the presence of this type of storage system."}
{"DOCID": "2561", "TEXT": "A Heuristic Approach to Inductive Inference in Fact Retrieval Systems: Heuristic procedures are presented which have been developed to perform inferences by generalizing from available information.  The procedures make use of a similarity structure which is imposed on the data base using nonnumerical clustering algorithms.  They are implemented in a model fact retrieval system which uses a formal query language and a property-list data structure.  A program of experiments is described wherein the procedures are used with test data bases which are altered by deleting part of the data and by purposely introducing false data.  It is found that the system can infer the correct response under a variety of conditions involving incomplete and inconsistent data."}
{"DOCID": "2562", "TEXT": "Routing Problem (Algorithm R456)"}
{"DOCID": "2563", "TEXT": "Merge Sort Algorithm (R426)"}
{"DOCID": "2564", "TEXT": "Hidden-Line Plotting Program (Algorithm R420)"}
{"DOCID": "2565", "TEXT": "A Gaussian Pseudo-Random Number Generator (Algorithm 488)"}
{"DOCID": "2566", "TEXT": "Exact Cumulative Distribution of the Kolmogorov-Smirnov Statistic for Small Samples (Algorithm A487)"}
{"DOCID": "2567", "TEXT": "An Exponential Method for the Solution of Systems of Ordinary Differential Equations: An explicit, coupled, single-step method for the numerical solution of initial value problems for systems of ordinary differential equations is presented. The method was designed to be general purpose in nature but to be especially efficient when dealing with stiff systems of differential equations. It is, in general, second order except for the case of a linear system with constant coefficients and linear forcing terms; in that case, the method is third order.  It has been implemented and put to routine usage in biological applications-where stiffness frequently appears-with favorable results.  When compared to a standard fourth order Runge-Kutta implementation, computation time required by this method has ranged from comparable for certain nonstiff problems to better than two orders of magnitude faster for some highly stiff systems."}
{"DOCID": "2568", "TEXT": "A Graph Formulation of a School Scheduling Algorithm: The problem classically titled \"The Examination Schedule Problem\" takes various forms in the literature.  Most of these formulations can be presented in the terminology of classical Network Theory. One such formulation is:  Given a nondirected network, partition its nodes into a minimal number of subsets such that no two members of the same subset are connected by anarc.  An obvious lower limit to this number is the size of the largest strongly connected subgraph.  Kirchgassner proved that an upper limit is this size plus one.  One logical extension of the previous work is the introduction of variable length examinations where W(I) is the number of periods for exam I.  The object of this paper is to generalize the definition of largest strongly connected subgraph to include the weighting of nodes, to present an approximate algorithm which usually finds the largest strongly connected subgraph, and to discuss the application of this algorithm to the solution of school scheduling and exam scheduling problems."}
{"DOCID": "2569", "TEXT": "Computer Generation of Gamma Random Variates with Non-integral Shape Parameters: When the shape parameter, a, is integral, generating gamma random variables with a digital computer is straightforward.  There is no simple method for generating gamma random variates with non-integral shape parameters.  A common procedure is to approximately generate such random variables by use of the so-called probability switch method.  Another procedure, which is exact, is due to Johnk.  This paper presents a rejection method for exactly generating gamma random variables when a is greater than 1. The efficiency of the rejection method is shown to be better than the efficiency of Johnk's method. The paper concludes that when a is non-integral the following mix of procedures yields the best combination of accuracy and efficiency: (1) when a is less than 1, use Johnk's method; (2) when 1 is less than a and a is less than 5, use the rejection method; (3) when a is greater than 5, use the probability switch method."}
{"DOCID": "2570", "TEXT": "A Comparison of List Schedules for Parallel Processing Systems: The problem of scheduling two or more processors to minimize the execution time of a program which consists of a set of partially ordered tasks is studied.  Cases where task execution times are deterministic and others in which execution times are random variables are analyzed.  It is shown that different algorithms suggested in the literature vary significantly in execution time and that the B-schedule of Coffman and Graham is near-optimal.  A dynamic programming solution for the case in which execution times are random variables is presented."}
{"DOCID": "2571", "TEXT": "An Analytic Model of the Hasp Execution Task Monitor: The HASP Execution Task Monitor periodically rearranges the OS/360 dispatching chain to give tasks preemptive execution priority in inverse order to that of their cpu utilization history.  The effect is to keep the I/O bound tasks active and to prevent cpu bound tasks from locking out other tasks.  This paper develops a simple model of the Execution Task Monitor and employs it to study the effectiveness of the monitor in improving system performance.  A modified strategy monitor control is investigated for the case of task execution in a memory hierarchy of varying speeds."}
{"DOCID": "2572", "TEXT": "Arguments for a Moratorium on the Construction of a Community Information Utility: In this article the author urges a prudent and decentralized approach to the question of the design and desirability of computerized community information utilities.  Before accepting the inevitability and desirability of this or any technology, we should: (1) be sure of the feasibility (internally and externally) of what is proposed; (2) project and perhaps wait for changes in complementary techniques; (3) evaluate current and projected supplementary techniques; (4) establish the existence of demand for what is proposed; (5) take steps to involve a representative group of ultimate users in systems design, and (6) carefully think through possible side effects on man and his world view.  Current proposals for community information utilities are examined in this framework, and the conclusion is drawn that society is not yet in a position to justify either the construction of an information utility in a prototype community or the acceptance of a policy in favor of its widespread implementation."}
{"DOCID": "2573", "TEXT": "Computer Programming as an Art"}
{"DOCID": "2574", "TEXT": "Multiple Exists from a Loop Using Neither GO TO nor Labels"}
{"DOCID": "2575", "TEXT": "The Best-Match Problem in Document Retrieval"}
{"DOCID": "2576", "TEXT": "A Simple Technique for Representing Strings in Fortran IV"}
{"DOCID": "2577", "TEXT": "An On-Site Data Management System Application in Field Archaeology"}
{"DOCID": "2578", "TEXT": "Self-stabilizing Systems in Spite of Distributed Control"}
{"DOCID": "2579", "TEXT": "Register Allocation Via Usage Counts: This paper introduces the notion of usage counts, shows how usage counts can be developed by algorithms that eliminate redundant computations, and describes how usage counts can provide the basis for register allocation.  The paper compares register allocation based on usage counts to other commonly used register allocation techniques, and presents evidence which shows that the usage count technique is significantly better than these other techniques."}
{"DOCID": "2580", "TEXT": "A Method for Composing Simple Traditional Music by Computer: A method is described for composing musical rounds by computer.  This method uses some music theory plus additional heuristics.  Fundamental to the method is a set of productions together with sets of applicability rules and weight rules which operate on the productions deciding when and to what extent they are available for use.  Several rounds generated by the computer implementation of the method are presented.  Generally, the resultant music sounds mediocre to the professional although usually pleasing to the layman.  It appears that full-blown music theory is not needed for rounds--all the hardware required for structural levels is not necessary for these pieces. The author has tried to address both musicians and computer scientists."}
{"DOCID": "2581", "TEXT": "A Locally-Organized Parser for Spoken Input: This paper describes LPARS, a locally-organized parsing system, designed for use in a continuous speech recognizer.  LPARS processes a string of phonemes which contains ambiguity and error. The system is locally-organized in the sense that it builds local parse structures from reliable word candidates recognized anywhere in an input utterance.  These local structures are used as \"islands of reliability\" to guide the search for more highly garbled words which might complete the utterance."}
{"DOCID": "2582", "TEXT": "Improving Locality by Critical Working Sets: A new approach to program locality improvement via restructuring is described.  The method is particularly suited to those systems where primary memory is managed according to a working set strategy. It is based on the concept of critical working set, a working set which does not contain the next memory reference.  The data the method operates upon are extracted from a trace of the program to be restructured. It is shown that, except in some special cases, the method is not optimum.  However, the experimental results obtained by using the method to restructure an interactive text editor and the file system module of an operating system have shown its substantial superiority over the other methods proposed in the literature."}
{"DOCID": "2583", "TEXT": "Guidelines for Humanizing Computerized Information Systems: A Report from Stanley House"}
{"DOCID": "2584", "TEXT": "Enumerating Full-Time Programmers: Data from the 1970 Census and the Department of Labor's Area Wage Surveys are used to derive estimates of the number of full-time programmers employed during the years 1969 through 1973.  The 1973 figure of 180,000 is considerably less than suggested in earlier reports.  It is recommended that educational administrators consider whether the many courses aimed at training programmers are justified on a vocational basis."}
{"DOCID": "2585", "TEXT": "Efficient Implementation of a Variable Projection Algorithm for Nonlinear Least Squares Problems (Errata)"}
{"DOCID": "2586", "TEXT": "Adapting Optimal Code Generation for Arithmetic Expressions to the Instruction Sets Available on Present-Day Computers (Errata)"}
{"DOCID": "2587", "TEXT": "On the Construction of a Representative Synthetic Workload (Errata)"}
{"DOCID": "2588", "TEXT": "Rosenbrock Function Minimization (Algorithm R450)"}
{"DOCID": "2589", "TEXT": "A Computer Routine for Quadratic and Linear Programming Problems (Algorithm R431)"}
{"DOCID": "2590", "TEXT": "Hypergeometric (Algorithm C191)"}
{"DOCID": "2591", "TEXT": "Numerical Inversion of Laplace Transform (Algorithm A486)"}
{"DOCID": "2592", "TEXT": "On Generation of Test Problems for Linear Programming Codes: Users of linear programming computer codes have realized the necessity of evaluating the capacity, effectiveness, and accuracy of the solutions provided by such codes.  Large scale linear programming codes at most installations are assumed to be generating correct solutions without ever having been \"bench-marked\" by test problems with known solutions.  The reason for this failure to adequately test the codes is that rarely are there large problems with known solutions readily available.  This paper presents a theoretical justification and an illustrative implementation of a method for generating linear programming test problems with known solutions.  The method permits the generation of test problems that are of arbitrary size and have a wide range of numerical characteristics."}
{"DOCID": "2593", "TEXT": "A Back-end Computer for Data Base Management: It is proposed that the data base management function be placed on a dedicated back-end computer which accepts commands (in a relatively high level language such as the CODASYL Data Base Task Group, April 1971 Report) from a host computer, accesses the data base on secondary storage, and returns results. The advantages of such a configuration are discussed.  An experimental implementation, called the experimental Data Management System, XDMS, is described and certain conclusions about the back-end approach are drawn from this implementation."}
{"DOCID": "2594", "TEXT": "Structured Data Structures: Programming systems which permit arbitrary linked list structures enable the user to create complicated structures without sufficient protection. Deletions can result in unreachable data elements, and there is no guarantee that additions will be performed properly.  To remedy this situation, this paper proposes a gauge which provides for the creation of a restricted class of data structures but ensures the correctness of the program.  This is accomplished by an explicit structure declaration facility, a restriction on the permissible operations, and execution-time checks."}
{"DOCID": "2595", "TEXT": "A Note on the Calculation Working Set Size: Finite-length reference string of arbitrary structure are considered, and an exact expression for average working set size in terms of \"corrected\" interreference interval statistics is derived. An example is discussed; upper and lower bounds are obtained; and the average working set size function is shown to be efficiently obtained for a set of page sizes, in a single pass of the reference string. This work follows the developments of a paper by Denning and Schwartz, who consider infinite-length reference strings which satisfy certain statistical properties and who derive an expression relating the asymptotic average working set size to the asymptotic missing page rate function under working set replacement."}
{"DOCID": "2596", "TEXT": "A Weighted Buddy Method for Dynamic Storage Allocation: An extension of the buddy method, called the weighted buddy method, for dynamic storage allocation is presented.  The weighted buddy method allows block sizes of 2^k and 3(2^k), whereas the original buddy method allowed only block sizes of 2^k. This extension is achieved at an additional cost of only two bits per block.  Simulation results are presented which compare this method with the buddy method.  These results indicate that for a uniform request distribution, the buddy system has less total memory fragmentation than the weighted buddy algorithm.  However, the total fragmentation is smaller for the weighted buddy method when the requests are for exponentially distributed block sizes."}
{"DOCID": "2597", "TEXT": "Monitors: An Operating System Structuring Concept: This paper develops Brinch-Hansen's concept of a monitor as a method of structuring an operating system.  It introduces a form of synchronization, describes a possible method of implementation in terms of semaphores and gives a suitable proof rule.  Illustrative examples include a single resource scheduler, a bounded buffer, an alarm clock, a buffer pool, a disk head optimizer, and a version of the problem of readers and writers."}
{"DOCID": "2598", "TEXT": "Extending the Information Theory Approach to Converting Limited-Entry Decision Tables to Computer Programs: This paper modifies an earlier algorithm for converting decision tables into flowcharts which minimize subsequent execution time when compiled into a computer program.  The algorithms considered in this paper perform limited search and, accordingly, do not necessarily result in globally optimal solutions.  However, the greater search effort needed to obtain a globally optimal solution for complex decision tables is usually not justified by sufficient savings in execution time.  There is an analogy between the problem of converting decision tables into efficient flowcharts and the well-understood problem in information theory of noiseless coding.  The results of the noiseless coding literature are used to explore the limitations of algorithms used to solve the decision table problem.  The analogy between the two problems is also used to develop improvements to the information algorithm in extending the depth of search under certain conditions and in proposing additional conditions to be added to the decision table.  Finally, the information algorithm is compared with an algorithm proposed in a recent paper by Verhelst."}
{"DOCID": "2599", "TEXT": "First Order Approximation to the Optimum Checkpoint Interval"}
{"DOCID": "2600", "TEXT": "Computation of g-Splines via a Factorization Method [E2] (Algorithm A485)"}
{"DOCID": "2601", "TEXT": "Evaluation of the Modified Bessel Functions K0(Z) and K1(Z) for Complex Arguments [S17] (Algorithm A484)"}
{"DOCID": "2602", "TEXT": "Masked Three-Dimensional Plot Program with Rotations [J6] (Algorithm A483)"}
{"DOCID": "2603", "TEXT": "The Equivalence of Reducing Transition Languages and Deterministic Languages: The class of reducing transition languages introduced by Eickel, Paul, Bauer, and Samelson was shown by Morris to be a proper superclass of the simple precedence languages.  In this paper this result is extended, showing that, in fact, the first class is equivalent to the class of deterministic context free languages."}
{"DOCID": "2604", "TEXT": "An Interactive Graphic Display for Region Partitioning by Linear Programming: Using linear programming, an interactive graphic display system has been implemented to solve the region design problem of partitioning a region into N nonoverlapping subregions in such a way that their areas are in specified proportions and that the total cost of servicing them is a minimum.  In a conversational manner, a user can easily obtain different partitionings by specifying and modifying the boundary, the service centers' locations, the area proportions, and the cost functions.  Examples are included."}
{"DOCID": "2605", "TEXT": "A Precise Numerical Analysis Program: A description is given of a program for computing the solution to a small number of standard numerical analysis problems to any specified accuracy, up to a limit of 2000 correct decimal places. Each computed number is bounded in an interval with a multiple precision midpoint.  Arithmetic operations involving these numbers are executed according to interval arithmetic concepts, with non-significant digits automatically discarded.  Details are supplied of problem specification and problem computation."}
{"DOCID": "2606", "TEXT": "A New Integration Algorithm for Ordinary Differential Equations Based on Continued Fraction Approximations: A new integration algorithm is found, and an implementation is compared with other programmed algorithms.  The new algorithm is a step-by-step procedure for solving the initial value problem in ordinary differential equations.  It is designed to approximate poles of small integer order in the solutions of the differential equations by continued fractions obtained by manipulating the sums of truncated Taylor series expansions.  The new method is compared with Gragg-Bulirsh-Stoer, and the Taylor series method. The Taylor series method and the new method are shown to be superior in speed and accuracy, while the new method is shown to be most superior when the solution is required near a singularity.  The new method can finally be seen to pass automatically through singularities where all the other methods which are discussed will have failed."}
{"DOCID": "2607", "TEXT": "A Problem-List of Issues Concerning Computers and Public Policy"}
{"DOCID": "2608", "TEXT": "Recurrence Relations for the Fresnel Integral and Similar Integrals"}
{"DOCID": "2609", "TEXT": "Interpolation with Rounded Ramp Functions: A new interpolation function is introduced. It has infinitely many continuous derivatives and is a composition of ramp functions with smoothed bends called Rounded Ramp Functions.  How the interpolation function can be extended to more than one variable is shown.  An efficient Fortran program is given by which the interpolation function can be obtained for a given point set."}
{"DOCID": "2610", "TEXT": "Gauss Harmonic Interpolation Formulas: Let R be an open, bounded, simply connected region in the (x,y)-plane and let (x*,y*) be a point in R.  Assuming R is starlike with respect to (x*,y*), we discuss a method for computing Gauss harmonic interpolation formulas for R and the point (x*,y*). Such formulas approximate a harmonic function at (x*,y*) in terms of a linear combination of its values at certain selected points on the boundary of R.  Such formulas are useful for approximating the solution of the Dirichlet problem for R."}
{"DOCID": "2611", "TEXT": "The Complex Method for Constrained Optimization (Algorithm R454)"}
{"DOCID": "2612", "TEXT": "Rosenbrock Function Minimization (Algorithm R450)"}
{"DOCID": "2613", "TEXT": "Transitivity Sets [G7] (Algorithm A482)"}
{"DOCID": "2614", "TEXT": "Arrow to Precedence Network Transformation [H] (Algorithm A481)"}
{"DOCID": "2615", "TEXT": "Procedures for computing Smoothing and Interpolating Natural Splines [E1] (Algorithm A480)"}
{"DOCID": "2616", "TEXT": "On the Conversion of Programs to Decision Tables: Method and Objectives: The problems of converting programs to decision tables are investigated.  Objectives of these conversions are mainly program debugging and optimization in practice.  Extensions to the theory of computation and computability are suggested."}
{"DOCID": "2617", "TEXT": "A Note on Subexpression Ordering in the Evaluation of Arithmetic Expressions"}
{"DOCID": "2618", "TEXT": "A New Solution of Dijkstra's Concurrent Programming Problem: A simple solution to the mutual exclusion problem is presented which allows the system to continue to operate despite the failure of any individual component."}
{"DOCID": "2619", "TEXT": "Graph Coloring Conditions for the Existence of Solutions to the Timetable Problem: A necessary and sufficient condition is presented for the existence of a solution to the Gotlieb class-teacher timetable problem.  Several relationships are established between the class-teacher timetable problem and graphs with preconditions.  These preconditions place additional restrictions on the coloration of a graph.  The preconditions correspond to the unavailability constraints and preassigned meetings in the class-teacher timetable problem.  Using some recent results that convert graphs with preconditions to graphs without them, it is shown that the existence of a coloration of a graph is the required necessary and sufficient condition."}
{"DOCID": "2620", "TEXT": "Execution Time Requirements for Encipherment Programs: Although encipherment has often been discussed as a means to protect computer data, its costs are not well established.  Five experiments were conducted to measure the cpu time on a CDC 6400 required by additive ciphers programmed both in assembly language and in Fortran: a \"null transformation\" to measure the time to move data without encipherment; encipherment with one-word key; encipherment with a 125-word key; double key encipherment; and encipherment using a pseudo random key.  The results were analyzed for consistency over 100 runs, and the effects of constant and intermittent errors were considered. Timing rates for assembly language encipherment ranged from 498,800 characters per second for a pseudo random key cipher to 2,092,000 characters per second for a constant one-word key cipher.  The latter is almost equivalent to the rate required simply to move data without encipherment.  Fortran tests required over four times as much cpu time.  This paper introduces the idea on enciphering time coefficient the ratio of enciphering time to the time taken to fetch and store data without encipherment."}
{"DOCID": "2621", "TEXT": "A High Security Log-in Procedure: The protection of time sharing systems from unauthorized users is often achieved by the use of passwords.  By using one-way ciphers to code the passwords, the risks involved with storing the passwords in the computer can be avoided.  We discuss the selection of a suitable one-way cipher and suggest that for this purpose polynomials over a prime modulus are superior to one-way ciphers derived from Sannon codes."}
{"DOCID": "2622", "TEXT": "A User Authentication Scheme Not Requiring Secrecy in the Computer: In many computer operating systems a user authenticates himself by entering a secret password known solely to himself and the system.  The system compares this password with one recorded in a Password Table which is available to only the authentication program.  The integrity of the system depends on keeping the table secret.  In this paper a password scheme is presented which does not require secrecy in the computer.    All aspects of the system, including all relevant code and data bases, may be known by anyone attempting to intrude.  The scheme is based on using a function H which the would-be intruder is unable to invert.  This function is applied to the user's password and the result compared to a table entry, a match being interpreted as authentication of the user.  The intruder may know all about H and have access to the table, but he can penetrate the system only if he can invert H to determine an input that produces a given output.  This paper discusses issues surrounding selection of a suitable H.  Two different plausible arguments are given that penetration would be exceedingly difficult, and it is then argued that more rigorous results are unlikely.  Finally, some human engineering problems relating to the scheme are discussed."}
{"DOCID": "2623", "TEXT": "A New Technique for Compression and Storage of Data: The widespread tendency toward storage of large programs and blocks off text has produced a need for efficient methods of compressing and storing data.  This paper describes techniques that can, in most cases, decrease storage size by a factor of from two to four.  The techniques involve special handling of leading and trailing blanks, and the encoding of other symbols in groups of fixed size as unique fixed point numbers.  The efficiency of the system is considered and pertinent statistics are given and compared with statistics for other information coding techniques."}
{"DOCID": "2624", "TEXT": "Formal Requirements for Virtualizable Third Generation Architectures: Virtual machine systems have been implemented on a limited number of third generation computer systems, e.g. CP-67 on the IBM 360/67.  From previous empirical studies, it is known that certain third generation computer systems, e.g. the DEC PDP-10, cannot support a virtual machine system.  In this paper, model of a third-generation-like computer system is developed.  Formal techniques are used to derive precise sufficient conditions to test whether such an architecture can support virtual machines."}
{"DOCID": "2625", "TEXT": "Capability-Based Addressing: Various addressing schemes making use of segment tables are examined.  The inadequacies of these schemes when dealing with shared addresses are explained. These inadequacies are traced to the lack of an efficient absolute address for objects in these systems.  The direct use of a capability as an address is shown to overcome these difficulties because it provides the needed absolute address. Implementation of capability-based addressing is discussed. It is predicted that the use of tags to identify capabilities will dominate.  A hardware address translation scheme which never requires the modification of the representation of capabilities is suggested. The scheme uses a main memory hash table for obtaining a segment's location in main memory given its unique code.  The hash table is avoided for recently accessed segments by means of a set of associative registers.  A computer using capability-based addressing may be substantially superior to present systems on the basis of protection, simplicity of programming conventions, and efficient implementation."}
{"DOCID": "2626", "TEXT": "Protection and the Control of Information Sharing in Multics: The design of mechanisms to control the sharing of information in the Multics system is described. Five design principles help provide insight into the tradeoffs among different possible designs.  The key mechanisms described include access control lists, hierarchical control of access specifications, identification and authentication of users, and primary memory protection.  The paper ends with a discussion of several known weaknesses in the current protection mechanism design."}
{"DOCID": "2627", "TEXT": "Scheduling Independent Tasks to Reduce Mean Finishing Time: Sequencing to minimize mean finishing time (or mean time in system) is not only desirable to the user, but it also tends to minimize at each point in time the storage required to hold incomplete tasks.  In this paper a deterministic model of independent tasks is introduced and new results are derived which extend and generalize the algorithms known for minimizing mean finishing time.  In addition to presenting and analyzing new algorithms it is shown that the most general mean-finishing-time problem for independent tasks is polynomial complete, and hence unlikely to admit of a non-enumerative solution"}
{"DOCID": "2628", "TEXT": "Minimal-Total-Processing Time Drum and Disk Scheduling Disciplines: This article investigates the application of minimal-total-processing-time (MTPT) scheduling disciplines to rotating storage units when random arrival of requests is allowed.  Fixed-head drum and moving-head drum and moving-head disk storage units are considered, and emphasis is placed on the relative merits of the MTPT scheduling discipline with respect to the shortest-latency-time-first (SLTF) scheduling discipline.  The results of the simulation studies presented show that neither scheduling discipline is unconditionally superior to the other.  For most fixed-head drum applications, the SLTF discipline is preferable to MTPT, but for intra-cylinder disk scheduling the MTPT discipline offers a distinct advantage over the SLTF discipline.  The computational requirements of an algorithm that implements the MTPT scheduling discipline are shown to be comparable to SLTF algorithms. In both cases, the sorting procedure is the most time-consuming phase of the algorithm."}
{"DOCID": "2629", "TEXT": "The UNIX Time-Sharing system: UNIX is a general-purpose, multi-user, interactive operating system for the Digital Equipment Corporation PDP-11/40 and 11/45 computers.  It offers a number of features seldom found even in larger operating systems, including: (1) a hierarchical file system incorporating demountable volumes; (2) compatible file, device, and inter-process I/O; (3) the ability to initiate asynchronous processes; (4) system command language selectable on a per-user basis; and (5) over 100 subsystems including a dozen languages.This paper discusses the nature and implementation of the file system and of the user command interface."}
{"DOCID": "2630", "TEXT": "On Computing Sets of Shortest Paths in a Graph: Two algorithms are presented that construct the k shortest paths between every pair of vertices in a directed graph.  These algorithms generalize the Floyd algorithm and the Dantzig algorithm for finding the shortest path between every pair of vertices in a directed graph."}
{"DOCID": "2631", "TEXT": "An Information-Theoretic Approach to Text Searching in Direct Access Systems: Using direct access computer files of bibliographic information, an attempt is made to overcome one of the problems often associated with information retrieval, namely, the maintenance and use of large dictionaries, the greater part of which is used only infrequently.  A novel method is presented, which maps the hyperbolic frequency distribution.  This is more suited to implementation on storage devices. This method treats text as a string of characters rather than words bounded by spaces, and chooses subsets of strings such that their frequencies of occurrence are more even than those of word types.  The members of this subset are then used as index keys for retrieval. The rectangular distribution of key frequencies results in a much simplified file organization and promises considerable cost advantages."}
{"DOCID": "2632", "TEXT": "HYDRA: The Kernel of a Multiprocessor Operating System: This paper describes the design philosophy of HYDRA-the kernel of an operating system for C.mmp, the Carnegie-Mellon Multi-Mini-Processor.  This philosophy is realized through the introduction of a generalized notion of \"resource\", both physical and virtual, called an \"object\".  Mechanisms are presented for dealing with objects, including the creation of new types, specification of new operations applicable to a given type, sharing, and protection of any reference to a given object against improper application of any of the operations defined with respect to that type of object.  The mechanisms provide a coherent basis for extension of the system in two directions: the introduction of new facilities, and the creation of highly secure systems."}
{"DOCID": "2633", "TEXT": "Compact Representation of Contour Plots for Phone Line Transmission: Methods for the compact representation of contour plots are described and tested.  These are intended to reduce the cost of transmitting contour plots over phone lines.  We feel some of these methods could be used to transmit contour plots over voice grade phone lines."}
{"DOCID": "2634", "TEXT": "An Evaluation of Statistical Software in the Social Sciences: Several hundred college and university computer installations now offer various types of statistical packages for general use.  Among those most widely available are OSIRIS, SPSS, BMD, DATA-TEXT, and TSAR. In order to provide users with a basis for selection and use, tests were made for each of these systems, and the results are summarized as to cost and performance."}
{"DOCID": "2635", "TEXT": "Exact Probabilities for R X C Contingency Tables (Algorithm R434)"}
{"DOCID": "2636", "TEXT": "Generation of Random Correlated Normal Variables (Algorithm R425)"}
{"DOCID": "2637", "TEXT": "Hidden-Line Plotting Program (Algorithm R420)"}
{"DOCID": "2638", "TEXT": "Hidden-Line Plotting Program (Algorithm R420)"}
{"DOCID": "2639", "TEXT": "Calculation of Fourier Integrals (Algorithm R418)"}
{"DOCID": "2640", "TEXT": "Modified Havie Integration (Algorithm R400)"}
{"DOCID": "2641", "TEXT": "A Minimal Spanning Tree clustering Method [Z] (Algorithm A479)"}
{"DOCID": "2642", "TEXT": "Solution of an Overdetermined System of Equations in the L1 Norm [F4] (Algorithm A478)"}
{"DOCID": "2643", "TEXT": "The Minimization of Spatially-Multiplexed Character Sets: The paper describes a technique for compacting character sets in a digital computer while retaining fast access to individual bits.  It considers the problem of minimizing the storage needed to contain such tables.  Reduction techniques are developed, and the problem is shown to reduce to a covering problem."}
{"DOCID": "2644", "TEXT": "A Theorem-Proving Language for Experimentation: Because of the large number of strategies and inference rules presently under consideration in automated theorem proving, there is a need for developing a language especially oriented toward automated theorem proving.  This paper discusses some of the features and instructions of this language.  The use of this language permits easy extension of automated theorem-proving programs to include new strategies and/or new inference rules.  Such extend ability will permit general experimentation with the various alternative systems."}
{"DOCID": "2645", "TEXT": "Two Languages for Estimating Program Efficiency: Two languages enabling their users to estimate the efficiency of computer programs are presented. The program whose efficiency one wishes to estimate is written in the first language, a go-to-less programming language which includes most of the features of Algol 60.  The second language consists of interactive commands enabling its users to provide additional information about the program written in the first language and to output results estimating its efficiency. Processors for the two languages are also described.  The first processor is a syntax-directed translator which compiles a program into a symbolic formula representing the execution time for that program. The sound processor is a set of procedures for that program.  The second processor is a set of procedures for algebraic manipulation which can be called by the user to operate on the formula produced by the first processor.  Examples of the usage of the two languages are included.  The limitations of the present system, its relation to Knuth's work on the analysis of algorithms, and some of the directions for further research are also discussed."}
{"DOCID": "2646", "TEXT": "A Model for Masking Rotational Latency by Dynamic Disk Allocation: This paper presents the background and algorithms for masking the rotational latency of a disk or drum.  It discusses the anticipatory input and output of blocks of data to buffer and primary memories for a mono-programmed computer system.  A basic permutation algorithm and several variations are given. Because of the anticipatory nature of the I/O scheduling, these algorithms are restricted to classes of programs with predictable behavior.  While the methods are not restricted to numerical computations, matrix and partial differential equation methods are typical examples of their use.  It is shown that latency may be masked using a small amount of buffer memory.  The methods discussed are independent of the overall size of the data base being considered."}
{"DOCID": "2647", "TEXT": "More on Algorithms that Reveal Properties of Floating Point Arithmetic Units"}
{"DOCID": "2648", "TEXT": "A Design for a Number Theory Package with an Optimized Trial Division routine: A number theory package is described which uses doubly linked list structures for storing multiprecise integers.  The package has been coded in IBM's Basic Assembly Language and makes heavy use of the macro language and conditional assembly.  An optimally coded trial division routine is also described which can be used to determine the unique factorization of large integers."}
{"DOCID": "2649", "TEXT": "On the Distributions of Significant Digits and Roundoff Errors: Generalized logarithmic law is derived for the distribution of the first t significant digits of a random digital integer.  This result is then used to determine the distribution of the roundoff errors in floating-point operations, which is a mixture of uniform and reciprocal distributions."}
{"DOCID": "2650", "TEXT": "Order-n Correction for Regular Languages: A method is presented for calculating a string B, belonging to a given regular language L, which is \"nearest\" (in number of edit operations) to a given input string a.  B is viewed as a reasonable \"correction\" for the possibly erroneous string a, where a was originally intended to be a string of L. The calculation of B by the method presented requires time proportional to |a|, the number of characters in a.  The method should find applications in information retrieval, artificial intelligence, and spelling correction systems."}
{"DOCID": "2651", "TEXT": "The Treatment of Data Types in EL1: In constructing a general purpose programming language, a key issue is providing a sufficient set of data types and associated operations in a manner that permits both natural problem-oriented notation and efficient implementation.  The EL1 language contains a number of features specifically designed to simultaneously satisfy both requirements.  The resulting treatment of data types includes provision for programmer-defined data types data types and generic routines, programmer control over type conversion, and very flexible data type behavior, in a context that allows efficient compiled code and compact data representation."}
{"DOCID": "2652", "TEXT": "Reduction of Compilation Costs Through Language Contraction: Programming languages tailored to particular groups of users can often be constructed by removing unwanted features from a general purpose language.  This paper describes the use of simulation techniques to predict the savings in compilation cost achievable by such an approach.  The results suggest a function which describes the effect of changes in the power of a language on the compilation cost of an algorithm expressed in that language: when features not actually used by the algorithm are removed from the language, the cost of compiling the algorithm decreases moderately, but when features that are needed are removed, the compilation cost increases sharply."}
{"DOCID": "2653", "TEXT": "Solution of the Transcendental Equation w*exp(x)=x (Algorithm R443)"}
{"DOCID": "2654", "TEXT": "Generator of Set-Partitions to Exactly R Subsets [G7] (Algorithm A477)"}
{"DOCID": "2655", "TEXT": "Six Subprograms for Curve Fitting Using Splines Under Tension [E2] (Algorithm A476)"}
{"DOCID": "2656", "TEXT": "Scalar- and Planar- Valued Curve Fitting Using Splines Under Tension: The spline under tension was introduced by Schweikert in an attempt to imitate cubic splines but avoid the spurious critical points they induce. The defining equations are presented here, together with an efficient method for determining the necessary parameters and computing the resultant spline. The standard scalar-valued curve fitting problem is discussed, as well as the fitting of open and closed curves in the plane.  The use of these curves and the importance of the tension in the fitting of contour lines are mentioned as application."}
{"DOCID": "2657", "TEXT": "An Improved Program-Synthesizing Algorithm and Its Correctness: An improved program-synthesizing algorithm based on the algorithm proposed by Waldinger and Lee in 1969 is given.  In the old algorithm, the program-synthesizing problem is translated into a theorem-proving problem, and a program is obtained by analyzing a proof. For the improved algorithm, the analysis is not necessary, and a program is obtained as soon as the proof is completed.  This is achieved by using a modified variable tracing mechanism invented by Green in 1969.  The correctness of the improved algorithm is also proved; i.e. the program thus obtained always satisfies the specification."}
{"DOCID": "2658", "TEXT": "An Alternative Approach to Mutual Recursion in Algol 60 Using Restricted Compilers"}
{"DOCID": "2659", "TEXT": "Some Remarks on Lookup of Structured Variables"}
{"DOCID": "2660", "TEXT": "Addendum to M. L. Patrick Paper"}
{"DOCID": "2661", "TEXT": "Ideal Teaching Machines-A Solution to the Pedagogic Language Problem"}
{"DOCID": "2662", "TEXT": "Graduate Education: The Ph.D. Glut: Response and Rebuttal"}
{"DOCID": "2663", "TEXT": "A Study of Computer Use in a Graduate School of Business"}
{"DOCID": "2664", "TEXT": "Parallelism in Tape-Sorting: Two methods for employing parallelism in tape-sorting are presented.  Method A is the natural way to use parallelism. Method B is new.  Both approximately achieve the goal of reducing the processing time by a divisor which is the number of processors."}
{"DOCID": "2665", "TEXT": "Copying List Structures Using Bounded Workspace: Two new algorithms are presented for list structure copying using bounded workspace.  The first, of primarily theoretical interest, shows that without cell tag bits the task can be performed in time n^2.  The second algorithm, assuming one tag bit in each cell, delivers attractive practical speed. Any noncyclic structure is copied in linear speed, while cyclic structures are copied in average time less than nlogn.  No foreknowledge of cycle absence is necessary to achieve linear speed.  A variation of the second algorithm solves an open problem concerning list structure marking.  That result demonstrates that marking can be done in average time nlogn without the aid of supplemental tag bits or stacks."}
{"DOCID": "2666", "TEXT": "On Lions' Counter Example for Gotlieb's Method for the Construction of School Timetables: The timetable problem is an essentially discrete problem. Although the discrete problem may have no feasible solution, there may exist a solution to the equivalent continuous problem.  An example is given, for which the nondiscrete solution can be interpreted as a set of timetables, differing from week to week, which together satisfy the long-term requirements of the timetable problem."}
{"DOCID": "2667", "TEXT": "Execution Characteristics of Programs in a Page-on-Demand System: Data are presented which show the execution characteristics of two types of commonly used programs in a large-scale, time-shared computer system.  A software monitoring facility built into the supervisor was used for data collection during normal system operation. These data were analyzed, and results of this analysis are presented for a Fortran compiler and an interactive line file editor.  Probability distribution functions and other data are given for such things as CPU intervals, I/O intervals, and the number of such intervals during execution.  Empirical distributions are compared with simple theoretical distributions (exponential, hyperexponential, and geometric). Other data show paging characteristics of tasks as a function of the number of pages those tasks have in core."}
{"DOCID": "2668", "TEXT": "Computation of Page Fault Probability from Program Transition Diagram: An algorithm is given for calculating page fault probability in a virtual memory system operating under demand paging with various memory sizes and replacement rules.  A first order Markov model of program behavior is assumed, and a representation of the system based on memory states, control states, and memory substates is presented.  The algorithm is general in the sense that the page fault probabilities can be calculated for nonpredictive replacement rules applied to any program represented by a one-step Markov chain.  A detailed example is given to illustrate the algorithm for Random and Least Recently Used (LRU) replacement rules."}
{"DOCID": "2669", "TEXT": "A Simple Linear Model of Demand Paging Performance: Predicting the performance of a proposed automatically managed multilevel memory system requires a model of the patterns by which programs refer to the information stored in the memory.  Some recent experimental measurements on the Multics virtual memory suggest that, for rough approximations, a remarkably simple program reference model will suffice.  The simple model combines the effect of the information reference pattern with the effect of the automatic management algorithm to produce a single, composite statement: the mean number of memory references between paging exceptions increases linearly with the size of the paging memory.  The resulting model is easy to manipulate, and is applicable to such diverse problems as choosing an optimum size for a paging memory, arranging for reproducible memory usage charges, and estimating the amount of core memory sharing."}
{"DOCID": "2670", "TEXT": "Efficient Implementation of a Variable Projection Algorithm for Nonlinear Least Squares Problems: Nonlinear least squares frequently arise for which the variables to be solved for can be separated into a linear and a nonlinear part.  A variable projection algorithm has been developed recently which is designed to take advantage of the structure of a problem whose variables separate in this way.  This paper gives a slightly more efficient and slightly more general version of this algorithm than has appeared earlier."}
{"DOCID": "2671", "TEXT": "A Note on a Combinatorial Problem of Burnett and Coffman"}
{"DOCID": "2672", "TEXT": "Emotional Content Considered Dangerous"}
{"DOCID": "2673", "TEXT": "Quadratic Search for Hash Tables of Size p^n"}
{"DOCID": "2674", "TEXT": "Scan Conversion Algorithms for a Cell Organized Raster Display: Raster scan computer graphics with \"real time\" character generators have previously been limited to alphanumeric characters.  A display has been described which extends the capabilities of this organization to include general graphics.  Two fundamentally different scan conversion algorithms which have been developed to support this display are presented.  One is most suitable to non-interactive applications and the other to interactive applications.  The algorithms were implemented in Fortran on the CDC 6400 computer.  Results obtained from the implementations show that the noninteractive algorithms can significantly reduce display file storage requirements at little cost in execution time over that of a conventional raster display.  The interactive algorithm can improve response time and reduce storage requirements."}
{"DOCID": "2675", "TEXT": "A Computer Routine for Quadratic and Linear Programming Problems (Algorithm R431)"}
{"DOCID": "2676", "TEXT": "Zeros of a Complex Polynomial (Algorithm R419)"}
{"DOCID": "2677", "TEXT": "Incomplete Beta Ratio (Algorithm R179)"}
{"DOCID": "2678", "TEXT": "Visible Surface Plotting Program [J6] (Algorithm A475)"}
{"DOCID": "2679", "TEXT": "Some Performance Tests of \"quicksort\" and Descendants: Detailed performance evaluations are presented for six ACM algorithms: quicksort (No. 64), Shellsort (No. 201), stringsort (No. 207), \"TREESORT3\" (No. 245), quickersort (No. 271), and qsort (No. 402).  Algorithms 271 and 402 are refinements of algorithm 64, and all three are discussed in some detail. The evidence given here demonstrates that qsort (No. 402) requires many more comparisons than its author claims.  Of all these algorithms, quickersort requires the fewest comparisons to sort random arrays."}
{"DOCID": "2680", "TEXT": "Optimal Space Allocation on Disk Storage Devices: When the amount of space required for file storage exceeds the amount which can be kept on-line, decisions must be made as to which files are to be permanently resident and which mountable.  These decisions will affect the number of mount requests issued to the operators.  This is often a bottleneck in a computing facility, and reducing the number of mounts thus decreases turnaround time.  An optimization model for the assignment of files to disk packs, and packs to either resident or nonresident status is presented. Heuristics are suggested for those cases in which it is inefficient to compute the actual optimum."}
{"DOCID": "2681", "TEXT": "Dynamic Memory Repacking: A probabilistic model of a multiprogramming system is exercised in order to determine the conditions under which the dynamic repacking of main memory is beneficial. An expression is derived for the maximum interference that a repacking process may introduce before the original performance of the system is degraded.  Alternative approaches to repacking are discussed, and the operating conditions that lead to improved system throughput through repacking are delineated."}
{"DOCID": "2682", "TEXT": "On the Construction of a Representative Synthetic Workload: A general method of constructing a drive workload representative of a real workload is described. The real workload is characterized by its demands on the various system resources.  These characteristics of the real workload are obtained from the system accounting data. The characteristics of the drive workload are determined by matching the joint probability density of the real workload with that of the drive workload.  The drive workload is realized by using a synthetic program in which the characteristics can be varied by varying the appropriate parameters. Calibration experiments are conducted to determine expressions relating the synthetic program parameters with the workload characteristics.  The general method is applied to the case of two variables, cpu seconds and number of I/O activities; and synthetic workload with 88 jobs is constructed to represent a month's workload consisting of about 6000 jobs."}
{"DOCID": "2683", "TEXT": "The Synthesis of Loop Predicates: Current methods for mechanical program verification require a complete predicate specification on each loop.  Because this is tedious and error prone, producing a program with complete, correct predicates is reasonably difficult and would be facilitated by machine assistance.  This paper discusses techniques for mechanically synthesizing loop predicates.  Two classes of techniques are considered: (1) heuristic methods which derive loop predicates from boundary conditions and/or partially specified inductive assertions: (2) extraction methods which use input predicates and appropriate weak interpretations to obtain certain classes of loop predicates by an evaluation on the weak interpretation."}
{"DOCID": "2684", "TEXT": "Production Systems: or Can We Do Better than BNF?: Since the development of BNF, the definition of the syntax of programming languages has been almost universally associated with context-free requirements. Yet numerous interesting and difficult issues in syntax stem from the context-sensitive requirements, notably the compatibility between the declaration of an identifier and its uses, the correspondence between actual and formal parameters, and issues arising from block structure.  This paper explores the use of a formal notation called Production Systems in providing a readable and complete formal definition of syntax.  As a practical illustration, a small but significant subset of PL/I is considered.  A more detailed presentation, as well as the application to define abstract syntax and translations between languages, is given in a previous paper by the author."}
{"DOCID": "2685", "TEXT": "The Parallel Execution of DO Loops: Methods are developed for the parallel execution of different iterations of a DO loop.  Both asynchronous multiprocessor computers and array computers are considered.  Practical application to the design of compilers for such computers is discussed."}
{"DOCID": "2686", "TEXT": "An Approximate Method for Generating Asymmetric Random Variables: Tukey's lambda distribution is generalized to provide an algorithm for generating values of unimodal asymmetric random variables.  This algorithm has the same advantages as the symmetric random variable generator previously given by the authors, except that the addition of another parameter complicates the problem of finding the parameter values to fit a distribution."}
{"DOCID": "2687", "TEXT": "A Cell Organized Raster Display for Line Drawings: Raster scan computer graphics displays with \"real time\" character generators have previously been limited to alphanumeric characters.  A display is described which extends the capabilities of this organization to include general graphics.  The feasibility of such a display is shown by deriving the minimum number of patterns required in the read only memory of the character generator to synthesize an arbitrary line.  The synthesis process does not compromise picture quality since the resulting dot patterns are identical with those of a conventional raster display.  Furthermore, the time constraints of a raster display are shown to be satisfied for a typical design for very complex line drawings."}
{"DOCID": "2688", "TEXT": "Attribute Based File Organization in a Paged Memory Environment: The high cost of page accessing implies a need for more careful data organization in a paged memory than is typical of most inverted file and similar approaches to multi-key retrieval.  This article analyses that cost and proposes a method called multiple key hashing which attempts to minimize it. Since this approach is not always preferable to inversion, a combined method is described.  The exact specifications of this combination for a file with given data and traffic characteristics is formulated as a mathematical program.  The proposed heuristic solution to this program can often improve on a simple inversion technique by a factor of 2 or 3."}
{"DOCID": "2689", "TEXT": "A CRT Report Generating System"}
{"DOCID": "2690", "TEXT": "A Numbering Systems for Combinations"}
{"DOCID": "2691", "TEXT": "Comments on the Algorithms of Verhelst for the Conversion of Limited-Entry Decision Tables to Flowcharts"}
{"DOCID": "2692", "TEXT": "Reentrant Polygon Clipping: A new family of clipping algorithms is described. These algorithms are able to clip polygons against irregular convex plane-faced volumes in three dimensions, removing the parts of the polygon which lie outside the volume.  In two dimensions the algorithms permit clipping against irregular convex windows. Polygons to be clipped are represented as an ordered sequence of vertices without repetition of first and last, in marked contrast to representation as a collection of edges as was heretofore the common procedure.  Output polygons have an identical format, with new vertices introduced in sequence to describe any newly-cut edge or edges.  The algorithms easily handle the particularly difficult problem of detecting that a new vertex may be required at a corner of the clipping window.  The algorithms described achieve considerable simplicity by clipping separately against each clipping plane or window boundary.  Code capable of clipping the polygon against a single boundary is reentered to clip against subsequent boundaries. Each such reentrant stage of clipping need store only two vertex values and may begin its processing as soon as the first output vertex from the proceeding stage is ready.  Because the same code is reentered for clipping against subsequent boundaries, clipping against very complex window shapes is practical. For perspective applications in three dimentions, a six-plane truncated pyramid is chosen as the clipping volume.  The two additional planes parallel to the projection screen serve to limit the range of depth preserved through the projection.  A perspective projection method which provides for arbitrary view angles and depth of field in spite of simple fixed clipping planes is described.  This method is ideal for subsequent hidden-surface computations."}
{"DOCID": "2693", "TEXT": "Bivariate Interpolation and Smooth Surface Fitting Based on Local Procedures [E2] (Algorithm A474)"}
{"DOCID": "2694", "TEXT": "Computation of Legendre Series Coefficients [C6] (Algorithm A473)"}
{"DOCID": "2695", "TEXT": "Tridiagonalization by Permutations: Tridiagonalizing a matrix by similarity transformations is an important computational tool in numerical linear algebra. Consider the class of sparse matrices which can be tridiagonalized using only row and corresponding column permutations.  The advantages of using such a transformation include the absence of round-off errors and improved computation time when compared with standard transformations. A graph theoretic algorithm which examines an arbitrary n x n matrix and determines whether or not it can be permuted into tridiagonal form is given.  The algorithm requires no arithmetic while the number of comparisons, the number of assignments, and the number of increments are linear in n.  This compares very favorably with standard transformation methods. If the matrix is permutable into tridiagonal form, the algorithm gives the explicit tridiagonal form. Otherwise, early rejection will occur."}
{"DOCID": "2696", "TEXT": "A Method of Bivariate Interpolation and Smooth Surface Fitting Based on Local Procedures: A method is designed for interpolating values given at points of a rectangular grid in a plane by a smooth bivariate function z=z(x,Y).  The interpolating function is a bicubic polynomial in each cell of the rectangular grid.  Emphasis is an avoiding excessive undulation between given grid points. The proposed method is an extension of the method of univariate interpolation developed earlier by the author and is likewise based on local procedures."}
{"DOCID": "2697", "TEXT": "A Fast Method for Solving a Class of Tridiagonal Linear Systems: The solution of linear systems having real, symmetric, diagonally dominant,tridiagonal coefficient matrices with constant diagonals is considered.  It is proved that the diagonals of the LU decomposition converges when floating-point precision.  It is also proved that the computed LU decomposition converges when floating-point arithmetic is used and that the limits of the LU diagonals using floating point are roughly within machine precision of the limits using real arithmetic.  This fact is exploited to reduce the number of floating-point operations required to solve a linear system from 8n-7 to 5n+2k-3, where k is much less than n, the order of the matrix.  If the elements of the subdiagonals and superdiagonals are 1, then only 4n+2k-3 operations are needed.  The entire LU decomposition takes k words of storage, and considerable savings in array subscripting are achieved. Upper and lower bounds on k are obtained in terms of the ratio of the coefficient matrix diagonal constants and parameters of the floating-point number system.  Various generalizations of these results are discussed."}
{"DOCID": "2698", "TEXT": "Syntax-Directed Least-Errors Analysis for Context-Free Languages: A Practical Approach: A least-errors recognizer is developed informally using the well-known recognizer of Earley, along with elements of Bellman's dynamic programming. The analyzer takes a general class of context-free grammars as drivers, and any finite string as input. Recognition consists of a least-errors count for a corrected version of the input relative to the driver grammar. The algorithm design emphasizes practical aspects which help in programming it."}
{"DOCID": "2699", "TEXT": "Automatic Data Structure Choice in a Language of Very High Level: SETL is a set-theoretically oriented language of very high level whose repertoire of semantic objects includes finite sets, ordered n-tuples, and sets of ordered n-tuples usable as mappings.  This paper describes the structure of an optimizer for this language.  Among other methods of interest, the optimizer uses techniques which allow relations of inclusion and membership to be established, the domains and ranges of (tabulated) mappings to be estimated from above and below, and the single-valuedness of (tabulated) mappings to be proved.  Once facts of this kind have been established, automatic choice of data structures becomes possible. The methods employed are based upon, and extend, known techniques of data flow analysis."}
{"DOCID": "2700", "TEXT": "Reduction: A Method of Proving Properties of Parallel Programs: When proving that a parallel program has a given property it is often convenient to assume that a statement is indivisible, i.e. that the statement cannot be interleaved with the rest of the program. Here sufficient conditions are obtained to show that the assumption that a statement is indivisible can be relaxed and still preserve properties such as halting.  Thus correctness proofs of a parallel system can often be greatly simplified."}
{"DOCID": "2701", "TEXT": "A Fast and Usually Linear Algorithm for Global Flow Analysis (Abstract only--Complete paper JACM 23,1 January, 1976): A new algorithm for global flow analysis on reducible graphs is presented. The algorithm is shown to treat a very general class of function spaces. For a graph of e edges, the algorithm has a worst case time bound of O(e log e) function operations. It is also shown that in programming terms, the number of operations is proportional to e plus the number of exits from program loops.  Consequently a restriction to one-entry one-exit control structures linearity.  The algorithm can be extended to yet larger classes of function spaces and graphs by relaxing the time bound.  Examples are given of code improvement problems which can be solved using the algorithm."}
{"DOCID": "2702", "TEXT": "On the Complexity of LR(k) Testing: The problem of determining whether an arbitrary context-free grammar is a member of some easily parsed subclass of grammars such as the LR(k) grammars is considered.  The time complexity of this problem is analyzed both when k is considered to be a fixed integer and when k is considered to be a parameter of the test.  In the first case, it is shown that for every k there exists an O(n(k+2)) algorithm for testing the LR(k) property, where n is the size of the grammar in question.  On the other hand, if both k and the subject grammar are problem parameters, then the complexity of the problem depends very strongly on the representation chosen for k.  More specifically, it is shown that this problem is NP-complete when k is expressed in unary.  When k is expressed in binary the problem is complete for nondeterministic exponential time.  These results carry over to many other parameterized classes of grammars, such as the LL(k), strong LL(k), SLR(k), LC(k), and strong LC(k) grammars."}
{"DOCID": "2703", "TEXT": "The Intrinsically Exponential Complexity of the Circularity Problem for Attribute Grammars: Attribute grammars are an extension of context-free grammars devised by Knuth as a mechanism for including the semantics of a context-free language with the syntax of the language.  The circularity problem for a grammar is to determine whether the semantics for all possible sentences (programs) in fact will be well defined.  It is proved that this problem is, in general, computationally intractable. Specifically, it is shown that any deterministic algorithm which solves the problem must for infinitely many cases use an exponential amount of time.An improved version of Knuth's circularity testing algorithm is also given, which actually solves the problem within exponential time."}
{"DOCID": "2704", "TEXT": "Exception Handling: Issues and a Proposed Notation: This paper defines exception conditions, discusses the requirements exception handling language features must satisfy, and proposes some new language features for dealing with exceptions in an orderly and reliable way.  The proposed language features serve to highlight exception handling issues by showing how deficiencies in current approaches can be remedied."}
{"DOCID": "2705", "TEXT": "Programming Languages, Natural Languages, and Mathematics: Some social aspects of programming are illuminated through analogies with similar aspects of mathematics and natural languages.  The split between pure and applied mathematics is found similarly in programming. The development of natural languages toward flexion less, word-order based language types speaks for programming language design based on general, abstract constructs.  By analogy with incidents of the history of artificial, auxiliary languages it is suggested that Fortran and Cobol will remain dominant for a long time to come.  The most promising avenues for further work of wide influence are seen to be high quality program literature (i.e. programs) of general utility and studies of questions related to program style."}
{"DOCID": "2706", "TEXT": "A Note on the Set Basis Problem Related to the Compaction of Character Sets: This note discusses the reduction of the set basis problem to the clique cover problem."}
{"DOCID": "2707", "TEXT": "Backtrack Programming Techniques: The purpose of this paper is twofold.  First, a brief exposition of the general backtrack technique and its history is given.  Second, it is shown how the use of macros can considerably shorten the computation time in many cases.  In particular, this technique has allowed the solution of two previously open combinatorial problems, the computation of new terms in a well-known series, and the substantial reduction in computation time for the solution to another combinatorial problem."}
{"DOCID": "2708", "TEXT": "Practical Syntactic Error Recovery: This paper describes a recovery scheme for syntax errors which provides automatically-generated high quality recovery with good diagnostic information at relatively low cost. Previous recovery techniques are summarized and empirical comparisons are made.  Suggestions for further research on this topic conclude the paper."}
{"DOCID": "2709", "TEXT": "A Genealogy of Control Structures: The issue of program control structures has had a history of heated controversy.  To put this issue on a solid footing, this paper reviews numerous theoretical results on control structures and explores their practical implications.  The classic result of Bohm and Jacopini on the theoretical completeness of if-then-else and while-do is discussed. Several recent ideas on control structures are then explored. These include a review of various other control structures, results on time/space limitations, and theorems relating the relative power of control structures under notions of equivalence.  In conclusion, the impact of theoretical results on the practicing programmer and the importance of one-in, one-out control structures as operational abstractions are discussed.  It is argued further that there is insufficient evidence to warrant more than if-then-else, while-do, and their variants."}
{"DOCID": "2710", "TEXT": "Specifying Queries as Relational Expressions: The SQUARE Data Sublanguage: This paper presents a data sublanguage called SQUARE, intended for use in ad hoc, interactive problem solving by non-computer specialists. SQUARE is based on the relational model of data, and is shown to be relationally complete; however, it avoids the quantifiers and bound variables required by languages based on the relational calculus.  Facilities for query, insertion, deletion, and update on tabular data bases are described.  A syntax is given, and suggestions are made for alternative syntaxes, including a syntax based on English key words for users with limited mathematical background."}
{"DOCID": "2711", "TEXT": "A Vector Space Model for Automatic Indexing: In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density.  An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstrating the usefulness of the model."}
{"DOCID": "2712", "TEXT": "Horner's Rule for the Evaluation of General Closed Queueing Networks: The solution of separable closed queueing networks requires the evaluation of homogeneous multinomial expressions.  The number of terms in those expressions grows combinatorially with the size of\u0019 the network such that a direct summation may become impractical.  An algorithm is given which does not show a combinatorial operation count.  The algorithm is based on a generalization of Horner's rule for polynomials.  It is also shown how mean queue size and throughput an be obtained at negligible extra cost once the normalization constant is evaluated."}
{"DOCID": "2713", "TEXT": "Remark on Stably Updating Mean and Standard Deviation of Data (Corrigendum)"}
{"DOCID": "2714", "TEXT": "Merging with Parallel Processors: Consider two linearly ordered sets A, B, |A|=m, |B|=n, m<=n, and p, p<=m, parallel processors working synchronously.  The paper presents an algorithm for merging A and B with the p parallel processors, which requires at most 2[log2 (2m+1)]+[3m/p] + [m/p][log2 (n/m)] steps.  If n = (2^B)m (B an integer), the algorithm requires at most 2[log2 (m+1)] + [m/p](2+B) steps.  In the case where m and n are of the same order of magnitude, i.e. n=km with k being a constant, the algorithm requires 2[log2 (m+1)] + [m/p](3+k) steps.  These performances compare very favorably with the previous best parallel merging algorithm, Batcher's algorithm, which requires n/p + ((m+n)/2p)log2 m steps in the general case and km/p + ((k+1)/2)(m/p)log2 m in the special case where n=km."}
{"DOCID": "2715", "TEXT": "Implementation of a Structured English Query Language: The relational model of data, the XRM Relational Memory System, and the SEQUEL language have been covered in previous papers and are reviewed. SEQUEL is a relational data sublanguages intended for the ad hoc interactive problem solving by non-computer specialists.  A version of SEQUEL that has been implemented in a prototype interpreter is described. The interpreter is designed to minimize the data accessing operations required to respond to an arbitrary query.  The optimization algorithms designed for this purpose are described."}
{"DOCID": "2716", "TEXT": "Optimizing the Performance of a Relational Algebra Database Interface: An approach for implementing a \"smart\" interface to support a relational view of data is proposed. The basic idea is to employ automatic programming techniques so that the interface analyzes and efficiently refines the high level query specification supplied by the user.  A relational algebra interface, called SQUIRAL, which was designed using this approach, is described in detail. SQUIRAL seeks to minimize query response time and space utilization by: (1) performing global query optimization, (2) exploiting disjoint and pipelined concurrency, (3) coordinating sort orders in temporary relations, (4) employing directory analysis, and (5) maintaining locality in page references. Algorithms for implementing the operators of E. F. Codd's relational algebra are presented, and a methodology for composing them to optimize the performance of a particular user query is described."}
{"DOCID": "2717", "TEXT": "CONVERT: A High Level Translation Definition Language for Data Conversion: This paper describes a high level and nonprocedural translation definition language, CONVERT, which provides very powerful and highly flexible data restructuring capabilities. Its design is based on the simple underlying concept of a form which enables the users to visualize the translation processes, and thus makes data translation a much simpler task. \"CONVERT\" has been chosen for conveying the purpose of the language and should not be confused with any other language or program bearing the same name."}
{"DOCID": "2718", "TEXT": "A Preliminary System for the Design of DBTG Data Structures: The functional approach to database design is introduced.  In this approach the goal of design is to derive a data structure which is capable of supporting a set of anticipated queries rather than a structure which \"models the business\" in some other way. An operational computer program is described which utilizers the functional approach to design data structures conforming to the Data Base Task Group specifications.  The automatic programming technology utilized by this program, although typically used to generate procedure, is here used to generate declaratives."}
{"DOCID": "2719", "TEXT": "Mechanical Program Analysis: One means of analyzing program performance is by deriving closed-form expressions for their execution behavior.  This paper discusses the mechanization of such analysis, and describes a system, Metric, which is able to analyze simple Lisp programs and produce, for example, closed-form expressions for their running time expressed in terms of size of input. This paper presents the reasons for mechanizing program analysis, describes the operation of Metric, explains its implementation, and discusses its limitations."}
{"DOCID": "2720", "TEXT": "Optimal Balancing of I/O Requests to Disks: Determining a policy for efficient allocation and utilization of a set of disk drives with differing operational characteristics is examined using analytical techniques.  Using standard queueing theory, each disk drive is characterized by a queueing model with service time of a disk drive represented by the probability density function of the sum of two uniform distributions. Total response time of the set of disk models is then minimized under varying load conditions. The results indicate that faster devices should have higher utilization factors and that the number of different device types utilized tends to decrease with decreasing load.  Specific examples using 2314 and 3330 combinations are examined."}
{"DOCID": "2721", "TEXT": "The Digital Simulation of River Plankton Population Dynamics: This paper deals with the development of a mathematical model for and the digital simulation in Fortran IV of phytoplankton and zooplankton population densities in a river using previously developed rate expressions.  In order to study the relationships between the ecological mechanisms involved, the simulation parameters were varied illustrating the response of the ecosystem to different conditions, including those corresponding to certain types of chemical and thermal pollution.  As an investigation of the accuracy of the simulation methods, a simulation of the actual population dynamics of Asterionella in the Columbia River was made based on approximations of conditions in that river. Although not totally accurate, the simulation was found to predict the general annual pattern of plankton growth fairly well and, specifically, revealed the importance of the annual velocity cycle in determining such patterns. In addition, the study demonstrates the usefulness of digital simulations in the examinations of certain aquatic ecosystems, as well as in environmental planning involving such examinations."}
{"DOCID": "2722", "TEXT": "Multidimensional Binary Search Trees Used for Associative Searching: This paper develops the multidimensional binary search tree (or k-d tree, where k is the dimensionality of the search space) as a data structure for storage of information to be retrieved by associative searches. The k-d tree is defined and examples are given. It is shown to be quite in its storage requirements. A significant advantage of this structure is that a single data structure can handle many types of queries very efficiently.  Various utility algorithms are developed; their proven average running times in an n record file are: insertion, O (log n); deletion of the root, O (n^(k-1)/k); deletion of a random node, O (log n); and optimization (guarantees logarithmic performance of searches), O (n log n).  Search algorithms are given for partial match queries with t keys specified [proven maximum running time of O (n^(k-t)/k)] and for nearest neighbor queries [empirically observed average running time of O (log n).]  These performances far surpass the best currently known algorithms for these tasks.  An algorithm is presented to handle any general intersection query. The main focus of this paper theoretical.  It is felt, however, that k-d trees could be quite useful in many applications, and examples of potential uses are given."}
{"DOCID": "2723", "TEXT": "Multiprocessing Compactifying Garbage Collection: Algorithms for a multiprocessing compactifying garbage collector are presented and discussed. The simple case of two processors, one performing LISP-like list operations and the other performing garbage collection continuously, is thoroughly examined. The necessary capabilities of each processor are defined, as well as interprocessor communication and interlocks. Complete procedures for garbage collection and for standard list processing primitives are presented and thoroughly explained.  Particular attention is given to the problems of marking and relocating list cells while another processor may be operating on them.  The primary aim throughout is to allow the list processor to run unimpeded while the other processor reclaims list storage.  The more complex cases involving several list processors and one or more garbage collection processors are also briefly discussed."}
{"DOCID": "2724", "TEXT": "The Lemniscate Constants (Corrigendum)"}
{"DOCID": "2725", "TEXT": "A Comparison of Simulation Event List Algorithms (Corrigendum)"}
{"DOCID": "2726", "TEXT": "Combining Decision Rules in a Decision Table: The techniques for minimizing logic circuits are applied to the simplification of decision tables by the combining of decision rules. This method is logically equivalent to the Quien-McCluskey method for finding prime implicants.  If some of the decision rules implied in the ELSE Rule occur with low frequency, then the ELSE Rule can be used to further simplify the decision table.  Several objectives merit consideration in optimizing a decision table:(1) reducing machine execution time; (2) reducing preprocessing time; (3) reducing required machine memory; (4) reducing the number of decision rules. (This often improves the clarity of the decision table to a human reader.)  It will be shown that objectives (3) and (4) can be furthered with the above methods. Objective (1) is also attained if overspecified decision rules are not combined.  Objective (2) must be compared against the potential benefits of objectives (1), (3), and (4) in deciding whether to use the above methods."}
{"DOCID": "2727", "TEXT": "Multiple Byte Processing with Full-Word Instructions: A method is described which allows parallel processing of packed data items using only ordinary full-word computer instructions, even though the processing requires operations whose execution is contingent upon the value of a datum.  It provides a useful technique for processing small data items such as alphanumeric characters."}
{"DOCID": "2728", "TEXT": "Consecutive Storage of Relevant Records with Redundancy: This paper studies the properties of a new class of file organizations (CRWR) where records relevant to every query are stored in consecutive storage locations but the organizations contain redundancy. Some theorems which provide tools for reducing redundancy in CRWR organizations have been also developed. Redundancies obtained by the application of these theorems are compared with that of query-inverted file organizations.  Some CRWR organization with minimum redundancy have also been developed for queries which specify sets of keys."}
{"DOCID": "2729", "TEXT": "Comments on a Paper by T. C. Chen and I. T. Ho"}
{"DOCID": "2730", "TEXT": "Interactive Consulting via Natural Language: Interactive programming systems often contain help commands to give the programmer on-line instruction regarding the use of the various systems commands.  It is argued that it would be relatively easy to make these help commands significantly more helpful by having them accept requests in natural language.  As a demonstration, Weizenbaum's ELIZA program has been provided with a script that turns it into a natural language system consultant."}
{"DOCID": "2731", "TEXT": "Remark on Stably Updating Mean and Standard Deviation of Data"}
{"DOCID": "2732", "TEXT": "Guarded Commands, Nondeterminacy and Formal Derivation of Programs: So-called \"guarded commands\" are introduced as a building block for alternative and repetitive constructs that allow nondeterministic program components for which at least the activity evoked, but possible even the final state, is not necessarily uniquely determined by the initial state.  For the formal derivation of programs expressed in terms of these constructs, a calculus will be shown."}
{"DOCID": "2733", "TEXT": "Deterministic Parsing of Ambiguous Grammars: Methods of describing the syntax of programming languages in ways that are more flexible and natural than conventional BNF descriptions are considered. These methods involve the use of ambiguous context-free grammars together with rules to resolve syntactic ambiguities.  It is shown how efficient LR and LL parsers can be constructed directly from certain classes of these specifications."}
{"DOCID": "2734", "TEXT": "On the External Storage Fragmentation Produced by First-Fit and Best-Fit Allocation Strategies: Published comparisons of the external fragmentation produced by first-fit and best-fit memory allocation have not been consistent.  Through simulation, a series of experiments were performed in order to obtain better data on the relative performance of first-fit and best-fit and a better understanding of the reasons underlying observed differences. The time-memory-product efficiencies of first-fit and best-fit were generally within 1 to 3 percent of each other.  Except for small populations, the size of the request population had little effect on allocation efficiency.  For exponential and hyperexponential distributions of requests, first-fit outperformed best-fit; but for normal and uniform distributions, and for exponential distributions distorted in various ways, best-fit outperformed first-fit.  It is hypothesized that when first-fit outperforms best-fit, it does so because first-fit, by preferentially allocating toward one end of memory, encourages large blocks to grow at the other end.  Sufficient contiguous space is thereby more likely to be available for relatively large requests.  Results of simulation experiments supported this hypothesis and showed that the relative performance of first-fit and best-fit depends on the frequency of request.  When the coefficient of variation of the request distribution is greater than or approximately equal to unity, first-fit outperformed best-fit."}
{"DOCID": "2735", "TEXT": "Discrimination in the Employment of Women in the Computer Industry"}
{"DOCID": "2736", "TEXT": "A Note on Hash Linking"}
{"DOCID": "2737", "TEXT": "Determining the Minimum-Area Encasing Rectangle for an Arbitrary Closed Curve: This paper describes a method for finding the rectangle of minimum area in which a given arbitrary plane curve can be contained.  The method is of interest in certain packing and optimum layout problems. It consists of first determining the minimal-perimeter convex polygon that encloses the given curve and then selecting the rectangle of minimum area capable of containing this polygon.  Three theorems are introduced to show that one side of the minimum-area rectangle must be colinear with an edge of the enclosed polygon and that the minimum-area encasing rectangle for the convex polygon is also the minimum-area rectangle for the curve."}
{"DOCID": "2738", "TEXT": "Use of the Concept of Transparency in the Design of Hierarchically Structured Systems: This paper deals with the design of hierarchically structured programming systems.  It develops a method for evaluating the cost of requiring programmers to work with an abstraction of a real machine. A number of examples from hardware and software are given as illustrations of the method."}
{"DOCID": "2739", "TEXT": "The Restriction Language for Computer Grammars of Natural Language: Over the past few years, a number of systems for the computer analysis of natural language sentences have been based on augmented context-free grammars: a context-free grammar which defines a set of parse trees for a sentence, plus a group of restrictions to which a tree must conform in order to be a valid sentence analysis.  As the coverage of the grammar is increased, an efficient representation becomes essential for further development.  This paper presents a programming language designed specifically for the compact and perspicuous statement of restrictions of a natural language grammar.  It is based on ten years' experience parsing text sentences with the comprehensive English grammar of the N.Y.U. Linguistic String Project, and embodies in its syntax and routines the relations which were found to be useful and adequate for computerized natural language analysis.  The language is used in the current implementation of the Linguistic String Parser."}
{"DOCID": "2740", "TEXT": "A Large Semaphore Based Operating System: The paper describes the internal structure of a large operating system as a set of cooperating sequential processes.  The processes synchronize by means of semaphores and extended semaphores (queue semaphores).  The number of parallel processes is carefully justified, and the various semaphore constructions are explained.  The system is proved to be free of \"deadly embrace\" (deadlock).  The design principle is an alternative to Dijkstra's hierarchical structuring of operating systems.  The project management and the performance are discussed, too.  The operating system is the first large one using the RC 4000 multiprogramming system."}
{"DOCID": "2741", "TEXT": "Decomposability, Instabilities, and Saturation in Multiprogramming Systems: A step-by-step approach to model the dynamic behavior and evaluate the performance of computing systems is proposed.  It is based on a technique of variable aggregation and the concept of nearly decomposable system, both borrowed from Econometrics.  This approach is taken in order to identify in multiprogramming paging systems (i) unstable regimes of operations and (ii) critical computing loads which bring the system into states of saturation.  This analysis leads to a more complete definition of the circumstances in which \"thrashing\" can set in."}
{"DOCID": "2742", "TEXT": "Improved Event-Scanning Mechanisms for Discrete Event Simulation: Simulation models of large, complex \"real-world\" applications have occasionally earned the reputation of eating up hours of computer time.  This problem may be attributed in part to difficulties such as slow stochastic convergence.  However, an additional problem lies in the fact that a significant amount of bookkeeping time is required to keep future events in their proper sequence.  This paper presents a method for significantly reducing the time spent scanning future event lists in discrete event simulations. There models are presented, all of which improve in effectiveness as the events-list scan problem becomes more burdensome."}
{"DOCID": "2743", "TEXT": "Sorting X + Y"}
{"DOCID": "2744", "TEXT": "Addition in an Arbitrary Base Without Radix Conversion: This paper presents a generalization of an old programming technique; using it,one may add and subtract numbers represented in any radix, including a mixed radix, and stored one digit per byte in bytes of sufficient size.  Radix conversion is unnecessary, no looping is required, and numbers may even be stored in a display (I/O) format.  Applications to Cobol, MIX, and hexadecimal sums are discussed."}
{"DOCID": "2745", "TEXT": "A Linear Space Algorithm for Computing Maximal Common Subsequences: The problem of finding a longest common subsequence of two strings has been solved in quadratic time and space.  An algorithm is presented which will solve this problem in quadratic time and in linear space."}
{"DOCID": "2746", "TEXT": "Efficient String Matching: An Aid to Bibliographic Search: This paper describes a simple, efficient algorithm to locate all occurrences of any of a finite number of keywords in a string of text.  The algorithm consists of constructing a finite state pattern matching machine from the keywords and then using the pattern matching machine to process the text string in a single pass.  Construction of the pattern matching machine takes time proportional to the sum of the lengths of the keywords.  The number of state transitions made by the pattern matching machine in processing the text string is independent of the number of keywords.  The algorithm has been used to improve the speed of a library bibliographic search program by a factor of 5 to 10."}
{"DOCID": "2747", "TEXT": "A Simplified Recombination Scheme for the Fibonacci Buddy System: A simplified recombination scheme for the Fibonacci buddy system which requires neither tables nor repetitive calculations and uses only two additional bits per buffer is presented."}
{"DOCID": "2748", "TEXT": "Indirect Threaded Code: An efficient arrangement for interpretive code is described.  It is related to Bell's notion of threaded code but requires less space and is more amenable to machine independent implementations."}
{"DOCID": "2749", "TEXT": "Significant Event Simulation: This paper compares a new method of simulation organization, called the significant event method, with an old one, called the clock pulse method, using as examples two automobile traffic models.  The significant event method is found to be more efficient than the clock pulse method at low levels of system interaction and less efficient at high levels.  A simple mathematical model for the trade-off in the relative running time of the two methods is developed. The model aids in choosing between the two simulation methods for a particular experiment.  It is concluded that the significant event method can be of value in the simulation of some systems when computational efficiency is of sufficient importance."}
{"DOCID": "2750", "TEXT": "A Cost Oriented Algorithm for Data Set Allocation in Storage Hierarchies: Data set allocation in today's multilevel storage systems is usually based on qualitative, ad hoc decisions.  While it would be desirable to obtain an optimal solution to this allocation problem, it is clear that the number of parameters involved makes it intractable to straight-forward solution. In such a situation, we must find a set of assumptions which simplify the problem greatly, but which still provide a basis for considering all significant cost elements. This paper presents such a first, quantitative allocation step.  It considers many of the significant detailed costs of system utilization, data storage, data staging, and data migration.  Although many avenues of further improvement are available, the present algorithm seems to be usefully accurate. As such, it can aid in quantifying the problems of data set allocation, storage system configuration, and new device designs."}
{"DOCID": "2751", "TEXT": "Illumination for Computer Generated Pictures: The quality of computer generated images of three-dimensional scenes depends on the shading technique used to paint the objects on the cathode-ray tube screen.  The shading algorithm itself depends in part on the method for modeling the object, which also determines the hidden surface algorithm.  The various methods of object modeling, shading, and hidden surface removal are thus strongly interconnected. Several shading techniques corresponding to different methods of object modeling and the related hidden surface algorithms are presented here.  Human visual perception and the fundamental laws of optics are considered in the development of a shading rule that provides better quality and increased realism in generated images."}
{"DOCID": "2752", "TEXT": "Generation of All the Cycles of a Graph from a Set of Basic Cycles [H] (Algorithm 492)"}
{"DOCID": "2753", "TEXT": "A Heuristic Problem Solving Design System for Equipment or Furniture Layouts: The Designer Problem Solver (DPS) demonstrates that the computer can perform simple design tasks.  In particular, it designs furniture and equipment layouts.  This task was chosen because it is simple, well defined, and characteristic of many design tasks in architecture, engineering, urban planning, and natural resource management.  These space planning tasks usually involve manipulating two-dimensional representations of objects to create feasible or optimal solutions for problems involving topological and metric spatial constraints.  The paper describes extensive tests performed on the program.  DPS is a heuristic problem solver with a planning phase prefixed to it.  It uses the planning process to give it a sense of direction, diagnostic procedures to locate difficulties, and remedial actions to recover from difficulties.  It uses a convex polygon representation to accurately describe the objects and the layout.  This representation allows topological and metric constraints to be tested and the design to be easily updated.  DPS has been applied to 50 problems. While it is slow and limited in scope, the ideas behind it are general.  It demonstrates the need for selectivity in controlling search and the methods used to achieve it: task-specific information, planning, diagnostic procedures, remedial actions, and selective alternative generators."}
{"DOCID": "2754", "TEXT": "A Syntactic Algorithm for Peak Detection in Waveforms with Applications to Cardiography: Peaks in a digitized waveform are detected by an algorithm incorporating piecewise linear approximation and tabular parsing techniques.  Several parameters serve to identify the waveform context enabling accurate measurement of peak amplitude, duration, and shape.  The algorithm is of sufficient speed to allow on-line real-time processing.  An example of its application is demonstrated on an electrocardiogram."}
{"DOCID": "2755", "TEXT": "The New Math of Computer Programming (Corrigendum)"}
{"DOCID": "2756", "TEXT": "A Problem-List of Public Policy Issues Concerning Computers and Health Care"}
{"DOCID": "2757", "TEXT": "More on kth Shortest Paths"}
{"DOCID": "2758", "TEXT": "A Note on the LU Factorization of a Symmetric Matrix"}
{"DOCID": "2759", "TEXT": "Solution of an Overdetermined System of Equations in the L1 Norm (Algorithm R478)"}
{"DOCID": "2760", "TEXT": "Visible Surface Plotting Program (Algorithm R475)"}
{"DOCID": "2761", "TEXT": "Visible Surface Plotting Program (Algorithm R475)"}
{"DOCID": "2762", "TEXT": "Ten Subroutines for the Manipulation of Chebyshev Series (Algorithm R446, C446)"}
{"DOCID": "2763", "TEXT": "Basic Cycle Generation [H] (Algorithm 491)"}
{"DOCID": "2764", "TEXT": "An Intelligent Analyzer and Understander of English: The paper describes a working analysis and generation program for natural language, which handles paragraph length input.  Its core is a system of preferential choice between deep semantic patterns, based on what we call \"semantic density.\"  The system is contrasted: (1) with syntax oriented linguistic approaches, and (2) with theorem proving approaches to the understanding problem."}
{"DOCID": "2765", "TEXT": "Analysis and performance of Inverted Data Base Structures: The need to envision and architecture data base systems in a hierarchical level by level framework is stressed. The inverted data base (file) organization is then analyzed, considering implementation oriented aspects.  The inverted directory is viewed realistically as another large data base which itself is subjected to inversion.  Formulations are derived to estimate average access time (read only) and storage requirements, formalizing the interaction of data base content characteristics, logical complexity of queries, and machine timing and blocking specifications identified as having a first-order effect on performance.  The formulations presented are necessary to be used in conjunction with any index selection criteria to determine the optimum set of index keys."}
{"DOCID": "2766", "TEXT": "Copying Cyclic List Structures in Linear Time Using Bounded Workspace: A bounded workspace copying algorithm for arbitrary list structures is given.  This algorithm operates in linear time and does not require tag bits. The best previous bounded workspace copying algorithms achieved n^2 time without tag bits and n log n time with one tag.  The only restriction on the algorithm given here is that the copy must be placed into a contiguous section of memory.  The method is applicable to fixed or variable size nodes."}
{"DOCID": "2767", "TEXT": "A Comparison of Simulation Event List Algorithms: Four algorithms are considered which can be used to schedule events in a general purpose discrete simulation system.  Two of the algorithms are new, one is based on an end-order tree structure for event notices, and another uses an indexed linear list. The algorithms are tested with a set of typical stochastic scheduling distributions especially chosen to show the advantages and limitations of the algorithms. The end-order tree algorithm is shown to be an advantageous, immediate replacement for the algorithm in use with current simulation languages.  The most promising algorithm uses the indexed list concept. It will require an adaptive routine before it can be employed in general purpose simulators,but its performance is such that further study would be fruitful."}
{"DOCID": "2768", "TEXT": "An Algorithm for Locating Adjacent Storage Blocks in the Buddy System: A simple scheme for the determination of the location of a block of storage relative to other blocks is described.  This scheme is applicable to the buddy type storage allocation systems."}
{"DOCID": "2769", "TEXT": "A Modification of Warshall's Algorithm for the Transitive Closure of Binary Relations: An algorithm is given for computing the transitive closure of a binary relation that is represented by a Boolean matrix. The algorithm is similar to Warshall's although it executes faster for sparse matrices on most computers, particularly in a paging environment."}
{"DOCID": "2770", "TEXT": "The Quadratic Hash Method When the Table Size Is Not a Prime Number: Previous work on quadratic hash methods is limited mainly to the case where the table size is a prime number.  Here, certain results are derived for composite numbers.  It is shown that all composite numbers containing at least the square of one of the component primes have full-period integer-coefficient quadratic hash functions."}
{"DOCID": "2771", "TEXT": "The Synthesis of Solids Bounded by Many Faces: A technique is presented which allows a class of solid objects to be synthesized and stored using a computer.  Synthesis begins with primitive solids like a cube, wedge, or cylinder.  Any solid can be moved, scaled, or rotated.  Solids may also be added together or subtracted.  Two algorithms to perform addition are described.  For practical designers, the technique has the advantage that operations are concise, readily composed, and are given in terms of easily imagined solids.Quite short sequences of operations suffice to build up complex solids bounded by many faces."}
{"DOCID": "2772", "TEXT": "On Maintenance of the Opportunity List for Class-Teacher Timetable Problems: One of the principal components of procedures for the solution of class-teacher timetable problems is that for maintenance of the opportunity list.  Opportunity list maintenance methods are based on necessary conditions for the existence of a solution. A general framework for necessary conditions, together with four specific sets of necessary conditions, is given."}
{"DOCID": "2773", "TEXT": "A Weighted Buddy Method for Dynamic Storage Allocation (Corrigendum)"}
{"DOCID": "2774", "TEXT": "Remark on Algorithm 475"}
{"DOCID": "2775", "TEXT": "The Dilogarithm Function of a Real Argument [S22] (Algorithm 490)"}
{"DOCID": "2776", "TEXT": "Computer Networks in Higher Education: Socio-Economic-Political Factors: This study presents the results of a nationwide survey of computer networks in higher education conducted during 1971-73.  Five major and 18 minor networks were identified.  The five major networks included: the ARPA Net, the California State College network, the University of Iowa/Iowa State University network, the Michigan Educational Research Information Triad, Inc., and the Triangle Universities Computation Center network in North Carolina. In-depth studies were conducted of the latter two nets.  Based on the experiences of these operating networks, a number of factors are identified for consideration in developing networks.  Finally, recommendations are advanced regarding the development of networks in higher education in the future."}
{"DOCID": "2777", "TEXT": "On a Solution to the Cigarette Smoker's Problem (Without Conditional Statements): This report discusses a problem first introduced by Patil, who has claimed that the cigarette smoker's problem cannot be solved using the P and V operations introduced by Dijkstra unless conditional statements are used.  An examination of Patil's proof shows that he has established this claim only under strong restrictions on the use of P and V.  These restrictions eliminate programming techniques used by Dijkstra and others since the first introduction of the semaphore concept.  This paper contains a solution to the problem.  It also discusses the need for the generalized operators suggested by Patil."}
{"DOCID": "2778", "TEXT": "Perturbations of Eigenvalues of Non-normal Matrices (Corrigendum)"}
{"DOCID": "2779", "TEXT": "Discrete Least Squares Polynomial Fits: The recurrence relation between orthogonal polynomials is widely used for discrete least squares data fitting.  A variant of the classical algorithm which has better numerical properties is presented and the reason for its improved performance is explained."}
{"DOCID": "2780", "TEXT": "On Computing Certain Elements of the Inverse of a Sparse Matrix: A recursive algorithm for computing the inverse of a matrix from the LU factors based on relationships in Takahashi, et al., is examined.  The formulas for the algorithm are given; the dependency relationships are derived; the computational costs are developed; and some general comments on application and stability are made."}
{"DOCID": "2781", "TEXT": "The Algorithm Sequential Access Method: An Alternative to Index Sequential"}
{"DOCID": "2782", "TEXT": "A Reply to Gentleman and Marovich"}
{"DOCID": "2783", "TEXT": "The Algorithm SELECT-for Finding the ith Smallest of n Elements [M1] (Algorithm 489)"}
{"DOCID": "2784", "TEXT": "Expected Time Bounds for Selection: A new selection algorithm is presented which is shown to be very efficient on the average, both theoretically and practically.  The number of comparisons used to select the ith smallest of n numbers is n+min(i,n-i)+o(n).  A lower bound within 9 percent of the above formula is also derived."}
{"DOCID": "2785", "TEXT": "Glypnir-A Programming Language for Illiac IV: GLYPNIR is one of the earliest existing languages designed for programming the Illiac IV computer. The syntax of the language is based on ALGOL 60, but has been extended to allow the programmer explicitly to specify the parallelism of his algorithm in terms of 64-word vectors.  This paper describes the characteristics, goals and philosophy of the language, and discusses some of the problems associated with parallel computer architectures."}
{"DOCID": "2786", "TEXT": "A System for Typesetting Mathematics: This paper describes the design and implementation of a system for typesetting mathematics. The language has been designed to be easy to learn and to use by people (for example, secretaries and mathematical typists) who know neither mathematics nor typesetting.  Experience indicates that the language can be learned in an hour or so, for it has few rules and fewer exceptions.  For typical expressions, the size and font changes, positioning, line drawing, and the like necessary to print according to mathematical conventions are all done automatically.  For example, the input sum from i=o to infinity x sub i=pi over 2 produces (formula).  The syntax of the language is specified by a small context-free grammar; a compiler-compiler is used to make a compiler that translates this language into typesetting commands.  Output maybe produced on either a phototypesetter or on a terminal with forward and reverse half-line motions.  The system interfaces directly with text formatting programs, so mixtures of text and mathematics may be handled simply.  This paper was typeset by the authors using the system described"}
{"DOCID": "2787", "TEXT": "Matrix Reduction-An Efficient Method: The paper describes an efficient method for reduction of the binary matrices which arise in some school time-tabling problems.  It is a development of that described by John Lions.  It has been generalized and adapted to fit into the complete timetabling process; to use a more compact data representation and more efficient processing techniques; to take fuller advantage of possible available previous knowledge about the matrix.  And it is designed as a structured program, which can readily be coded by the reader in the high level or low level programming language of his choice.  Practical tests of the method have shown it to be a good basis for a realistic timetabling algorithm."}
{"DOCID": "2788", "TEXT": "Finding Circles by an Array of Accumulators"}
{"DOCID": "2789", "TEXT": "A Minimal Spanning Tree Clustering Method (Algorithm R479)"}
{"DOCID": "2790", "TEXT": "The Elementary Circuits of a Graph (Algorithm R459)"}
{"DOCID": "2791", "TEXT": "Exact Probabilities for R x C Contingency Tables (Algorithm R434)"}
{"DOCID": "2792", "TEXT": "Jacobi Polynomials (Algorithm R332)"}
{"DOCID": "2793", "TEXT": "Chi-Square Quantiles (Algorithm C451)"}
{"DOCID": "2794", "TEXT": "State-Space, Problem-Reduction, and Theorem Proving-Some Relationships: This paper suggests a bidirectional relationship between state-space and problem-reduction representations. It presents a formalism based on multiple-input and multiple-output operators which provides a basis for viewing the two types of representations in this manner.  A representation of the language recognition problem which is based on the Cocke parsing algorithm is used as an illustration. A method for representing problems in first-order logic in such a way that the inference system employed by a resolution-based theorem prover determines whether the set of clauses is interpreted in the state-spacer mode or in the problem-reduction mode is presented. The analogous concepts in problem-reduction and theorem proving, and the terminology used to refer to them, are noted.  The relationship between problem-reduction, input resolution, and linear resolution is discussed."}
{"DOCID": "2795", "TEXT": "Sentence Paraphrasing from a Conceptual Base: A model of natural language based on an underlying language-free representation of meaning is described.  A program based on this model is able to produce sentence paraphrases which demonstrate understanding with respect to a given context.  This generator operates in conjunction with a natural language analyzer and a combined memory and inference model. In generating sentences from meaning structures, the program employs both the information retrieval and deduction capabilities of the memory model.  The model encompasses several diverse classes of linguistic knowledge, which include: (1) executable tests of conceptual properties stored in discrimination nets; (2) information relating conceptual to syntactic roles, stored in a word-sense dictionary, and (3) surface grammatical knowledge, stored in a formal grammar."}
{"DOCID": "2796", "TEXT": "Monitors: An Operating System Structuring Concept (Corrigendum)"}
{"DOCID": "2797", "TEXT": "A First Order Approximation to the Optimal Checkpoint Interval (Corrigendum)"}
{"DOCID": "2798", "TEXT": "Analysis of Interleaved Memory Systems Using Blockage Buffers: A model of interleaved memory systems is presented, and the analysis of the model by Monte Carlo simulation is discussed.  The simulations investigate the performance of various system structures, i.e. schemes for sending instruction and data requests to the memory system.  Performance is measured by determining the distribution of the number of memory modules in operation during a memory cycle. An important observation from these investigations is that separately grouping instruction and data requests for memory can substantially increase the average number of memory modules in operation during a memory cycle.  Results of the simulations and an analytical study are displayed for various system structures."}
{"DOCID": "2799", "TEXT": "Stably Updating Mean and Standard Deviation of Data: By considering the (sample) mean of a set of data as a fit to this data by a constant function, a computational method is given based on a matrix formulation and Givens transformations. The (sample) mean and standard deviation can be updated as data accumulates.  The procedure is numerically stable and does not require storage of the data.  Methods for dealing with weighted data and data removal are presented.  When updating the mean and square of the standard deviation, the process requires no square roots."}
{"DOCID": "2800", "TEXT": "Connections Between Accuracy and Stability Properties of Linear Multistep Formulas: This paper is concerned with stability and accuracy of families of linear k-step formulas depending on parameters, with particular emphasis on the numerical solution of stiff ordinary differential equations. An upper bound, p=k, is derived for the order of accuracy of A(inf)-stable formulas.  Three criteria are given for A(0)-stability.  It is shown that (1) for p=k, k arbitrary, A(inf)-stability implies certain necessary conditions for A(0)-stability and for strict stability (meaning that the extraneous roots of p(psi) satisfy |psi|<1); (2) for p=k=2,3,4,and 5, A(inf)-stability (for k=5 together with another constraint) implies strict stability; and (3) for certain one-parameter classes of formulas with p=k=3,4,and/or 5, A(inf)-stability implies A(0)-stability."}
{"DOCID": "2801", "TEXT": "Storage-Efficient Representation of Decimal Data: Usually n decimal digits are represented by 4n bits in computers.  Actually, two BCD digits can be compressed optimally and reversibly into 7 bits, and three digits into 10 bits, by a very simple algorithm based on the fixed-length combination of two variable field-length encodings.  In over half of the cases the compressed code results from the conventional BCD code by simple removal of redundant 0 bits.  A long decimal message can be subdivided into three-digit blocks, and separately compressed; the result differs from the asymptotic minimum length by only 0.34 percent.  The hardware requirement is small, and the mappings can be done manually."}
{"DOCID": "2802", "TEXT": "The New Math of Computer Programming: Structured programming has proved to be an important methodology for systematic program design and development.  Structured programs are identified as compound function expressions in the algebra of functions. The algebraic properties of these function expressions permit the reformulation (expansion as well as reduction) of a nested subexpression independently of its environment, thus modeling what is known as stepwise program refinement as well as program execution.  Finally, structured programming is characterized in terms of the selection and solution of certain elementary equations defined in the algebra of functions.  These solutions can be given in general formulas, each involving a single parameter, which display the entire freedom available in creating correct structured programs."}
{"DOCID": "2803", "TEXT": "Pseudoinverses and Conjugate Gradients: This paper is devoted to the study of connections between pseudoinverses of matrices and conjugate gradients and conjugate direction routines."}
{"DOCID": "2804", "TEXT": "Elementary Divisors of Tensor Products: The elementary divisors of a tensor product of linear transformations have been known for 40 years.  This paper provides a short, easily accessible proof of these results, and points out an interesting combinatorial consequence of the proof."}
{"DOCID": "2805", "TEXT": "Perturbations of Eigenvalues of Non-normal Matrices: The problem considered is to give bounds for finite perturbations of simple and multiple eigenvalues of nonnormal matrices, where these bounds are in terms of the eigenvalues, the departure from normality, and the Frobenius norm of the perturbation matrix, but not in terms of the eigen system.  The bounds which are derived are shown to be almost attainable for any set of matrices."}
{"DOCID": "2806", "TEXT": "Two Hadamard Numbers for Matrices: A discussion is given of two functions of the entries of a square matrix, both related to Hadamard's determinant theorem, which have some merits as alternatives to norm-bound \"condition numbers.\"  One (for linear systems) is known; the other (for eigen systems) seems to be new."}
{"DOCID": "2807", "TEXT": "On the Stability of Gauss-Jordan Elimination with Pivoting: The stability of the Gauss-Jordan algorithm with partial pivoting for the solution of general systems of linear equations is commonly regarded as suspect. It is shown that in many respects suspicions are unfounded, and in general the absolute error in the solution is strictly comparable with that corresponding to Gaussian elimination with partial pivoting plus back substitution.  However, when A is ill conditioned, the residual corresponding to the Gauss-Jordan solution will often be much greater than that corresponding to the Gaussian elimination solution."}
{"DOCID": "2808", "TEXT": "The Lemniscate Constants: The lemniscate constants, and indeed some of the methods used for actually computing them, have played an enormous part in the development of mathematics.  An account is given here of some of the methods used-most of the derivations can be made by elementary methods.  This material can be used for teaching purposes, and there is much relevant and interesting historical material.  The acceleration methods developed for the purpose of evaluating these constants are useful in other problems."}
{"DOCID": "2809", "TEXT": "Positivity and Norms: Following some lines of joint work with A. S. Householder, the character and use of algebraic methods in the theory of norms is demonstrated.  New results concerning norms with values in an Archimedian vector lattice (not necessarily being totally ordered) are given, in particular for the generalization of order unit norms, L-norms and M-norms.  An example of application to operator norms is given concerning contraction properties of positive operators."}
{"DOCID": "2810", "TEXT": "Professionalism in the Computing Field: The term professional means different things to different people; nevertheless, there are certain general technical and social standards normally associated with a professional.  Further, the term is more generally applied to the practitioner rather than to the researcher.  But within the rather broad definition specified, the computing practitioner is, as yet, not regarded as a professional.  Each of the four types of institutions-academic, industry, government, and the professional society- that educate, employ, regulate, and mold the practitioner contributes to the \"nonprofessional\" status of the computing practitioner.  The roles of these institutions are examined, various shortcomings are noted, and recommended changes are suggested.  In the last analysis, professional status is not bestowed; it is earned.  However, universities and industry, specifically, can make certain improvements to help the computing practitioner achieve professional status."}
{"DOCID": "2811", "TEXT": "Structural Pattern Recognition Of Carotid Pulse Waves Using A General Waveform Parsing System: A general waveform parsing system with application to structural pattern recognition of carotid pulse waves is described.  The carotid arterial pulse wave is of medical importance because of variation in its structure induced by arterial aging and cardiovascular disease.  The syntax-driven waveform analysis system has been applied with good results to these pulse waves to detect and measure structural variations. The waveform parsing system is modeled on a compiler-compiler system and allows the user to enter application specific information as data.  It is thus general enough to be applicable to other waveforms."}
{"DOCID": "2812", "TEXT": "Computer-Aided Analysis and Design of Information Systems: This paper describes the use of computer-aided analysis for the design and development of an integrated financial management system by the Navy Material Command Support Activity (NMCSA).  Computer-aided analysis consists of a set of procedures and computer programs specifically designed to aid in the process of applications software design, computer selection and performance evaluation.  There are four major components: Problem Statement Language, Problem Statement Analyzer, Generator of Alternative Designs, and Performance Evaluator. The statement of requirements was written in ADS (Accurately Defined Systems) and analyzed by a Problem Statement Analyzer for ADS. The ADS problem definition was supplemented with additional information in order to create a complete problem definition.  The analyzed problem statement was translated to the form necessary for use by the SODA (Systems Optimization and Design Algorithm) program for the generation of alternative specifications of program modules and logical database structures."}
{"DOCID": "2813", "TEXT": "The Computer Science and Engineering Research Study (COSERS): The Computer Science and Engineering Research Study (COSERS) is briefly described.  The motivation, organization, and schedule for this NSF supported study are given.  For possible further reference, the subject area panel chairmen and the members of the Steering Committee are identified."}
{"DOCID": "2814", "TEXT": "Roster of Programming Languages for 1974-75"}
{"DOCID": "2815", "TEXT": "High-Level Binding with Low-Level Linkers: An easy to implement scheme is described by which a compiler can enforce agreement between complex data types in separately compiled modules. The scheme is designed to work with any existing link editor or linking loader, no matter how deficient. Obscure run-time errors caused by inconsistent usages are forestalled by static errors detected at linking time."}
{"DOCID": "2816", "TEXT": "Optimal Reorganization of Distributed Space Disk Files: In most database organizations, the cost of accessing the database will increase due to structural changes caused by updates and insertions.  By reorganizing the database,the access costs can be reduced. A basic problem is to establish the proper tradeoff between performance, storage costs, and reorganization costs.  This paper considers the optimum points at which to reorganize a database.  A disk file organization which allows for distributed free space is described. A cost function describing the excess costs due to physical disorganization is defined, and this function is minimized to obtain the optimum reorganization points.  Numerical examples based on the characteristics of existing disk storage devices are given."}
{"DOCID": "2817", "TEXT": "The Notions of Consistency and Predicate Locks in a Database System: In database systems, users access shared data under the assumption that the data satisfies certain consistency constraints.  This paper defines the concepts of transaction, consistency and schedule and shows that consistency requires that a transaction cannot request new locks after releasing a lock. Then it is argued that a transaction needs to lock a logical rather than a physical subset of the database. These subsets may be specified by predicates.  An implementation of predicate locks which satisfies the consistency condition is suggested."}
{"DOCID": "2818", "TEXT": "Interference in Multiprocessor Computer Systems with Interleaved Memory (Corrigendum)"}
{"DOCID": "2819", "TEXT": "Experiments in Text File Compression: A system for the compression of data files, viewed as strings of characters, is presented. The method is general, and applies equally well to English, to PL/I, or to digital data.  The system consists of an encoder, an analysis program, and a decoder. Two algorithms for encoding a string differ slightly from earlier proposals.  The analysis program attempts to find an optimal set of codes for representing substrings of the file.  Four new algorithms for this operation are described and compared.  Various parameters in the algorithms are optimized to obtain a high degree of compression for sample texts."}
{"DOCID": "2820", "TEXT": "The Design and Implementation of a Table Driven, Interactive Diagnostic Programming System: CAPS is a highly interactive diagnostic compiler/interpreter that allows beginning programmers to prepare, debug, and execute fairly simple programs at a graphics display terminal.  Complete syntax checking and most semantic analysis is performed as the program is entered and as it is subsequently edited.  Analysis is performed character by character. The most remarkable feature of CAPS is its ability to automatically diagnose errors both at compile time and at run time.  Errors are not automatically corrected.  Instead, CAPS interacts with the student to help him find the cause of his error.  Most components of CAPS are table driven, both to reduce the space needed for implementation and to increase the flexibility of the multilingual system.  Over 500 students have used CAPS to learn Fortran, PL/I, or Cobolin conjunction with a computer assisted course on introductory computer science."}
{"DOCID": "2821", "TEXT": "Cobol Under Control: A sample set of Cobol programming standards is offered.  These standards constrain code to be developed in a \"structured\" form for both data and control structures.  They do not require syntax beyond the existing Cobol language and in fact utilize a typical limited subset of the 1974 ANS Cobol standard.  These standards have proved extremely valuable in practice and have reduced the cost and time to produce and maintain large software systems that have been deployed in live multiple customer environments."}
{"DOCID": "2822", "TEXT": "Homilies for Humble Standards: Copyright 1976, Association for Computing Machinery, Inc. General permission to republish, but not for profit, all or part of this material is granted provided that ACM's copyright notice is given and that reference is made to the publication, to its data of issue, and to the fact that reprinting privileges were granted by permission of the Association for Computing Machinery."}
{"DOCID": "2823", "TEXT": "The Status of Women and Minorities in Academic Computer Science: The results of a survey concerning women and minority students and faculty in computer science during the years 1971 to 1975 are presented.  Analysis of the data indicated that effective affirmative action programs for recruitment into graduate degree programs are needed to enlarge the number of women and minorities qualified for later employment in computer science.  Also, possible discrimination in employment of women and minority graduate students was revealed."}
{"DOCID": "2824", "TEXT": "An Improvement to Martin's Algorithm for Computation of Linear Precedence Functions"}
{"DOCID": "2825", "TEXT": "The BMD and BMDP Series of Statistical Computer Programs"}
{"DOCID": "2826", "TEXT": "Interactive Skeleton Techniques for Enhancing Motion Dynamics in Key Frame Animation: A significant increase in the capability for controlling motion dynamics in key frame animation is achieved through skeleton control.  This technique allows an animator to develop a complex motion sequence by animating a stick figure representation of an image.  This control sequence is then used to drive an image sequence through the same movement. The simplicity of the stick figure image encourages a high level of interaction during the design stage. Its compatibility with the basic key frame animation technique permits skeleton control to be applied selectively to only those components of a composite image sequence that require enhancement."}
{"DOCID": "2827", "TEXT": "A Parametric Algorithm for Drawing Pictures of Solid Objects Composed of Quadric Surfaces: An algorithm for drawing pictures of three-dimensional objects, with surfaces made up of patches of quadric surfaces, is described.  The emphasis of this algorithm is on calculating the intersections of quadric surfaces. A parameterization scheme is used. Each quadric surface intersection curve (QSIC) is represented as a set of coefficients and parameter limits.  Each value of the parameter represents at most two points, and these may easily be distinguished. This scheme can find the coordinates of points of even quartic (fourth-order) intersection curves, using equations of no more than second order.  Methods of parameterization for each type of OSIC are discussed, as well as surface bounding and hidden surface removal."}
{"DOCID": "2828", "TEXT": "Hierarchical Geometric Models for Visible Surface Algorithms: The geometric structure inherent in the definition of the shapes of three-dimensional objects and environments is used not just to define their relative motion and placement, but also to assist in solving many other problems of systems for producing pictures by computer.  By using an extension of traditional structure information, or a geometric hierarchy, five significant improvements to current techniques are possible.  First, the range of complexity of an environment is greatly increased while the visible complexity of any given scene is kept within a fixed upper limit.  Second, a meaningful way is provided to vary the amount of detail presented in a scene.  Third, \"clipping\" becomes a very fast logarithmic search for the resolvable parts of the environment within the field of view.  Fourth, frame to frame coherence and clipping define a graphical \"working set,\" or fraction of the total structure that should be present in primary store for immediate access by the visible surface algorithm.  Finally, the geometric structure suggests a recursive descent, visible surface algorithm in which the computation time potentially grows linearly with the visible complexity of the scene."}
{"DOCID": "2829", "TEXT": "Texture and Reflection in Computer Generated Images: In 1974 Catmull developed a new algorithm for rendering images of bivariate surface patches. This paper describes extensions of this algorithm in the areas of texture simulation and lighting models. The parameterization of a patch defines a coordinate system which is used as a key for mapping patterns onto the surface.  The intensity of the pattern at each picture element is computed as a weighted average of regions of the pattern definition function. The shape and size of this weighting function are chosen using digital signal processing theory.  The patch rendering algorithm allows accurate computation of the surface normal to the patch at each picture element, permitting the simulation of the mirror reflections. The amount of light coming from a given direction is modeled in a similar manner to the texture mapping and then added to the intensity obtained from the texture mapping.  Several examples of images synthesized using these new techniques are included."}
{"DOCID": "2830", "TEXT": "A Practitioner's Guide to Addressing Algorithms (Corrigendum)"}
{"DOCID": "2831", "TEXT": "Analysis of the PFF Replacement Algorithm via a Semi-Markov Model (Corrigendum)"}
{"DOCID": "2832", "TEXT": "Faster Retrieval from Context Trees (Corrigendum): Context trees provide a convenient way of storing data which is to be viewed as a hierarchy of contexts.  This note presents an algorithm which improves on previous context tree retrieval algorithms. It is based on the observation that in typical uses context changes are infrequent relative to retrievals, so that data can be cached to speed up retrieval.  A retrieval is started from the position of the previous retrieval and auxiliary structures are built up to make the search rapid.  Algorithms for addition and deletion of data and for garbage collection are outlined."}
{"DOCID": "2833", "TEXT": "An Efficient, Incremental, Automatic Garbage Collector: This paper describes a new way of solving the storage reclamation problem for a system such as Lisp that allocates storage automatically from a heap, and does not require the programmer to give any indication that particular items are no longer useful or accessible.  A reference count scheme for reclaiming non-self-referential structures, and a linearizing, compacting, copying scheme to reorganize all storage at the users discretion are proposed.  The algorithms are designed to work well in systems which use multiple levels of storage, and large virtual address space.  They depend on the fact that most cells are referenced exactly once, and that reference counts need only be accurate when storage is about to be reclaimed.  A transaction file stores changes to reference counts, and a multiple reference table stores the count for items which are referenced more than once."}
{"DOCID": "2834", "TEXT": "Efficient Generation of the Binary Reflected Gray Code and Its Applications: Algorithms are presented to generate the n-bit binary reflected Gray code and codewords of fixed weight in that code.  Both algorithms are efficient in that the time required to generate the next element from the current one is constant.  Applications to the generation of the combinations of n things taken k at a time, the compositions of integers, and the permutations of a multiset are discussed."}
{"DOCID": "2835", "TEXT": "Recursion Analysis for Compiler Optimization: A relatively simple method for the detection of recursive use of procedures is presented for use in compiler optimization.  Implementation considerations are discussed, and a modification of the algorithm is given to further improve optimization. This analysis can also be used to determine what possible subset of values could be assumed by variables which can only take on a relatively small discrete set of values.  The most common are parameters of variables assuming values of label, procedure, or Pascal's enumerated type."}
{"DOCID": "2836", "TEXT": "Weighted Derivation Trees: The nodes of a weighted derivation tree are associated with weighting functions over the vocabulary of a context-free grammar.  An algorithm is presented for constructing the optimal derivation tree having the same structure as a given weighted derivation tree. In addition, the correctness of the algorithm is established.  The method may be applied to problems involving probabilistic parsing or combinatorial optimization."}
{"DOCID": "2837", "TEXT": "New Upper Bounds for Selection: The worst-case minimum number of comparisons complexity Vi(n) of the i-th selection problem is considered.  A new upper bound for Vi(n) improves the bound given by the standard Hadian-Sobel algorithm by a generalization of the Kirkpatrick-Hadian-Sobel algorithm, and extends Kirkpatrick's method to a much wider range of application.  This generalization compares favorably with a recent algorithm by Hyafil."}
{"DOCID": "2838", "TEXT": "Analysis of an Algorithm for Real Time Garbage Collection: A real time garbage collection system avoids suspending the operations of a list processor for the long times that garbage collection normally requires by performing garbage collection on a second processor in parallel with list processing operations, or on a single processor time-shared with them. Algorithms for recovering discarded list structures in this manner are presented and analyzed to determine sufficient conditions under which the list processor never needs to wait on the collector.  These techniques are shown to require at most twice as much processing power as regular garbage collectors, if they are used efficiently.  The average behavior of the program is shown to be very nearly equal to the worst-case performance, so that the sufficient conditions are also suitable for measuring the typical behavior of the algorithm."}
{"DOCID": "2839", "TEXT": "An Insertion Technique for One-Sided Height-Balanced Trees: A restriction on height-balanced binary trees is presented.  It is seen that this restriction reduces the extra memory requirements by half (from two extra bits per node to one) and maintains fast search capabilities at a cost of increased time requirements for inserting new nodes."}
{"DOCID": "2840", "TEXT": "Protection in Operating Systems: A model of protection mechanisms in computing systems is presented and its appropriateness is argued.  The \"safety\" problem for protection systems under this model is to determine in a given situation whether a subject can acquire a particular right to an object.   In restricted cases, it can be shown that this problem is decidable, i.e. there is an algorithm to determine whether a system in a particular configuration is safe.  In general, and under surprisingly weak assumptions, it cannot be decided if a situation is safe. Various implications of this fact are discussed."}
{"DOCID": "2841", "TEXT": "Designing Surfaces in 3-D: An experimental system for computer-aided design of free-form surfaces in three dimensions is described. The surfaces are represented in the system as parametric basis splines. The principal features of the system are: (1) the surfaces are rendered as isoparametric line drawings on a head-mounted display, and they are designed with the aid of a three-dimensional \"wand,\" which allows 3-D movements of the points controlling the shapes of the surfaces, (2) all of the interactions with the surfaces are in real-time, and (3) the mathematical formulations used assume no knowledge of them by the user of the system.  Also examined are some of the features that should be part of a practical 3-D system for designing space-forms."}
{"DOCID": "2842", "TEXT": "The Denotational Semantics of Programming Languages: This paper is a tutorial introduction to the theory of programming language semantics developed by D. Scott and C. Strachey.  The application of the theory to formal language specification is demonstrated and other applications are surveyed. The first language considered, LOOP, is very elementary and its definition merely introduces the notation and methodology of the approach.  Then the semantic concepts of environments, stores, and continuations are introduced to model classes of programming language features and the underlying mathematical theory of computation due to Scott is motivated and outlined.  Finally, the paper presents a formal definition of the language GEDANKEN."}
{"DOCID": "2843", "TEXT": "Tools and Philosophy for Software Education: This paper describes a set of tools and a philosophy for teaching software that have been found very useful in course at MIT over the past seven years. The tools include programs such as simulators, graders, compilers, and monitor.  These allow the instructor to augment the basic concepts with relevant, exciting, and economical student project activities."}
{"DOCID": "2844", "TEXT": "Heaps Applied to Event Driven Mechanisms"}
{"DOCID": "2845", "TEXT": "A Buddy System Variation for Disk Storage Allocation: A generalization of the buddy system for storage allocation is described. The set of permitted block sizes {SIZE(i)}, i=0,n, must satisfy the condition SIZE(i)=SIZE(i-1)+SIZE(i-k(i)) where k may be any meaningful integral-valued function.  This makes it possible to force logical storage blocks to coincide with physical storage blocks, such as tracks and cylinders."}
{"DOCID": "2846", "TEXT": "Compressed Tries: This paper presents a new data structure, called a compressed trie or C-trie, to be used in information retrieval systems.  It has the same underlying m-ary tree structure as a trie, where m is a parameter of the trie, but whereas the fields of the nodes in a trie have to be large enough to hold a key or at least a pointer, the fields in a C-trie are only one bit long.  In the analysis part of the paper it will be shown that for a collection of n keys the retrieval time, measured in terms of bit inspections of one key, is of the order logm(n) and the storage requirement of the order n*(m+log2 n) bits.  This improvement in storage requirements and retrieval time is achieved at the cost of decreasing the flexibility of the structure, and therefore updating costs are increased. First the C-trie is analyzed as a data structure, and then several methods of its use for relatively static databases are discussed."}
{"DOCID": "2847", "TEXT": "Sampling from the Gamma Distribution on a Computer: This paper describes a method of generating gamma variates that appears to be less costly than Wallace's recently suggested method.  For large shape parameter (a); the cost of computation is proportional to (a), whereas Wallace's method is proportional to (a). Experimentation by Robinson and Lewis indicates that for small (a) the method suggested here also dominates methods recently suggested by Dieter and Ahrens, albeit those methods dominate for large (a).  The method suggested here uses the rejection technique."}
{"DOCID": "2848", "TEXT": "Synthesis of Decision Rules: Decision tables can be used as an effective tool during an interview to record the logic of processes to be automated.  The result of such an interview is not a structure of complete decision tables but rather sets of decision rules.  The purpose of this paper is to provide a procedure for synthesizing the decision rules and thus provide an aid in developing a structure of complete decision tables."}
{"DOCID": "2849", "TEXT": "Ethernet: Distributed Packet Switching for Local Computer Networks: Ethernet is a branching broadcast communication system for carrying digital data packets among locally distributed computing stations. The packet transport mechanism provided by Ethernet has been used to build systems which can be viewed as either local computer networks or loosely coupled multiprocessors. An Ethernet's shared communication facility, its Ether, is a passive broadcast medium with no central control.  Coordination of access to the Ether for packet broadcasts is distributed among the contending transmitting stations using controlled statistical arbitration. Switching of packets to their destinations on the Ether is distributed among the receiving stations using packet address recognition.  Design principles and implementation are described, based on experience with an operating Ethernet of 100 nodes along a kilometer of coaxial cable.  A model for estimating performance under heavy loads and a packet protocol for error controlled communication are included for completeness."}
{"DOCID": "2850", "TEXT": "Symbolic Execution and Program Testing: This paper describes the symbolic execution of programs.  Instead of supplying the normal inputs to a program (e.g. numbers) one supplies symbols representing arbitrary values.  The execution proceeds as in a normal execution except that values may be symbolic formulas over the input symbols.  The difficult, yet interesting issues arise during the symbolic execution of conditional branch type statements.  A particular system called EFFIGY which provides symbolic execution for program testing and debugging is also described.  It interpretively executes programs written in a simple PL/I style programming language. It includes many standard debugging features, the ability to manage and to prove things about symbolic expressions, a simple program testing manager, and a program verifier.  A brief discussion of the relationship between symbolic execution and program proving is also included."}
{"DOCID": "2851", "TEXT": "Formal Verification of Parallel Programs: Two formal models for parallel computation are presented: an abstract conceptual model and a parallel-program model.  The former model does not distinguish between control and data states.  The latter model includes the capability for the representation of an infinite set of control states by allowing there to be arbitrarily many instruction pointers (or processes) executing the program.  An induction principle is presented which treats the control and data state sets on the same ground.  Through the use of \"place variables,\" it is observed that certain correctness conditions can be expressed without enumeration of the set of all possible control states. Examples are presented in which the induction principle is used to demonstrate proofs of mutual exclusion. It is shown that assertions-oriented proof methods are special cases of the induction principle. A special case of the assertions method, which is called parallel place assertions, is shown to be incomplete.  A formalization of \"deadlock\" is then presented. The concept of a \"norm\" is introduced, which yields an extension, to the deadlock problem, of Floyd's technique for proving termination.  Also discussed is an extension of the program model which allows each process to have its own local variables and permits shared global variables.  Correctness of certain forms of implementation is also discussed. An Appendix is included which relates this work to previous work on the satisfiability of certain logical formulas."}
{"DOCID": "2852", "TEXT": "The Technology of Computer Center Management: A Proposed Course for Graduate Professional Programs in Computer Science or in Information Systems: McFarlan and Nolan have made a strong case for adding a course on information systems administration to the 13 courses proposed by the ACM Curriculum Committee on Computer Education for Management for Graduate Professional Programs in Information Systems.  This paper is a report on a course entitled, \"The Technology of Computer Center Management,\" which has been offered at Purdue for the past four years.  The course is suitable either for graduate professional programs in information systems or for graduate professional programs in computer science."}
{"DOCID": "2853", "TEXT": "A Numbering System for Permutations of Combinations"}
{"DOCID": "2854", "TEXT": "Multiprocessing Compactifying Garbage Collection (Corrigendum)"}
{"DOCID": "2855", "TEXT": "An Efficient List-Moving Algorithm Using Constant Workspace: An efficient algorithm is presented for moving arbitrary list structures, using no storage (apart from program variables) other than that required to hold the original list and the copy.  The original list is destroyed as it is moved.  No mark bits are necessary, but pointers to the copy must be distinguishable from pointers to the original.  The algorithm is superior in execution speed to previous algorithms for the same problem.  Some variations and extensions of the algorithm are discussed."}
{"DOCID": "2856", "TEXT": "The Synthetic Approach to Decision Table Conversion: Previous approaches to the problem of automatically converting decision tables to computer programs have been based on decomposition.  At any stage, one condition is selected for testing, and two smaller problems (decision tables with one less condition) are created.  An optimal program (with respect to average execution time or storage space, for example) is located only through implicit enumeration of all possible decision trees using a technique such as branch-and-bound.  The new approach described in this paper uses dynamic programming to synthesize an optimal decision tree from which a program can be created.  Using this approach, the efficiency of creating an optimal program is increased substantially, permitting generation of optimal programs for decision tables with as many as ten to twelve conditions."}
{"DOCID": "2857", "TEXT": "Referencing Lists by an Edge: An edge reference into a list structure is a pair of pointers to adjacent nodes.  Such a reference often requires little additional space, but its use can yield efficient algorithms. For instance, a circular link between the ends of a list is redundant\u0019 if the list is always referenced by that edge, and list traversal is easier when that link is null.  Edge references also allow threading of nonrecursive lists, can replace some header cells, and enhance the famous exclusive-or-trick to double-link lists"}
{"DOCID": "2858", "TEXT": "A Process for the Determination of Addresses in Variable Length Addressing: An algorithm is presented for the assignment of instruction addresses and formats under the following conditions: (1) the length of the instruction varies as a function of the distance of the instruction from its target; (2) there exists an optimality criterion which implies some preferential choices subject to the addressing constraints.  This may be, for example, achieving the smallest number of long instructions, in which case the total code length is minimized, or minimizing the assigned address of a specified point in the program.  The algorithm is suitable for arbitrary program structure and a choice of optimization criteria."}
{"DOCID": "2859", "TEXT": "Interference in Multiprocessor Computer Systems with Interleaved Memory: This paper analyzes the memory interference caused by several processors simultaneously using several memory modules.  Exect results are computed for a simple model of such a system.   The limiting value is derived for the relative degree of memory interference as the system size increases.  The model of the limiting behavior of the system yields approximate results for the simple model and also suggests that the results are valid for a much larger class of models, including those more nearly like real systems that the simple model are tested against some measurements of program behavior and simulations of systems using memory references from real programs.  The model results provide a good indication of the performance that should be expected from real system of this type."}
{"DOCID": "2860", "TEXT": "A Practitioner's Guide To Addressing Algorithms: This paper consolidates a number of popular rules of thumb which have been suggested for the design of record addressing algorithms, and discusses the applicability of these rules to large commercial databases.  Guidelines for selecting identifier transformations, overflow techniques, loading factors, bucket sizes, and loading order and considered.  Particular attention is focused on the reasonableness of common heuristics for determining primary or secondary bucket sizes. A mathematical model which explicitly considers storage device characteristics and time/space cost tradeoffs is used to analyze the effect of design parameters on overall system costs.  A specific design example is presented and solved."}
{"DOCID": "2861", "TEXT": "Production and Employment of Ph.D.'s in Computer Science"}
{"DOCID": "2862", "TEXT": "Analysis of the PFF Replacement Algorithm via a Semi-Markov Model: An analytical model is presented to estimate the performance of the Page Fault Frequency (PFF) replacement algorithm.  In this model, program behavior is represented by the LRU stack distance model and the PFF replacement algorithm is represented by a semi-Markov model.  Using these models, such parameters as the inter-page-fault interval distribution, the probability of the number of distinct pages being referenced during an inter-page-fault interval, etc. are able to be analytically determined.  Using these models to evaluate these parameter values permits study of the performance of the replacement algorithm by simulating the page fault events rather than every page reference event.  This significantly reduces the required computation time in estimating the performance of the PFF algorithm."}
{"DOCID": "2863", "TEXT": "VMIN-An Optimal Variable-Space Page Replacement Algorithm: A criterion for comparing variable space page replacement algorithms is presented.  An optimum page replacement algorithm, called VMIN, is described and shown to be optimum with respect to this criterion. The results of simulating VMIN, Denning's working set, and the page partitioning replacement algorithms on five virtual memory programs are presented to demonstrate the improvement possible over the known realizable variable space algorithms."}
{"DOCID": "2864", "TEXT": "Characteristics of Program Localities: The term \"locality\" has been used to denote that subset of a program's segments which are referenced during a particular phase of its execution.  A program's behavior can be characterized in terms of its residence in localities of various sizes and lifetimes, and the transitions between these localities. In this paper the concept of a locality is made more explicit through a formal definition of what constitutes a phase of localized reference behavior, and by a corresponding mechanism for the detection of localities in actual reference strings.  This definition provides for the existence of a hierarchy of localities at any given time, and the reasonableness of the definition is supported by examples taken from actual programs.  Empirical data from a sample of production Algol 60 programs is used to display distributions of locality sizes and lifetimes, and these results are discussed in terms of their implications for the modeling of program behavior and memory management in virtual memory systems."}
{"DOCID": "2865", "TEXT": "Verifying Properties of Parallel Programs: An Axiomatic Approach: An axiomatic method for proving a number of properties of parallel programs is presented. Hoare has given a set of axioms for partial correctness, but they are not strong enough in most cases. This paper defines a more powerful deductive system which is in some sense complete for partial correctness. A crucial axiom provides for the use of auxiliary variables, which are added to a parallel program as an aid to proving it correct.  The information in a partial correctness proof can be used to prove such properties as mutual exclusion, freedom from deadlock, and program termination.  Techniques for verifying these properties are presented and illustrated by application to the dining philosophers problem."}
{"DOCID": "2866", "TEXT": "Proving Monitors: Interesting scheduling and sequential properties of monitors can be proved by using state variables which record the monitor's history and by defining extended proof rules for their wait and signal operations. These two techniques are defined, discussed, and applied to examples to prove properties such as freedom from indefinitely repeated overtaking or unnecessary waiting upper bounds on queue lengths, and historical behavior."}
{"DOCID": "2867", "TEXT": "Modularization and Hierarchy in a Family of Operating Systems: This paper describes the design philosophy used in the construction of a family of operating systems. It is shown that the concepts of module and level do not coincide in a hierarchy of functions. Family members can share much software as a result of the implementation of run-time modules at the lowest system level."}
{"DOCID": "2868", "TEXT": "Reflections on an Operating System Design: The main features of a general purpose multiaccess operating system developed for the CDC 6400 at Berkeley are presented, and its good and bad points are discussed as they appear in retrospect.  Distinctive features of the design were the use of capabilities for protection, and the organization of the system into a sequence of layers, each building on the facilities provided by earlier ones and protecting itself from the malfunctions of later ones. There were serious problems in maintaining the protection between layers when levels were added to the memory hierarchy; these problems are discussed and a new solution is described."}
{"DOCID": "2869", "TEXT": "Security Kernel Validation in Practice: A security kernel is a software and hardware mechanism that enforces access controls within a computer system. The correctness of a security kernel on a PDP-11/45 is being proved.  This paper describes the technique used to carry out the first step of the proof: validating a formal specification of the program with respect to a axioms for a secure system."}
{"DOCID": "2870", "TEXT": "A Lattice Model of Secure Information Flow: This paper investigates mechanisms that guarantee secure information flow in a computer system. These mechanisms are examined within a mathematical framework suitable for formulating the requirements of secure information flow among security classes. The central component of the model is a lattice structure derived from the security classes and justified by the semantics of information flow.  The lattice properties permit concise formulations of the security requirements of different existing systems and facilitate the construction of mechanisms that enforce security. The model provides a unifying view of all systems that restrict information flow, enables a classification of them according to security objectives, and suggests some new approaches.  It also leads to the construction of automatic program certification mechanisms for verifying the secure flow of information through a program."}
{"DOCID": "2871", "TEXT": "Logical Analysis of Programs: Most present systems for verification of computer programs are incomplete in that intermediate inductive assertions must be provided manually by the user, termination is not proven, and incorrect programs are not treated.  As a unified solution to these problems, this paper suggests conducting a logical analysis of programs by using invariants which express what is actually occurring in the program. The first part of the paper is devoted to techniques for the automatic generation of invariants.  The second part provides criteria for using the invariants to check simultaneously for correctness (including termination) or incorrectness.  A third part examines the implications of the approach for the automatic diagnosis and correction of logical errors."}
{"DOCID": "2872", "TEXT": "A Counterintuitive Example of Computer Paging (Corrigendum)"}
{"DOCID": "2873", "TEXT": "LG: A Language for Analytic Geometry: A conversational programming language for analytic geometry is described, together with some aspects of its implementation.  The language allows the flexible definition of geometric objects and elements, computes their parameters, and displays the results. It also provides the capability of specifying a geometric figure via a collection of parameters and displaying various loci corresponding to these parameters. A third characteristic consists of the possibility of using this language to design other user oriented languages.  LG has been specifically designed for use by nonprogrammers; it is easy to learn and very close to the natural language used in geometry."}
{"DOCID": "2874", "TEXT": "A Comparative Evaluation of Versions of BASIC: From its inception, The BASIC language has grown in terms of its usage, scope of usage, and its features.  This article compares ten of the current versions of BASIC with each other, with two earlier versions, and with the proposed standard for minimal BASIC. The comparison is arranged by the features of the versions and by computational comparison of computation and times and processing costs."}
{"DOCID": "2875", "TEXT": "Development of an International System for Legal Protection of Computer Programs"}
{"DOCID": "2876", "TEXT": "Intentional Resolution of Privacy Protection in Database Systems: Traditionally, privacy protection in database systems is understood to be the control over what information a given user can get from a database. This paper is concerned with another, independent, dimension of privacy protection, the control over what a user is allowed to do with a piece of information supplied to him by the database.  The ability to condition the supply of information on its intended use is called here \"intentional resolution\" of privacy protection.  The practical importance of intentional resolution is demonstrated by several examples, and its realization is discussed.  It is shown that intentional resolution can be achieved, but that it involves a radical change from the traditional approach to the process of user-database interaction.  In particular, it appears to be necessary for the database to impose a certain amount of control over the internal behavior of users' programs which interact with it.  A model for user-database interaction which admits such a control is developed."}
{"DOCID": "2877", "TEXT": "A Program Data Flow Analysis Procedure: The global data relationships in a program can be exposed and codified by the static analysis methods described in this paper.  A procedure is given which determines all the definitions which can possibly \"reach\" each node of the control flow graph of the program and all the definitions that are \"live\" on each edge of the graph.  The procedure uses an \"interval\" ordered edge listing data structure and handles reducible and irreducible graphs indistinguishably."}
{"DOCID": "2878", "TEXT": "Joining Policies in a Multipriority Multiclass Batch Computer System: Consider a multipriority batch computer system which users from several different classes may join, its toll, service, and waiting charges.  Such a system is formulated here as a semi-Markov decision process, in which the aim of arriving users is to minimize their expected loss.  The optimal joining policy is one of arriving users who may join the system at some of its queues is a control limit policy, with a single control number for any possible queue and the user's class; a newly arriving user will join a queue that is not filled up to the control number corresponding to this queue and the user's class. In this paper control numbers, as well as lower and upper bounds for the control numbers and the capacities of the system's queues, are derived."}
{"DOCID": "2879", "TEXT": "Computer Science as Empirical Inquiry: Symbols and Search"}
{"DOCID": "2880", "TEXT": "A Fast Division Technique for Constant Divisors: A fast algorithm for division by constant divisors is presented.  The method has proved very useful implemented as microcode ona binary machine, and can be adapted directly into hardware.  The mathematical foundations of the algorithm are presented as well as some performance measures."}
{"DOCID": "2881", "TEXT": "A Counterintuitive Example of Computer Paging: A counterexample is exhibited to a natural conjecture concerning the optimal way to group records into pages in the independent reference model of computer paging (an organization is said to be optimal if the \"least recently used\" miss ratio is minimized)."}
{"DOCID": "2882", "TEXT": "A Stochastic Evaluation Model for Database Organization in Data Retrieval Systems: Experimental work in the valuation of large scale data retrieval systems has been scarce due to its difficulty and prohibitive cost. This paper discusses a simulation model of a data retrieval system which has the effect of significantly reducing the cost of experimentation and enabling research never attempted before.  The model is designed to estimate the retrieval workload of alternative data retrieval systems.  These data retrieval systems can be organized under several database organizations, including inverted list, threaded list, and cellular list organizations and hybrid combinations of these systems.  Effectiveness of the methodology is demonstrated by using the model to study the effect of database organizations in data retrieval systems.  In particular, the impact of query complexity is analyzed."}
{"DOCID": "2883", "TEXT": "An Application of Heuristic Search Methods to Edge and Contour Detection: This paper presents a method for detecting edges and contours in noisy pictures.  The properties of an edge are embedded in a figure of merit and the edge detection problem becomes the problem of minimizing the given figure of merit.  This problem can be represented as a shortest path problem on a graph and can be solved using well-known graph search algorithms. The relations between this representation of the minimization problem and a dynamic programming approach are discussed, showing that the graph search method can lead to substantial improvements in computing time.  Moreover, if heuristic search methods are used, the computing time will depend on the amount of noise in the picture.  Some experimental results are given; these show how various information about the shape of the contour of an object can be embedded in the figure of merit, thus allowing the extraction of contours from noisy picture and the separation of touching objects."}
{"DOCID": "2884", "TEXT": "Permutation Enumeration: Four New Permutation Algorithms: Classical permutation enumeration algorithms encounter special cases requiring additional computation every nth permutation when generating the n! permutations on n marks.  Four new algorithms have the attribute that special cases occur every n(n-1)permutations. Two of the algorithms produce the next permutation with a single exchange of two marks.  The other two algorithms infrequently exchange more than two marks, but the rules for generating the next permutation are very simple.  Performance tests which have counted execution of assignment statements, comparisons, arithmetic operations, and subscripted array references have shown superiority of the new algorithms compared to Boothroyd's implementation of M. B. Well's algorithm and Ehrlich's implementation of the Johnson-Trotter algorithm."}
{"DOCID": "2885", "TEXT": "On Self-Organizing Sequential Search Heuristics: This paper examines a class of heuristics for maintaining a sequential list in approximately optimal order with respect to the average time required to search for a specified element, assuming that each element is searched for with a fixed probability independent of previous searches performed.  The \"move to front\" and \"transposition\" heuristics are shown to be optimal to within a constant factor, and the transposition rule is shown to be the more efficient of the two. Empirical evidence suggests that transposition is in fact optimal for any distribution of search probabilities."}
{"DOCID": "2886", "TEXT": "Semantic Evaluation from Left to Right: This paper describes attribute grammars and their use for the definition of programming languages and compilers; a formal definition of attribute grammars and a discussion of some of its important aspects are included. The paper concentrates on the evaluation of semantic attributes in a few passes from left to right over the derivation tree of a program.  A condition for an attribute grammar is given which assures that the semantics of any program can be evaluated in a single pass over the derivation tree, and an algorithm is discussed which decides how many passes from left to right are in general necessary, given the attribute grammar. These notions are explained in terms of an example grammar which describes the scope rules of Algol 60.  Practical questions, such as the relative efficiency of different evaluation schemes, and the ease of adapting the attribute grammar of a given programming language to the left-to-right evaluation scheme are discussed."}
{"DOCID": "2887", "TEXT": "A Study of Errors, Error-Proneness, and Error Diagnosis in Cobol: This paper provides data on Cobol error frequency for correction of errors in student-oriented compilers, improvement of teaching, and changes in programming language.  Cobol was studied because of economic importance, widespread usage, possible error-including design, and lack of research.  The types of errors were identified in a pilot study; then, using the 132 error types found, 1,777 errors were classified in 1,4000 runs of 73 Cobol students.  Error density was high: 20 percent of the types contained 80 percent of the total frequency, which implies high potential effectiveness for software based correction of Cobol.  Surprisingly, only four high-frequency errors were error-prone, which implies minimal error inducing design. 80 percent of Cobol misspellings were classifiable in the four error categories of previous researchers, which implies that Cobol misspellings are correctable by existent algorithms.  Reserved word usage was not error-prone, which implies minimal interference with usage of reserved words.  Over 80 percent of error diagnosis was found to be inaccurate. Such feedback is not optimal for users, particularly for the learning user of Cobol."}
{"DOCID": "2888", "TEXT": "Information Reference Coding: Items in business systems have to be identified by reference codes, which can later be used as data codes and file keys in an associated data processing system.  In business systems associated with large collections of integrated files (database) it is vital to assign codes in a methodical way so as to control future extension and changes while maintaining correct program action. The principles of methodical coding are discussed, and the way in which logical connections between data items must be reflected in the reference code framework is shown through a set-theoretic information model."}
{"DOCID": "2889", "TEXT": "Performance of Height-Balanced Trees: This paper presents the results of simulations that investigate the performance of height-balanced (HB[k]) trees.  It is shown that the only statistic of HB[1] trees (AVL trees) that is a function of the size of the tree is the time to search for an item in the tree.  For sufficiently large trees, the execution times of all procedures for maintaining HB[1] trees are independent of the size of the tree. In particular, an average of .465 restructures are required per insertion, with an average of 2.78 nodes revisited to restore the HB[1] property; an average of .214 restructures are required per deletion, with an average of 1.91 nodes revisited to restore the HB[1] property.  Moreover,the execution times of procedures for maintaining HB[k] trees, for k>1, are also independent of the size of the tree except for the average number of nodes revisited on a delete operation in order to restore the HB[k] property on trace back. The cost of maintaining HB[k] trees drops sharply as the allowable imbalance (k) increases.  Both analytical and experimental results that show the cost of maintaining HB[k] trees as a function of k are discussed."}
{"DOCID": "2890", "TEXT": "On Quadratic Adaptive Routing Algorithms: Two analytic models of a store-and-forward communications network are constructed, one to find the optimal message routing and the other to illustrate the equilibrium (stationary state) maintained by an adaptive routing algorithm.  These models show that adaptive routing does not satisfy the necessary conditions for an optimal routing,  Adaptive routing tends to overuse the direct path and underuse alternate routes because it does not consider the impact of its current routing decision on the future state of the network.  The form of the optimality conditions suggests that a modification of the adaptive algorithm will result in optimality.  The modification requires the substitution of a quadratic bias term instead of a linear one in the routing table maintained at each network node.  Simulation results are presented which confirm the theoretical analysis for a simple network."}
{"DOCID": "2891", "TEXT": "An Anomaly in Disk Scheduling: A Comparison of FCFS and SSTF Seek Scheduling Using an Empirical Model for Disk Accesses: A model for disk accesses based on published measurements is developed.  The model is used to show that under highly probable conditions, FCFS seek scheduling is superior to SSTF scheduling in the sense of having a lower mean queue length.  A simple example of an arrival sequence illustration this anomaly is presented."}
{"DOCID": "2892", "TEXT": "A Study of Line Overhead in the Arpanet: The form, extent, and effect of the communication line overhead in the ARPANET are considered. The source of this over head is separated into various levels of protocol hierarchy and the characteristics of each level are summarized.  Then the line efficiency for various models of system use is studied. Some measurements of line efficiency for the ARPANET are presented and by extrapolation these measurements are used to anticipate overhead in a heavily loaded network. Similar results are derived for a recently proposed network protocol and compared with those for the current system."}
{"DOCID": "2893", "TEXT": "Computers as an Innovation in American Local Governments: Computers and electronic data processing are a major technological innovation in the operations of American local government. This paper establishes that there is substantial variation among the larger local governments in the rate at which they adopt computer technology, in the level of financial support they provide for EDP, and in the extensiveness and sophistication of their automated applications.  The central question addressed is: What might explain the differences between governments in the extent to which they adopt and use computers?  Hypotheses are tested for several streams of explanatory factors, using data from more than 500 city and county governments. The findings identify certain local government milieus which are particularly conducive to higher levels of computer innovation.  Somewhat unexpected findings reveal the significant impact of the distribution of control over EDP decisions and the dominant political values within the government. Other important factors include the measured need for computer applications and the presence of external funding support for computing.  Finally, the paper suggests a framework for identifying the key determinants of other technological innovations."}
{"DOCID": "2894", "TEXT": "A Methodology for Interactive Computer Service Measurement: A measurement methodology applicable to in teractive computer service is described.  Its primary purpose is to enable external, user-oriented assessment of computer performance, instead of the more frequently used in ternal system measurement techniques. The NBS Network Measurement System is employed as the external measurement tool.  Example data have been collected and analyzed.  A demonstration of the methodology, leading to a pragmatic figure-of-merit evaluation of results, is included."}
{"DOCID": "2895", "TEXT": "A Language for Formal Problem Specification: A language for specifying the in tended behavior of communicating parallel processes is described. The specifications are constrain ts on the order in which events of a computation can occur.  The language is used to write specifications of the readers/writers problem and the writer priority of the second readers/writers problem."}
{"DOCID": "2896", "TEXT": "An Exercise in Proving Parallel Programs Correct: A parallel program, Dijkstra's on-the-fly garbage collector, is proved correct using a proof method developed by Owicki.  The fine degree of in terleaving in this program makes it especially difficult to understand, and complicates the proof greatly.  Difficulties with proving such parallel programs correct are discussed."}
{"DOCID": "2897", "TEXT": "A Case Study of a New Code Generation Technique for Compilers: Recent developments in optimizing techniques have allowed a new design for compilers to emerge. Such a compiler translates the parsed source code into lower level code by a sequence of steps.  Each step expands higher level statements into blocks of lower level code and then performs optimizations on the result.  Each statement has only one possible expansion-the task of tailoring this code to take advantage of any special cases is done by the optimizations. This paper provides evidence that this strategy can indeed result in good object code.  The traditionally difficult PL/I concatenate statement was investigated as a detailed example.  A set of fairly simple optimizations was identified which allow the compiler to produce good code. More elaborate optimizations can further improve the object code. For most contexts of the concatenate statement, the code produced by a compiler using the expansion-optimization strategy described above compares favorably with the code produced by a conventional PL/I optimizing compiler."}
{"DOCID": "2898", "TEXT": "A Conceptual Framework for a Nonprocedural Programming Language: A sequential programming language forces the programmer to prescribe explicitly the order in which the operations in his program have to be executed, even if the order is not relevant to the solution of his problem.  The requirement to indicate irrelevant sequencing can be removed if the language provides facilities for specifying a task in a nonprocedural manner.  In general, a program specified in this way will allow concurrent evaluation.  This paper describes a conceptual framework for a high level programming language providing both nonprocedural and sequential facilities.  Within a program, nonprocedural and sequential program modules may be nested freely."}
{"DOCID": "2899", "TEXT": "A Survey of Computer Science Offerings In Small Liberal Arts Colleges.: Recent curricular development in computer science together with student in terest in pursuing topics in computer science beyond the usual programming courses have encouraged small liberal arts colleges to expand their offerings.  This paper summarizes the results of a survey taken to determine the type of computer science programs being offered in these colleges.  The results indicate that over half of these colleges either have no computer science program or offer only programming courses."}
{"DOCID": "2900", "TEXT": "Some Theorems to Aid in Solving the File Allocation Problem: The file allocation problem-i.e. the problem of finding the optimal set of network sites at which to locate copies of a file-is known to be, in general, polynomial complete.  Heuristics and other aids to finding optimal, or near-optimal, solutions are therefore much needed.  In this paper we present three theorems which can be applied a priori to indicate that certain sites should (or should not) be included in an optimal allocation."}
{"DOCID": "2901", "TEXT": "An Encoding Method for Multifield Sorting and Indexing: Sequences of character strings with an order relation imposed between sequences are considered. An encoding scheme is described which produces a single, order-preserving string from a sequence of strings.  The original sequence can be recovered from the encoded string, and one sequence of strings precedes another if and only if the encoding of the first precedes the encoding of the second.  The strings may be variable length, without a maximum length restriction, and no symbols need be reserved for control purposes.  Hence any symbol may occur in any string.  The scheme is useful for multifield sorting, multifield indexing, and other applications where ordering on more than one field is important."}
{"DOCID": "2902", "TEXT": "Dynamic Memory Allocation in Computer Simulation: This paper investigates the performance of 35 dynamic memory allocation algorithms when used to service simulation programs as represented by 18 test cases.  Algorithm performance was measured in terms of processing time, memory usage, and external memory fragmentation.  Algorithms main taining separate free space lists for each size of memory block used tended to perform quite well compared with other algorithms.  Simple algorithms operating on memory ordered lists (without any free list) performed surprisingly well.  Algorithms employing power-of-two block sizes had favorable processing requirements but generally unfavorable memory usage.  Algorithms employing LIFO, FIFO, or memory ordered free lists generally performed poorly compared with others."}
{"DOCID": "2903", "TEXT": "Improving Programs by the Introduction of Recursion: A new technique of program transformation, called \"recursion in troduction,\" is described and applied to two algorithms which solve pattern matching problems. By using recursion in troduction, algorithms which manipulate a stack are first translated into recursive algorithms in which no stack operations occur.  These algorithms are then subjected to a second transformation, a method of recursion elimination called \"tabulation,\" to produce programs with a very efficient running time.  In particular, it is shown how the fast linear pattern matching algorithm of Knuth, Morris, and Pratt can be derived in a few steps from a simple nonlinear stack algorithm."}
{"DOCID": "2904", "TEXT": "An Algorithm for Reduction of Operator Strength: A simple algorithm which uses an indexed temporary table to perform reduction of operator strength in strongly connected regions is presented.  Several extensions, including linear function test replacement, are discussed.  These algorithms should fit well into an integrated package of local optimization algorithms."}
{"DOCID": "2905", "TEXT": "Perfect Hashing Functions: A Single Probe Retrieving Method for Static Sets: A refinement of hashing which allows retrieval of an item in a static table with a single probe is considered.  Given a set I of identifiers, two methods are presented for building, in a mechanical way, perfect hashing functions, i.e. functions transforming the elements of I into unique addresses. The first method, the \"quotient reduction\" method, is shown to be complete in the sense that for every set I the smallest table in which the elements of I can be stored and from which they can be retrieved by using a perfect hashing function constructed by this method can be found.  However, for nonuniformly distributed sets, this method can give rather sparse tables. The second method, the \"remainder reduction\" method, is not complete in the above sense, but it seems to give minimal (or almost minimal) tables for every kind of set.  The two techniques are applicable directly to small sets.  Some methods to extend these results to larger sets are also presented.  A rough comparison with ordinary hashing is given which shows that this method can be used conveniently in several practical applications."}
{"DOCID": "2906", "TEXT": "A Very High Level Programming Language for Data Processing Applications: Application development today is too labor-in tensive. In recent years, very high-level languages have been increasingly explored as a solution to this problem.  The Business Definition Language (BDL) is such a language, one aimed at business data processing problems.  The concepts in BDL mimic those which have evolved through the years in businesses using manual methods.  This results in three different sublanguages or components: one for defining the business forms, one for describing the business organization, and one for writing calculations."}
{"DOCID": "2907", "TEXT": "The Optimal Approach to Recursive Programs: The classical fixed poin t approach toward recursive programs suggests choosing the \"least defined fixed poin t\" as the most appropriate solution to a recursive program.  A new approach is described which in troduction an \" optimal fixed point,\" which, in contrast to the least defined fixed poin t, embodies the maximal amount of valuable information embedded in the program.  The practical implications of this approach are discussed and techniques for proving properties of optimal fixed poin t are given.  The presentation is informal, with emphasis on examples."}
{"DOCID": "2908", "TEXT": "A Note On Reflection-Free Permutation Enumeration"}
{"DOCID": "2909", "TEXT": "What Can We Do about the Unnecessary Diversity of Notation for Syntactic Definitions?"}
{"DOCID": "2910", "TEXT": "Equivalence of Hough Curve Detection to Template Matching"}
{"DOCID": "2911", "TEXT": "Anomalous Behavior of the Fifty-Percent Rule in Dynamic Memory Allocation: This paper reports simulation data showing that, in dynamic memory allocation, the average free-to-allocated-block ratio can differ considerably and in both directions from the predictions of the 50 percent rule.  A new derivation is given, and it is shown that previous derivations make an assumption that may be violated frequently.  On the basis of the simulation data and the derivation, it is hypothesized that the anomalous behavior results from the combined effects of systematic placement and the statistics of the release process.  Additional simulations support this hypothesis.  Systematic placement, which refers to the natural convention of always allocating storage requests against the same end of the free block selected by the allocation strategy, tends to order blocks within contiguous groups according to their allocation time.  The degree of anomalous behavior depends on the extent to which allocated blocks are released in the order of their allocation.  For non-Markovian release processes, the extent of the correlation between allocation order and release order varies approximately inversely with the coefficient of variation of the memory residence time distribution. The simulations show that allocation efficiency depends strongly on the residence time distribution; efficiency decreases as the distribution's coefficient of variation increases.  Some practical implications are briefly discussed."}
{"DOCID": "2912", "TEXT": "Concurrent Reading and Writing: The problem of sharing data among asynchronous process is considered.  It is assumed that only one process at a time can modify the data, but concurrent reading and writing is permitted.  Two general theorems are proved, and some algorithms are presented to illustrate their use.  These include a solution to the general problem in which a read is repeated if it might have obtained an incorrect result, and two techniques for transmitting messages between processes. These solutions do not assume any synchronizing mechanism other than data which can be written by one process and read by other processes."}
{"DOCID": "2913", "TEXT": "The Aliasing Problem in Computer-Generated Shaded Images: Certain defects, such as jagged edges and disappearing detail, have long been an annoyance in digitally generated shaded images.  Although increasing the resolution or defocusing the display can attenuate them, an understanding of these defects leads to more effective methods.  This paper explains the observed defects in terms of the aliasing phenomenon inherent in sampled signals and discusses prefiltering as a recognized cure.  A method for evaluating filters is presented, the application of prefiltering to hidden-surface algorithms is discussed, and an implementation of a filtering tiler is shown accompanied by examples of its effectiveness."}
{"DOCID": "2914", "TEXT": "Use of the LRU Stack Depth Distribution for Simulation of Paging Behavior: Two families of probability distributions were needed for use by a virtual memory simulation model: headway between page fault distributions, and working set size distributions.  All members of both families can be derived from the LRU stack depth distribution. Simple expressions for the computation of both kinds of distributions are given.  Finally, examples are given of both families of distributions as computed from a published stack depth distribution."}
{"DOCID": "2915", "TEXT": "Considerations for Future Programming Language Standards Activities: This paper reviews the current state of programming language standards activities with respect to the anomalies which exist between the various published and proposed standards for Fortran, Cobol, PL/I, and Basic.  Proposals are made for the inclusion of formalisms within future standards and the extension of the standards to include additional items such as error conditions and documentation."}
{"DOCID": "2916", "TEXT": "A Fast String Searching Algorithm: An algorithm is presented that searches for the location, \"i,\" of the first occurrence of a character string, \"pat,\" in another string, \"string.\" During the search operation, the characters of pat are matched starting with the last character of pat.  The information gained by starting the match at the end of the pattern often allows the algorithm to proceed in large jumps through the text being searched.  Thus the algorithm has the unusual property that, in most cases, not all of the first i characters of string are inspected.  The number of characters actually inspected (on the average) decreases as a function of the length of pat.  For a random English pattern of length 5, the algorithm will typically inspect i/4 characters of string before finding a match at i.  Furthermore, the algorithm has been implemented so that (on the average) fewer than i+patlen machine instructions are executed.  These conclusions are supported with empirical evidence and a theoretical analysis of the average behavior of the algorithm. The worst case behavior of the algorithm is linear in i+patlen, assuming the availability of array space for tables linear in patlen plus the size of the alphabet."}
{"DOCID": "2917", "TEXT": "SITAR: An Interactive Text Processing System for Small Computers (Corrigendum)"}
{"DOCID": "2918", "TEXT": "Multiprocessor Memory Organization and Memory Interference: The structure of shared memory in a multiprocessor computer system is examined with particular attention to nonin terleaved memory.  Alternative memory organizations are compared and it is shown that a home memory organization, in which each processor is associated with one or more memories in which its address space is concentrated, is quite effective in reducing memory in terference.  Home memory organization is shown to be particularly suited to certain specialized computation problems as well as to possess advantages in terms of in terference and reliability for general purpose computation.  Results for in terleaved memory are drawn from previous work and are used for comparison.  Trace-driven simulations are used to verify the conclusions of the analysis."}
{"DOCID": "2919", "TEXT": "The Programmer's Workbench-A Machine for Software Development: On almost all software development projects the assumption is made that the program development function will be done on the same machine on which the eventual system will run.  It is only when this production machine is unavailable or when its programming environment is totally inadequate that alternatives are considered.  In this paper it is suggested that there are many other situations where it would be advantageous to separate the program development and main tenance function onto a specialized computer which is dedicated to that purpose.  Such a computer is here called a Programmer's Workbench.  The four basic sections of the paper in troduce the subject,outline the general concept, discuss areas where such an approach may prove beneficial, and describe an operational system utilizing this concept."}
{"DOCID": "2920", "TEXT": "Game Interpretation of the Deadlock Avoidance Problem: The deadlock avoidance problem may be defined informally as the determination, from some a priori information about the processes, resources, operating system, etc., of the \"safe situations\" which may be realized without endangering the smooth running of the system.  When each process specifies its future needs by a flowchart of need-defined steps, a global approach to the phenomenon and its in terpretation as a game between the operating system and the processes allows formalization of risk and safety concepts. The bipartite graph representation of this game may then be used to construct explicitly the set of safe states and to study their properties."}
{"DOCID": "2921", "TEXT": "Regular Right Part Grammars and Their Parsers: This paper in troduces an alternative to context-free grammars called regular right part (RRP) grammars, which resemble PASCAL syntax diagrams.  Formally, RRP grammars have production right parts, which are nondeterministic finite state machines (FSMs), and, as a special case, regular expressions, since these can be converted to FSMs.  RRP grammars describe the syntax of programming languages more concisely and more understandably than is possible with CF grammars.  Also in troduced is a class of parsers, RRP LR(m, k) parsers, which includes the CF LR(k) parsers and provides the same advantages.  Informally, an RRP LR(m, k) parser can determine the right end of each handle by considering at most k symbols to the right of the handle and the left end, after the right end has been found, by considering at most m symbols to the left of the handle.  A mechanism for determining the left end is required because there is no bound on the length of the handle."}
{"DOCID": "2922", "TEXT": "Two-Level Control Structure for Nondeterministic Programming: The basic ideas of nondeterministic programming are critically reconsidered to single out a proper attitude and programming style for language allowing direct control of nondeterministic features. The proposed attitude aims at retaining the purity of the nondeterministic formulation of search processes on one level (the attempt level), deferring the coordination of problem solving efforts to another (the choice level).  The feasibility of recognizing these two levels is discussed, stressing that the structure to be managed at the choice level is a free of contexts. The leaves are computational environments, each holding an alternative under inspection, while the other nodes are associated with choice poin ts. According to the proposed programming style, a generative function is associated with each choice poin t, which expresses the desired choice strategy. The main advantage on this approach is the localization of the search strategies: Each nonterminal node of the tree keeps track of the state of the computation as it was when the choice poin t was last interrogated, holding at the same time the strategy to coordinate the available alternatives.  Examples are given in term of ND-Lisp, an extension of Lisp designed and implemented according to these guidelines."}
{"DOCID": "2923", "TEXT": "High-Level Data Flow Analysis: In contrast to the predominant use of low-level in termediate text, high-level data flow analysis deals with programs essentially at source level and exploits the control flow information implicit in the parse tree.  The need for high-level flow analysis arises from several aspects of recent work on advanced methods of program certification and optimization. This paper proposes a simple general method of high-level data flow analysis that allows free use of escape and jump statements, avoids large graphs when compiling large programs, facilitates updating of data flow information to reflect program changes, and derives new global information helpful in solving many familiar global flow analysis problems.  An illustrative application to live variable analysis is presented. Many of the graphs involved are constructed and analyzed before any programs are compiled, thus avoiding certain costs that low-level methods incur repeatedly at compile time."}
{"DOCID": "2924", "TEXT": "An Interactive Computer Graphics Approach to Surface Representation: An in teractive computer graphics method has been developed for the rapid generation of arbitrary shaped three-dimensional surfaces.  The method is a synthesis of spline theory and algorithms, an in teractive means for man-machine communication, and software for static or dynamic graphics display.  The basic technique employed is a modified lofting method on which sectional curves are represented by uniform B-splines and the surface is in terpolated between sections by Cardinal splines.  Among the features of this method are algorithms which enable in teractive modification of the B-spline representation of the sectional curves.  At all stages of the process, the spatial information is graphically displayed to the user.  Complex surfaces can be created by the combination of a number of shapes that have been separately generated and automatically joined.  The system has been successfully in terfaced to a variety of analytical routines for structural, medical and graphical applications."}
{"DOCID": "2925", "TEXT": "Optimal Surface Reconstruction from Planar Contours: In many scientific and technical endeavors, a three-dimensional solid must be reconstructed from serial sections, either to aid in the comprehension of the object's structure or to facilitate its automatic manipulation and analysis.  This paper presents a general solution to the problem of constructing a surface over a set of cross-sectional contours. This surface, to be composed of triangular tiles, is constructed by separately determining an optimal surface between each pair of consecutive contours. Determining such a surface is reduced to the problem of finding certain minimum cost cycles in a directed toroidal graph.  A new fast algorithm for finding such cycles is utilized.  Also developed is a closed-form expression, in term of the number of contour poin ts, for an upper bound on the number of operations required to execute the algorithm.  An illustrated example which involves the construction of a minimum area surface describing a human head is included."}
{"DOCID": "2926", "TEXT": "Pagination of B*-Trees with Variable-Length Records: A strategy is presented for pagination of B*-trees with variable-length records.  If records of each length are uniformly distributed within the file, and if a wide distribution of record lengths exists within the file, then this strategy results in shallow trees with fast access times.  The performance of this strategy in an application is presented, compared with that of another strategy, and analyzed."}
{"DOCID": "2927", "TEXT": "Some New Upper Bounds on the Generation of Prime Numbers: Given an integer N, what is the computational complexity of finding all the primes less than N?  A modified sieve of Eratosthenes using doubly linked lists yields an algorithm of O(N) arithmetic complexity.  This upper bound is shown to be equivalent to the theoretical lower bound for sieve methods without preprocessing.  Use of preprocessing techniques involving space-time and additive-multiplicative tradeoffs reduces this upper bound to O(N/log logN) and the bit complexity to O(N logN log log logN). A storage requirement is described using O(N logN/log logN) bits as well."}
{"DOCID": "2928", "TEXT": "Hardware Estimation of a Process' Primary Memory Requirements: A minor hardware extension to the Honeywell 6180 processor is demonstrated to allow the primary memory requirements of a process in Multics to be approximated. The additional hardware required for this estimate to be computed consists of a program accessible register containing the miss rate of the associative memory used for page table words.  This primary memory requirement estimate was employed in an experimental version of Multics to control the level of multiprogramming in the system and to bill for memory usage.  The resulting system's tuning parameters display configuration insensitivity, and it is conjectured that the system would also track shifts in the referencing characteristics of its workload and keep the system in tune."}
{"DOCID": "2929", "TEXT": "An Analysis of Inline Substitution for a Structured Programming Language: An optimization technique known as inline substitution is analyzed.  The optimization consists of replacing a procedure invocation by a modified copy of the procedure body.  The general problem of using inline substitution to minimize execution time subject to size constrain ts is formulated, and an approximate algorithmic solution is proposed.  The algorithm depends on run-time statistics about the program to be optimized.  Preliminary results for the CLU structured programming language indicate that, in programs with a low degree of recursion, over 90 percent of all procedure calls can be eliminated, with little increase in the size of compiled code and a small savings in execution time.  Other conclusions based on these results are also presented."}
{"DOCID": "2930", "TEXT": "The GRE Advanced Test in Computer Science: This report describes the Advanced Test in Computer Science which was recently in troduced in the Graduate Record Examination Program.  The GRE program is described in general, and, the events leading to the establishment of the Advanced Computer Science Test are discussed.  Content specifications and their rationale are given.  A set of sample questions is included."}
{"DOCID": "2931", "TEXT": "Logic and Programming Languages: Logic has been long in terested in whether answers to certain questions are computable in principle, since the outcome puts bounds on the possibilities of formalization.  More recently, precise comparisons in the efficiency of decision methods have become available through the developments in complexity theory. These, however, are applications to logic, and a big question is whether methods of logic have significance in the other direction for the more applied parts of computability theory.  Programming languages offer an obvious opportunity as their syntactic formalization is well advanced; however, the semantical theory can hardly be said to be complete.  Though we have many examples, we have still to give wide-ranging mathematical answers to these queries:  What is a machine? What is a computable process?  How (or how well) does a machine simulate a process?  Programs naturally enter in giving descriptions of processes. The definition of the precise meaning of a program then requires us to explain what are the objects of computation (in a way, the statics of the problem) and how they are to be transformed (the dynamics). So far the theories of automata and of nets, though most in teresting for dynamics, have formalized only a portion of the field, and there has been perhaps too much concentration on the finite-state and algebraic aspects.  It would seem that the understanding of higher-level program features involves us with infinite objects and forces us to pass through several levels of explanation to go from the conceptual ideas to the final simulation on a real machine.  These levels can be made mathematically exact if we can find the right abstractions to represent the necessary structures. The experience of many independent workers with the method of data types as lattices (or partial orderings) under an information content ordering, and with their continuous mappings, has demonstrated the flexibility of this approach in providing definitions and proofs, which are clean and without undue dependence on implementations.  Nevertheless much remains to be done in showing how abstract conceptualizations can (or cannot) be actualized before we can say we have a unified theory."}
{"DOCID": "2932", "TEXT": "Complexity of Computations: The framework for research in the theory of complexity of computations is described, emphasizing the in terrelation between seemingly diverse problems and methods.  Illustrative examples of practical and theoretical significance are given.  Directions for new research are discussed."}
{"DOCID": "2933", "TEXT": "Another Advantage of Keyword Notation for Parameter Communication with Subprograms"}
{"DOCID": "2934", "TEXT": "Comment on Computing the k Shortest Paths in a Graph"}
{"DOCID": "2935", "TEXT": "Production and Employment of Ph.D.'s in Computer Science-1976 (Corrigendum)"}
{"DOCID": "2936", "TEXT": "An Efficient Data Structure for the Simulation Event Set: Recently algorithms have been presented for the realization of event scheduling routines suitable for general purpose discrete event simulation systems. Several exhibited a performance superior to that of commonly used simple linked list algorithms.  In this paper a new event scheduling algorithm is presented which improves on two aspects of the best of the previously published algorithms.  First, the new algorithm's performance is quite insensitive to skewed distributions, and second, its worst-case complexity is O( n), where n is the number of events in the set.  Furthermore, tests conducted to estimate the average complexity showed it to be nearly independent of n."}
{"DOCID": "2937", "TEXT": "An Experimental Evaluation of Data Type Conventions: The language in which programs are written can have a substantial effect on the reliability of the resulting programs.  This paper discusses an experiment that compares the programming reliability of subjects using a statically typed language and a \"typeless\" language.  Analysis of the number of errors and the number of runs containing errors shows that, at least in one environment, the use of a statically typed language can increase programming reliability. Detailed analysis of the errors made by the subjects in programming solutions to reasonably small problems shows that the subjects had difficulty manipulating the representation of data."}
{"DOCID": "2938", "TEXT": "Toward a Discipline of Real-Time Programming: Programming is divided into three major categories with increasing complexity of reasoning in program validation: sequential programming, multiprogramming, and real-time programming.  By adhering to a strict programming discipline and by using a suitable high-level language molded after this discipline, the complexity of reasoning about concurrency and execution time constrain ts may be drastically reduced. This may be the only practical way to make real-time systems analytically verifiable and ultimately reliable.  A possible discipline is outlined and expressed in terms of the language Modula."}
{"DOCID": "2939", "TEXT": "Abstraction Mechanisms in CLU: CLU is a new programming language designed to support the use of abstractions in program construction. Work in programming methodology has led to the realization that three kinds of abstractions-procedural, control, and especially data abstractions-are useful in the programming process.  Of these, only the procedural abstraction is supported well by conventional languages, through the procedure or subroutine. CLU provides, in addition to procedures, novel linguistic mechanisms that support the use of data and control abstractions.  This paper provides an in troduction to the abstraction mechanisms in CLU.  By means of programming examples, the utility of the three kinds of abstractions in program construction is illustrated, and it is shown how CLU programs may be written to use and implement abstractions.  The CLU library, which permits incremental program development with complete type checking performed at compile time, is also discussed."}
{"DOCID": "2940", "TEXT": "Abstraction and Verification in Alphard: Defining and Specifying Iteration and Generators: The Alphard \"form\" provides the programmer with a great deal of control over the implementation of abstract data types.  In this paper the abstraction techniques are extended from simple data representation and function definition to the iteration statement, the most important poin t of interaction between data and the control structure of the language itself.  A means of specializing Alphard's loops to operate on abstract entities without explicit dependence on the representation of those entities is in troduced. Specification and verification techniques that allow the properties of the generators for such iterations to be expressed in the form of proof rules are developed. Results are obtained that for common special cases of these loops are essentially identical to the corresponding constructs in other languages.  A means of showing that a generator will terminate is also provided."}
{"DOCID": "2941", "TEXT": "Early Experience with Mesa: The experiences of Mesa's first users-primarily its implementers-are discussed, and some implications for Mesa and similar programming languages are suggested. The specific topics addressed are: module structure and its use in defining abstractions, data-structuring facilities in Mesa, an equivalence algorithm for types and type coercions, the benefits of the type system and why it is breached occasionally, and the difficulty of making the treatment of variant records safe."}
{"DOCID": "2942", "TEXT": "An Algol-Based Implementation of SNOBOL 4 Patterns"}
{"DOCID": "2943", "TEXT": "Lucid, a Nonprocedural Language with Iteration: Lucid is a formal system in which programs can be written and proofs of programs carried out. The proofs are particularly easy to follow and straightforward to produce because the statements in a Lucid program are simply axioms from which the proof proceeds by (almost) conventional logical reasoning, with the help of a few axioms and rules of inference for the special Lucid functions.  As a programming language, Lucid is unconventional because, among other things, the order of statements is irrelevant and assignment statements are equations.  Nevertheless, Lucid programs need not look much different than iterative programs in a conventional structured programming language using assignment and conditional statements and loops."}
{"DOCID": "2944", "TEXT": "Shifting Garbage Collection Overhead to Compile Time: This paper discusses techniques which enable automatic storage reclamation overhead to be partially shifted to compile time.  The paper assumes a transaction oriented collection scheme, as proposed by Deutsch and Bobrow, the necessary features of which are summarized.  Implementing the described optimizations requires global flow analysis to be performed on the source program.  It is shown that at compile time certain program actions that affect the reference counts of cells can be deduced.  This information is used to find actions that cancel when the code is executed and those that can be grouped to achieve improved efficiency."}
{"DOCID": "2945", "TEXT": "Certification of Programs for Secure Information Flow: This paper presents a certification mechanism for verifying the secure flow of information through a program.  Because it exploits the properties of a lattice structure among security classes, the procedure is sufficiently simple that it can easily be included in the analysis phase of most existing compilers.  Appropriate semantics are presented and proved correct.  An important application is the confinement problem: The mechanism can prove that a program cannot cause supposedly nonconfidential results to depend on confidential input data."}
{"DOCID": "2946", "TEXT": "An Alternative to Event Queues for Synchronization in Monitors: In the monitor concept, as proposed by Brinch Hansen and Hoare, event are used for synchronization. This paper describes another synchronizing primitive which is nearly as expressive as the conditional wait, but can be implemented more efficiently.  An implementation of this primitive in terms of P and V operations is given together with a correctness proof. Two examples are presented: the readers and writers problem and the problem of information streams sharing a finite buffer pool."}
{"DOCID": "2947", "TEXT": "SITAR: An Interactive Text Processing System for Small Computers: SITAR, a low-cost in teractive text handling and text analysis system for nontechnical users, is in many ways comparable to in teractive bibliographical search and retrieval systems, but has several additional features. It is implemented on a PDP/11 time-sharing computer invoked by a CRT with microprogrammed editing functions.  It uses a simple command language designating a function, a file, and a search template consisting of the textual string desired and strings delimiting the context in which the hit is to be delivered.  Extensive experience with SITAR shows that the combined powers of simple commands, string orientation, circular file structure, a CRT with local memory, and conversational computing produce a system much more powerful than the sum of its parts."}
{"DOCID": "2948", "TEXT": "A Terminal-Oriented Communication System: This paper describes a system for full-duplex communication between a time-shared computer and its terminals.  The system consists of a communications computer directly connected to the time-shared system, a number of small remote computers to which the terminals are attached, and connecting medium speed telephone lines.  It can service a large number of terminals of various types.  The overall system design is presented along with the algorithms used to solve three specific problems: local echoing, error detection and correction on the telephone lines, and multiplexing of character output."}
{"DOCID": "2949", "TEXT": "A Correctness Proof of a Topology Information Main tenance Protocol for a Distributed Computer Network: In order for the nodes of a distributed computer network to communicate, each node must have information about the network's topology.  Since nodes and links sometimes crash, a scheme is needed to update this information.  One of the major constrain ts on such a topology information scheme is that it may not involve a central controller.  The Topology Information Protocol that was implemented on the MERIT Computer Network is presented and explained; this protocol is quite general and could be implemented on any computer network.  It is based on Baran's \"Hot Potato Heuristic Routing Doctrine.\"  A correctness proof of this Topology Information Protocol is also presented."}
{"DOCID": "2950", "TEXT": "A Unifying Approach to Scheduling: This paper presents a scheme for classifying scheduling algorithms based on an abstract model of a scheduling system which formalizes the notion of priority.  Various classes of scheduling algorithms are defined and related to existing algorithms.  A criterion for the implementation efficiency of an algorithm is developed and results in the definition of time-invariant algorithms, which include most of the commonly implemented ones.  For time-invariant algorithms, the dependence of processing rates on priorities is derived.  The abstract model provides a framework for implementing flexible schedulers in real operating systems.  The policy-driven scheduler of Bernstein and Sharp is discussed as an example of such an implementation"}
{"DOCID": "2951", "TEXT": "Dynamic Response Time Prediction for Computer Networks: If the ultimate aim of a computing network is resource sharing, then the human component as well as the technical component of networking must be fully investigated to achieve this goal.  This research is a first step toward assisting the user in participating in the vast store of resources available on a network. Analytical, simulation, and statistical performance evaluation tools are employed to investigate the feasibility of a dynamic response time monitor that is capable of providing comparative response time information for users wishing to process various computing applications at some network computing node.  The research clearly reveals that sufficient system data are currently obtainable, at least for the five diverse ARPA network systems studied in detail, to describe and predict the response time for network time-sharing systems as it depends on some measure of system activity or load level."}
{"DOCID": "2952", "TEXT": "Functions Realizable with Word-Parallel Logical and Two's-Complement Addition Instructions"}
{"DOCID": "2953", "TEXT": "Notes on Recursion Elimination: Various methods of recursion elimination are applied to the schematic recursive procedure: proc S(x); px then N(x); S(fx); S(gx); M(x) fi.  Procedures with this general form arise in connection with tree traversal and sorting algorithms.  Each method of recursion removal involves the use of one or more stacks, and the solutions are compared on the basis of their running time."}
{"DOCID": "2954", "TEXT": "A Bounded Storage Algorithm for Copying Cyclic Structures: A new algorithm is presented which copies cyclic list structures using bounded workspace and linear time. Unlike a previous similar algorithm, this one makes no assumptions about the storage allocation system in use and uses only operations likely to be available in a high-level language.  The distinctive feature of this algorithm is a technique for traversing the structure twice, using the same spanning tree in each case, first from left to right and then from right to left."}
{"DOCID": "2955", "TEXT": "Buddy Systems: Two algorithms are presented for implementing any of a class of buddy systems for dynamic storage allocation.  Each buddy system corresponds to a set of recurrence relations which relate the block sizes provided to each other. Analyses of the in ternal fragmentation of the binary buddy system, the Fibonacci buddy system, and the weighted buddy system are given. Comparative simulation results are also presented for in ternal, external, and total fragmentation."}
{"DOCID": "2956", "TEXT": "Some Ideas on Data Types in High-Level Languages: A number of issues are explored concerning the notion that a data type is a set of values together with a set of primitive operations on those values.  Among these are the need for a notation for iterating over the elements of any finite set (instead of the more narrow for i:= 1 to n notation), the use of the domain of an array as a data type, the need for a simple notation for allowing types of parameters to be themselves parameters (but in a restrictive fashion), and resulting problems with conversion of values from one type to another."}
{"DOCID": "2957", "TEXT": "Database Abstractions: Aggregation: Aggregation is in troduced as an abstraction which is important in conceptualizing the real world.  Aggregation transforms a relationship between objects into a higher-level object.  A new data type, called aggregation, is developed which, under certain criteria of \"well-definedness,\" specifies aggregation abstractions.  Relational databases defined as collections of aggregates are structured as a hierarchy on n-ary relations.  To main tain well-definedness, update operations on such databases must preserve two invariants.  Well-defined relations are distinct from relations in third normal form.  It is shown that these notions are complementary and both are important in database design.  A top-down methodology for database design is described which separates decisions concerning aggregate structure from decisions concerning key identification.  It is suggested that aggregate types, and other types which support real-world abstractions without in troducing implementation detail, should be incorporated into programming languages."}
{"DOCID": "2958", "TEXT": "Abstract Data Types and the Development of Data Structures: Abstract data types can play a significant role in the development of software that is reliable, efficient, and flexible.  This paper presents and discusses the application of an algebraic technique for the specification of abstract data types.  Among the examples presented is a top-down development of a symbol table for a block structured language; a discussion of the proof of its correctness is given. The paper also contains a brief discussion of the problems involved in constructing algebraic specifications that are both consistent and complete."}
{"DOCID": "2959", "TEXT": "The System for Business Automation (SBA): Programming Language: The system for business automation (SBA) is a system within which application experts-nonprogrammers-can describe and execute their applications on a computer. The user of SBA views his application as manipulation of information in two-dimensional pictures of tables, business forms, and reports on a display terminal. He can gradually automate this application by giving \"examples\" to the system of how he manually manipulates the information.  The Query-by-Example database language is a subset of the SBA programming language."}
{"DOCID": "2960", "TEXT": "Two Views of Data Abstraction"}
{"DOCID": "2961", "TEXT": "Experimental Investigations of the Utility of Detailed Flowcharts in Programming: This paper describes previous research on flowcharts and a series of controlled experiments to test the utility of detailed flowcharts as an aid to program composition, comprehension, debugging, and modification.  No statistically significant difference between flowchart and nonflowchart groups has been shown, thereby calling into question the utility of detailed flowcharting.  A program of further research is suggested."}
{"DOCID": "2962", "TEXT": "Production and Employment of Ph.D.'s in Computer Science-1976: Statistics are presented on the production and employment of Ph.D.'s in computer science for the calendar year 1975-76.  Data include profiles of graduate students and of faculty at 60 Ph.D.-producing departments as well as a breakdown of degrees granted by specialty areas.  Significant trends are noted and comparisons with comparable data gathered for the 1974-75 calendar year are made."}
{"DOCID": "2963", "TEXT": "A Fast Algorithm for Computing Longest Common Subsequences: Previously published algorithms for finding the longest common subsequence of two sequences of length n have had a best-case running time of O(n^2). An algorithm for this problem is presented which has a running time of O((r + n)log n), where r is the total number of ordered pairs of positions at which the two sequences match.  Thus in the worst case the algorithm has a running time of O(n^2 log n).  However, for those applications where most positions of one sequence match relatively few positions in the other sequence, a running time of O(n log n) can be expected."}
{"DOCID": "2964", "TEXT": "An Approach to Optimal Design of Storage Parameters in Databases"}
{"DOCID": "2965", "TEXT": "An Optimal Evaluation of Boolean Expressions in an Online Query System"}
{"DOCID": "2966", "TEXT": "The Choice of Reference Poin ts in Best-Match File Searching: Improvements to the exhaustive search method of best-match file searching have previously been achieved by doing a preprocessing step involving the calculation of distances from a reference poin t. This paper discusses the proper choice of reference poin ts and extends the previous algorithm to use more than one reference poin t.  It is shown that reference poin ts should be located outside of data clusters. The results of computer simulations are presented which show that large improvements can be achieved by the proper choice and location of multiple reference poin ts."}
{"DOCID": "2967", "TEXT": "A Comparison of Hardware and Software Associative Memories in the Context of Computer Graphics: The Associative Processing of Line Drawings (APLD) System utilizes a hardware associative memory and creates, modifies, deletes, stores, and retrieves two-dimensional line drawings consisting of poin ts, lines, rectangles, and triangles. The APLD functions were duplicated on the TX-2 computer at M.I.T.'s Lincoln Laboratory under the LEAP Language and Data Structure,  A comparison of the hardware approach with the software simulation illustrates the advantages of the hardware associative memory in three areas: (1) processing speed, (2) storage requirements, and (3) flexibility.  The major problem areas of hardware associative memory technology, namely input/output and cost effectiveness, are also addressed."}
{"DOCID": "2968", "TEXT": "A Comparison of Tree-Balancing Algorithms: Several algorithms-height-balance (i.e. AVL and extensions), weight-balance (i.e. BB and WB), and total restructuring-for building balanced binary search trees are compared.  The criteria for comparison encompass theoretical aspects (e.g. path lengths) and implementation independent and machine/algorithm-dependent measures (e.g. run time).  A detailed analysis of code is also presented at a level believed to be language-and compiler-independent.  The quality of the resulting trees and the overhead spent on building them are analyzed, and some guidelines are given for an efficient use of the methods.  If insertion and subsequent queries are the only operations of in terest, then \"pure\" AVL trees present the overall best qualities."}
{"DOCID": "2969", "TEXT": "Optimal Program and Data Locations in Computer Networks: An optimization procedure for the allocation of program and data files in a computer network is presented.  This algorithm takes into account the dependencies between files and programs such as occur in real heterogeneous computer networks.  Insights into whether or not to convert programs from one computer to another can also be gained from the model.  A search procedure for the file location problem is described, along with an example and a possible application of the model."}
{"DOCID": "2970", "TEXT": "Achieving Specific Accuracy in Simulation Output Analysis: This paper extends the use of the regenerative property of queueing systems in the analysis of simulation output.  In particular, it describes a sequential estimation method which when used with the regenerative property allows results to be obtained with specified statistical accuracy.  This method includes a test to check the normality assumption on which the sequential procedure relies.  The paper illustrates the method using the empty and idle state as the regenerative state.  A second example then describes how using the most frequently entered state as the regenerative state reduces the chance of making a costly error in a preliminary simulation run. The paper also described how a variance reduction method due to Page [9] can be used to obtain a specified accuracy with considerably fewer job completions than are required when no variance reduction technique is applied."}
{"DOCID": "2971", "TEXT": "SP/k: A System for Teaching Computer Programming: SP/k is a compatible subset of the PL/I  language that has been designed for teaching programming. The features of the SP/k language were chosen to encourage structured problem solving by computers, to make the language easy to learn and use, to eliminate confusing and redundant constructs, and to make the language easy to compile.  The resulting language is suitable for in troducing programming concepts used in various applications, including business data processing, scientific calculations and non-numeric computation.  SP/k is actually a sequence of language subsets called SP/1, SP/2,...SP/8.  Each subset in troduces new programming language constructs while retaining all the constructs of preceding subsets. Each subset is precisely defined and can be learned or implemented without the following subsets."}
{"DOCID": "2972", "TEXT": "Proof Techniques for Hierarchically Structured Programs: A method for describing and structuring programs that simplifies proofs of their correctness is presented.  The method formally represents a program in terms of levels of abstraction, each level of which can be described by a self-contained nonprocedural specification.  The proofs, like the programs, are structured by levels.  Although only manual proofs are described in the paper, the method is also applicable to semi-automatic and automatic proofs.  Preliminary results are encouraging, indicating that the method can be applied to large programs, such as operating systems."}
{"DOCID": "2973", "TEXT": "Sorting on a Mesh-Connected Parallel Computer: Two algorithms are presented for sorting n^2 elements on an n X n mesh-connected processor array that require O(n) routing and comparison steps. The best previous algorithm takes time O(n log n).  The algorithms of this paper are shown to be optimal in time within small constant factors.  Extensions to higher-dimensional arrays are also given."}
{"DOCID": "2974", "TEXT": "Comment on Weighted Increment Linear Search for Scatter Tables"}
{"DOCID": "2975", "TEXT": "Remark on Uniform Insertion in Structured Data Structures"}
{"DOCID": "2976", "TEXT": "Approximating Block Accesses in Database Organizations"}
{"DOCID": "2977", "TEXT": "The Stage Hypothesis and the S-Curve: Some Contradictory Evidence: This paper presents the results of a study testing the s-shaped budget curve of Nolan's stage model of computer development in an organization.  Research on the data processing budgets of California counties fails to support the s-shaped curve or the use of budgets as a basis for a stage model.  However, the results do not invalidate the concept of a stage model.  The analysis suggests an alternative model of budget growth and a separation between models of budgeting growth and growth stages in the development of the computer resource."}
{"DOCID": "2978", "TEXT": "Analysis of Design Alternatives for Virtual Memory Indexes: A class of index structures for use in a virtual memory environment is described.  Design alternatives within this class of index structures are analyzed.  These alternatives include a choice of search strategy, whether or not pages in the index are structured, and whether or not keys are compressed.  The average cost of retrieving entries from these indexes is expressed as a wieghted sum of the cost of a basic key comparison and the cost of crossing a page boundary in the index structure.  Formulas for the retrieval costs for possible combinations of design alternatives are given.  These are used in numerical case studies which compare the retrieval costs of the alternatives. Qualitative comparisons of the main tenance costs (insertion, deletion, reorganization) of the design alternatives are also included."}
{"DOCID": "2979", "TEXT": "Studies in Machine Cognition Using The Game of Poker: A progress report is presented of on-going research efforts concerning human decision making under uncertainly and risk and human problem solving and learning processes on the one hand, and machine learning, large scale programming systems, and novel programming techniques on the other.  There has also been in terest in how humans make deductive and inductive inferences and form and optimize heuristic rules, and how machines can reach similar results. Although the vehicle of these investigations has been the game of poker, a conceptual framework has been provided that should have a fairly wide range of applicability.  The models of human judgment, choice, and decision making are incorporated in a large scale complex program.  They represent both descriptive and normative theories of behavior. An in teractive game environment has been recently established which, besides its usefulness for experiments in game playing, enables humans to construct machine strategies \"on-line\" in a question answering, advice taking mode."}
{"DOCID": "2980", "TEXT": "The Editing  of Picture Segmentations Using Local Analysis of Graphs: A major problem in picture processing is the elimination of the large number of spurious regions that result from an initial segmentation by region growing techniques.  Such regions have been eliminated either on the basis of semantic information or on the basis of size and contrast.  A scheme is presented which performs eliminations on the basis of local properties of the region adjacency graph.  The scheme is based on definitions of graph properties which are satisfied when a spurious region is present; then editing is equivalent to fast graph operations.  A number of examples are shown."}
{"DOCID": "2981", "TEXT": "Subgoal Induction: A proof method, subgoal induction, is presented as an alternative or supplement to the commonly used inductive assertion method.  Its major virtue is that it can often be used to prove a loop's correctness directly from its input-output specification without the use of an invariant.  The relation between subgoal induction and other commonly used induction rules is explored and, in particular, it is shown that subgoal induction can be viewed as a specialized form of computation induction.  A set of sufficient conditions are presented which guarantee that an input-output specification is strong enough for the induction steps of a proof by subgoal induction to be valid."}
{"DOCID": "2982", "TEXT": "The Storage Requirement in Precedence Parsing"}
{"DOCID": "2983", "TEXT": "A Comparison of Next-fit, First-fit, and Best-fit"}
{"DOCID": "2984", "TEXT": "Cost/Utilization: A Measure of System Performance: A method is presented for evaluating computer system performance in terms of a cost/utilization factor and a measure of imbalance.  These coefficients indicate the extent to which the total system cost is effectively utilized.  The method includes a technique for the visual representation of system performance."}
{"DOCID": "2985", "TEXT": "Effects of Chargeout on User/Manager Attitudes: The relationship of in ternal pricing systems for computer services (chargeout systems) and user management attitudes about their computer-based information systems is investigated. Evidence is provided that the relationship conforms to a general pattern that would be expected from the hypothesis of the four stages of EDP growth [15].  The results also indicate that the chargeout systems characteristic of advanced EDP stage environments are associated with relatively high levels of positive user attitudes and marked increases in EDP training for users. Both factors are important to the user/manager involvement necessary for effective control of computer-based systems. Development and main tenance of computer-based systems is asserted to be a category of organizational change.  A \"felt need\" for the change on the part of the user/manager is prerequisite to any change taking place.  The research methods of behavioral science are applied to investigate the user/manager environment and the effects of chargeout."}
{"DOCID": "2986", "TEXT": "Operations on Sparse Relations: Various computations on relations, Boolean matrices, or directed graphs, such as the computation of precedence relations for a context-free grammar, can be done by a practical algorithm that is asymptotically faster than those in common use.  For example, how to compute operator precedence or Wirth-Weber precedence relations in O(n^2) steps is shown, as well as how to compute linear precedence functions in O(n^2) steps is shown, as well as how to compute linear precedence functions in O(n) steps, where n is the size of a grammer.  The heart of the algorithms is a general theorem giving sufficient conditions under which an expression whose operands are sparse relations and whose operators are composition, transitive closure, union, and inverse, can be computed efficiently."}
{"DOCID": "2987", "TEXT": "Representation of Many-Sided Polygons and Polygonal Lines for Rapid Processing: A representation for polygons and polygonal lines is described which allows sets of consecutive sides to be collectively examined.  The set of sides are arranged in a binary tree hierarchy by inclusion. A fast algorithm for testing the inclusion of a poin t in a many-sided polygon is given.  The speed of the algorithm is discussed for both ideal and practical examples.  It is shown that the poin ts of intersection of two polygonal lines can be located by what is essentially a binary tree search.  The algorithm and a practical example are discussed.  The representation overcomes many of the disadvantages associated with the various fixed-grid methods for representing curves and regions"}
{"DOCID": "2988", "TEXT": "Memory Management and Response Time: This paper presents a computationally tractable methodology for including accurately the effects of finite memory size and workload memory requirements in queueing network models of computer systems. Empirical analyses and analytic studies based on applying this methodology to an actual multiaccess in teractive system are reported.  Relations between workload variables such as memory requirement distribution and job swap time, and performance measures such as response time and memory utilization are graphically displayed. A multiphase, analytically soluble model is proposed as being broadly applicable to the analysis of in teractive computer systems which use nonpaged memories."}
{"DOCID": "2989", "TEXT": "Empirical Evaluation of Some Features of Instruction Set Processor Architectures: This paper presents methods for empirical evaluation of features of Instruction Set Processors (ISPs).  ISP features are evaluated in terms of the time used or saved by having or not having the feature. The methods are based on analysis of traces of program executions.  The concept of a register life is in troduced, and used to answer questions like: How many registers are used simultaneously? How many would be sufficient all of the time? Most of the time? What would the overhead be if the number of registers were reduced? What are registers used for during their lives? The paper also discusses the problem of detecting desirable but non-existing instructions. Other problems are briefly discussed.  Experimental results are presented, obtained by analyzing 41 programs running on the DEC system 10 ISP."}
{"DOCID": "2990", "TEXT": "Effective Information Retrieval Using Term Accuracy: The performance of information retrieval systems can be evaluated in a number of different ways.  Much of the published evaluation work is based on measuring the retrieval performance of an average user query.  Unfortunately, formal proofs are difficult to construct for the average case.  In the present study, retrieval evaluation is based on optimizing the performance of a specific user query.  The concept of query term accuracy is in troduced as the probability of occurrence of a query term in the documents relevant to that query.  By relating term accuracy to the frequency of occurrence of the term in the documents of a collection it is possible to give formal proofs of the effectiveness with respect to a given user query of a number of automatic indexing systems that have been used successfully in experimental situations.  Among these are inverse document frequency weighting, thesaurus construction, and phrase generation."}
{"DOCID": "2991", "TEXT": "Improving the Access Time for Random Access Files: Clustering in the key set is decreased by smoothing the key-to-address transformation, and by adding shadow buckets to an open chaining file.  The keys are pre-hashed before the address division, to remove the effect of sequential properties in the key set.  Shadow buckets in the key search sequence reduce the effect of nonuniformity in file loading, and decrease the number of maximum probes needed to locate a record.  The combined effects of these techniques lead to improved file performance for secondary storage devices, as shown by empirical studies."}
{"DOCID": "2992", "TEXT": "A Numbering System for Binary Trees"}
{"DOCID": "2993", "TEXT": "Occurrences of Cycling and Other Phenomena Arising in a Class of Linear Programming Models: An investigation into the average queue size for a certain class of queues has resulted in the formulation of linear programming problems which are ill-conditioned in some cases.  In attempting to solve these linear programming models, using IBM's MPS package, instances of cycling were encountered. Small perturbations in the input data resulted in problems which did not cycle.  This fact, plus several other observed phenomena suggest that the primary reason that cycling is not known to occur more frequently is the round-off errors in the computations perturb the problem sufficiently to prevent cycling (or at least to prevent indefinite cycling).  In one case maximizing and minimizing an objective function subject to the same constrain t set was attempted, but MPS solved only one of these while giving an indication of infeasibility for the other."}
{"DOCID": "2994", "TEXT": "A Linear Algorithm for Incremental Digital Display of Circular Arcs: Circular arcs can be drawn on an incremental display device such as a cathode ray tube, digital plotter, or matrix prin ter using only sign testing and elementary addition and subtraction.  This paper describes methodology for producing dot or step patterns closet to the true circle."}
{"DOCID": "2995", "TEXT": "Decomposability, Instabilities, and Saturation in Multiprogramming Systems (Corrigendum)"}
{"DOCID": "2996", "TEXT": "Transient-Free Working-Set Statistics: Transient-free average working set size and transient-free missing-page rate for a finite sample of a reference string are defined.  Use of these statistics is appropriate if the contents of the working set at the start of the recorded string are unknown. If a certain stationarity condition holds, these statistics provide unbiased estimates of expected working-set sizes, missing-page probabilities, and in terreference distance probabilities.  Two other pairs of estimators are shown to be biased.  Expressions for the transient-free statistics are obtained in terms of in terval statistics. Several methods of computation are discussed, the usefulness of each depending on length of the sample, number of distinct references, and the amount of main storage available to the computer performing the calculations.  In particular, methods are described for handling long strings containing many distinct page names."}
{"DOCID": "2997", "TEXT": "Convex Hulls of Finite Sets of Poin ts in Two and Three Dimensions: The convex hulls of sets of n poin ts in two and three dimensions can be determined with O(n log n) operations.  The presented algorithms use the \"divide and conquer\" technique and recursively apply a merge procedure for two nonin tersecting convex hulls. Since any convex hull algorithm requires at least O(n log n) operations, the time complexity of the proposed algorithms is optimal within a multiplicative constant."}
{"DOCID": "2998", "TEXT": "An Empirical Study of List Structure in Lisp: Static measurements of the list structure of five large Lisp programs are reported and analyzed in this paper.  These measurements reveal substantial regularity, or predictability, among poin ters to atoms and especially among poin ters to lists.  Pointers to atoms are found to obey, roughly, Zipf's law, which governs word frequencies in natural languages; poin ters to lists usually poin t to a location physically nearby in memory.  The use of such regularities in the space-efficient representation of list structure is discussed.  Linearization of lists, whereby successive cdrs (or cars) are placed in consecutive memory locations whenever possible, greatly strengthens the observed regularity of list structure.  It is shown that under some reasonable assumptions, the entropy or information content of a car-cdr pair in the programs measured is about 10 to 15 bits before linearization, and about 7 to 12 bits after."}
{"DOCID": "2999", "TEXT": "An Approach to Multidimensional Data Array Processing by Computer: Some recent work on the development of general-purpose computer-based statistical and data processing capabilities for handling multidimensional arrays of data is presented. Attention is first given to some of the general problems of multidimensional table and array processing.  This is followed by a summary of some recent developments in array processing capabilities at the World Bank, in particular, the system identified as WRAPS(World Bank Retrieval and Array Processing System)."}
{"DOCID": "3000", "TEXT": "Segment Sizes and Lifetimes in Algol 60 Programs: The characteristics of the virtual memory requirements of a sample of Algol 60 programs have been measured.  Distributions are presented for thesizes of memory requests and for their holding times (lifetimes).  The results are presented in terms of Johnston's contour model and a simple abstract machine. They provide new empirical evidence of certain aspects of the construction and behavior of real programs, and some of their implications for the design of virtual memory systems are presented and discussed."}
{"DOCID": "3001", "TEXT": "Detection of Combined Occurrences: In this paper it is supposed that the variables X1,...,Xn each have finite range with the variable Xi taking on Pi possible values and that the values of the variables are changing with time.  It is supposed further that it is desired to detect occurrences in which some subset of the variables achieve particular values.  Finally, it is supposed that the problem involves the detection of a large number of combined occurrences for a large number of changes of values of variables.  Two efficient solutions for this problem are described.  Both methods have the unusual property of being faster for systems where the sum P1 + ... + Pn is larger. The first solution is error-free and suitable for most cases.  The second solution is slightly more elegant and allows negation as well as conjunction, but is subject to the possibility of errors.  An error analysis is given for the second method and an empirical study is reported."}
{"DOCID": "3002", "TEXT": "A Record and File Partitioning Model: One of the main objectives in the design of a file system is the reduction of storage and data transfer costs.  This paper presents a model in which several  requests access the file system, and each request requires information from one or more variable length data-items.  The probabilities of access and the distribution of each data-item's length are assumed to be known, and to be mutually independent. The file system uses one or more storage devices, and each record may be partitioned into subrecords that are stored on different devices.  One of the subrecords is designated as the primary record; when a request for a record is made, the primary record is first accessed, and other subrecords are accessed only if the pertinent information is not stored in the primary record.  The model that is presented in this paper, both as a nonlinear programming model and a mixed integer programming model, is a very general one; several types of file systems may be derived from it by an appropriate selection of its parameters. This model has already been used in the optimization of library routines' storage at a large scale operating system."}
{"DOCID": "3003", "TEXT": "A Survey of the Literature in Computer Science Education Since Curriculum '68: A bibliography of approximately two hundred references in computer science education appearing in the literature since the publication of \"Curriculum '68\" is presented.  The bibliography itself is preceded by brief descriptive materials organizing the references into the categories of survey reports, activities of professional organizations, philosophy of programs, description of  programs, description of courses and other materials."}
{"DOCID": "3004", "TEXT": "Structured Programming in Cobol: An Approach for Application Programmers: Techniques for designing and writing Cobol programs are presented.  Previous work in structured programming is drawn upon and adapted.  The presentation is informal: the terminology is nonmathematical as far as possible, no theorems are proved, and examples are used frequently.  Top-down program design is implemented through the use of structured flowcharts, disciplined specifications, and step by step verification.  A well-formed Cobol program is defined. The proper use of the GO TO and other Cobol coding practices are discussed."}
{"DOCID": "3005", "TEXT": "Implications of Structured Programming for Machine Architecture: Based on an empirical study of more than 10,000 lines of program text written in a GOTO-less language, a machine architecture specifically designed for structured programs is proposed.  Since assignment, CALL, RETURN, and IF statements together account for 93 percent of all executable statements, special care is given to ensure that these statements can be implemented efficiently.  A highly compact instruction encoding scheme is presented, which can reduce program size by a factor of 3.  Unlike a Huffman code, which utilizes variable length fields, this method uses only fixed length (1-byte) op code and address fields.  The most frequent instructions consist of a single 1-byte field.  As a consequence, instruction decoding time is minimized, and the machine is efficient with respect to both space and time."}
{"DOCID": "3006", "TEXT": "Anomalies with Variable Partition Paging Algorithms: Five types of anomalous behavior which may occur in paged virtual memory operating systems a redefined.  One type of anomaly, for example, concerns the fact that, with certain reference strings and paging algorithms, an increase in mean memory allocation may result in an increase in fault rate. Two paging algorithms, are examined in terms of their anomaly potential, and reference string examples of various anomalies are presented.  Two paging algorithm properties, the inclusion property and the generalized inclusion property, are discussed and the anomaly implications of these properties presented."}
{"DOCID": "3007", "TEXT": "Complexity of Computations (Corrigendum)"}
{"DOCID": "3008", "TEXT": "Preserving Average Proximity in Arrays: Programmers and data structure designers are often forced to choose between alternative structures. In storing these structures, preserving logical adjacencies or \"proximity\" is usually an important consideration. The combinatorial problem of storing arrays as various kinds of list structures is examined.  Embeddings of graphs are used to model the loss of proximity involved in such storage schemes, and an elementary proof that arrays cannot be stored as linear lists with bounded loss of proximity is presented.  Average loss of proximity is then considered, and it is shown that arrays cannot be stored as linear lists with only bounded loss of average proximity, but can be so stored in binary trees.  The former result implies, for instance, that row major order is an asymptotically optimal storage strategy for arrays."}
{"DOCID": "3009", "TEXT": "Insertions and Deletions In One-Sided Height-Balanced Trees: Recently Hirschberg has established that insertions into one-sided height-balanced trees can be done in 0(log^2N) steps.  It is proved here that deletions can also be performed in 0(log^2N) steps, which answers the open problem posed by Hirschberg."}
{"DOCID": "3010", "TEXT": "Value Orientation of Computer Science Students: Technological and nontechnological value orientations are investigated with special attention to the complexity of value structures.  Computer science students, who are closely associated with technology, contrast with social science students, who are often technologically aloof.  This is confirmed by the value ratings of 313 students at the University of Minnesota in 1972.  Computer science majors were found to have a more complex value structure than social science majors."}
{"DOCID": "3011", "TEXT": "Management Utilization of Computers in American Local Governments: Traditional concepts of management information systems (MIS) bear little relation to the information systems currently in use by top management in most US local governments.  What exists is management-oriented computing, involving the use of relatively unsophisticated applications.  Despite the unsophisticated nature of these systems, management use of computing is surprisingly common, but also varied in its extent among local governments.  Management computing is most prevalent in those governments with professional management practices where top management is supportive of computing and tends to control computing decisions and where department users have less control over design and implementation activities.  Finally, management computing clearly has impacts for top managers, mostly involving improvements in decision information."}
{"DOCID": "3012", "TEXT": "The Use of an Interactive Information Storage and Retrieval System in Medical Research: This paper presents the results of a study of the use of an interactive computerized storage and retrieval system.  A monitor built into the computer system provided usage data for the study.  Additional data on user reactions were gathe red from a questionnaire. The results show the important role played by frequently chosen laboratory reference leaders in influencing the use of this system.  The implications of the study for the design of similar systems are discussed."}
{"DOCID": "3013", "TEXT": "Some New Methods of Detecting Step Edges in Digital Pictures: This note describes two operators that respond to step edges, but not to ramps.  The first is similar to the digital Laplacian, but uses the max, rather than the sum, of the x and y second differences. The second uses the difference between the mean and median gray levels in a neighborhood.  The outputs obtained from these operators applied to a set of test pictures are compared with each other and with the standard digital Laplacian and gradient.  A third operator, which uses the distance between the center and centroid of a neighborhood as an edge value, is also briefly considered; it turns out to be equivalent to one of the standard digital approximations to the gradient."}
{"DOCID": "3014", "TEXT": "Is \"Sometime\" Sometimes Better than \"Always\"? (Intermittent Assertions in Proving Program Correctness): This paper explores a technique for proving the correctness and termination of programs simultaneously. This approach, the intermittent-assertion method, involves documenting the program with assertions that must be true at some time when control passes through the corresponding point, but that need not be true every time.  The method, introduced by Burstall, promises to provide a valuable complement to the more conventional methods.  The intermittent-assertion method is presented with a number of examples of correctness and termination proofs.  Some of these proofs are markedly simpler than their conventional counterparts. On the other hand, it is shown that a proof of correctness or termination by any of the conventional techniques can be rephrased directly as a proof using intermittent assertions.  Finally, it is shown how the intermittent-assertion method can be applied to prove the validity of program transformations and the correctness of continuously operating programs."}
{"DOCID": "3015", "TEXT": "Relaxation Methods for Image Reconstruction: The problem of recovering an image (a function of two variables) from experimentally available integrals of its grayness over thin strips is of great importance in a large number of scientific areas. An important version of the problem in medicine is that of obtaining the exact density distribution within the human body from X-ray projections.One approach that has been taken to solve this problem consists of translating the available information into a system of linear inequalities.  The size and the sparsity of the resulting system (typically, 25,000 inequalities with fewer than 1 percent of the coefficients nonzero) makes methods using successive relaxations computationally attractive, as compared to other ways of solving systems of inequalities. In this paper, it is shown that, for a consistent system of linear inequalities, any sequence of relaxarion parameters lying strictly between 0 and 2 generates a sequence of vectors which converges to a solution. Under the same assumptions, for a system of linear equations, the relaxation method converges to the minimum norm solution.  Previously proposed techniques are shown to be special cases of our procedure with different choices of relaxation parameters.  The practical consequences for image reconstruction of the choice of the relaxation parameters are discussed."}
{"DOCID": "3016", "TEXT": "A Comparison of Numerical Techniques in Markov Modeling: This paper presents several numerical methods which may be used to obtain the stationary probability vectors of Markovian models.  An example of a nearly decomposable system is considered, and the results obtained by the different methods examined.  A post mortem reveals why standard techniques often fail to yield the correct results.  Finally, a means of estimating the error inherent in the decomposition of certain models is presented."}
{"DOCID": "3017", "TEXT": "B-trees Re-examined: The B-tree and its variants have, with increasing frequency, been proposed as a basic storage structure for multiuser database applications.  Here, three potential problems which must be dealt with in such a structure that do not arise in more traditional static directory structures are indicated. One problem is a possible performance penalty."}
{"DOCID": "3018", "TEXT": "Covering Edges by Cliques with Regard to Keyword Conflicts and Intersection Graphs: Kellerman has presented a method for determining keyword conflicts and described a heuristic algorithm which solves a certain combinatorial optimization problem in connection with this method. This optimization problem is here shown to be equivalent to the problem of covering the edges of a graph by complete subgraphs with the objective of minimizing the number of complete subgraphs.  A relationship between this edge-clique-cover problem and the graph coloring problem is established which allows algorithms for either one of these problems to be constructed from algorithm for the other.  As consequences of this relationship, the keyword conflict problem and the edge-clique-cover problem are shown to be NP-complete, and if P=/NP then they do not admit polynomial-time approximation algorithms which always produce solutions within a factor less than 2 from the optimum."}
{"DOCID": "3019", "TEXT": "The GRE Advanced Test in Computer Science"}
{"DOCID": "3020", "TEXT": "Systematic Recursion Removal: The recursion removal algorithm presented by Strong and Walker is amplified and applied to a relatively complex PL/I program.  The aim is to demonstrate systematic recursion-removal techniques on something more complex than Knuth's \"sturdy toddler\" and to obtain measurements of the cost of procedure linkage in PL/I and the savings achievable via procedure integration in the presence of recursion.  First, the paper describes the recursion-removal process and the example on which it will be illustrated.  Recursion removal is then applied to the two major parts of this example and the final result of the process is displayed.  Our performance comparison results are presented and our conclusions are briefly discussed."}
{"DOCID": "3021", "TEXT": "A Method for Obtaining Digital Signatures and Public-Key Cryptosystems: An encryption method is presented with the novel property that publicly revealing an encryption key does not thereby reveal the corresponding decryption key.  This has two important consequences: (1) Couriers or other secure means are not needed to transmit keys, since a message can be enciphered using an encryption key publicly revealed by the intended recipient. Only he can decipher the message, since only he knows the corresponding decryption key.  (2) A message can be \"signed\" using a privately held decryption key.  Anyone can verify this signature using the corresponding publicly revealed encryption key.  Signatures cannot be forged, and a signer cannot later deny the validity of his signature.  This has obvious applications in \"electronic mail\" and \"electronic funds transfer\" systems.  A message is encrypted by representing it as a number M, raising M to a publicly specified power e, and then taking the remainder when the result is divided by the publicly specified product, n, of two large secret prime numbers p and q.  Decryption is similar;only a different, secret, power d is used, where e * d = 1 (mod(p-1) * (q-1)).  the security of the system rests in part on the difficulty of factoring the published divisor, n."}
{"DOCID": "3022", "TEXT": "Computer Science Faculties: The Current Status of Minorities and Women: The results of a survey conducted in the fall of 1975 to determine the status of women and minority faculty members in academic computer science are presented.  Faculty members were compared with respect to professional background, salaries, teaching load, publication records, and research grants. Analysis of the data indicated that the over-all verdict is one of general equality among women, minorities, and men."}
{"DOCID": "3023", "TEXT": "Architecture of the IBM System/370: This paper discusses the design considerations for the architectural extensions that distinguish System/370 from System/360.  It comments on some experiences with the original objectives for System/360 and on the efforts to achieve them, and it describes the reasons and objectives for extending the architecture. It covers virtual storage, program control, data-manipulation instructions, timing facilities, multiprocessing, debugging and monitoring, error handling, and input/output operations.  A final section tabulates some of the important parameters of the various IBM machines which implement the architecture."}
{"DOCID": "3024", "TEXT": "The CRAY-1 Computer System: This paper describes the CRAY-1, discusses the evolution of its architecture, and gives an account of some of the problems that were overcome during its manufacture.  The CRAY-1 is the only computer to have been built to date that satisfies ERDA's Class VI requirement (a computer capable of processing from 20 to 60 million floating point operations per second) [1].  The CRAY-1's Fortran compiler (CFT) is designed to give the scientific user immediate access to the benefits of the CRAY-1's vector processing architecture.  An optimizing compiler, CFT, \"vectorizes\" innermost DO loops.  Compatible with the ANSI 1966 Fortran Standard and with many commonly supported Fortran extensions, CFT does not require any source program modifications or the use of additional nonstandard Fortran statements to achieve vectorization. Thus the user's investment of hundreds of man months of effort to develop Fortran programs for other contemporary computers is protected."}
{"DOCID": "3025", "TEXT": "The Evolution of the DEC system 10: The DEC system 10, also known as the PDP-10, evolved from the PDP-6 (circa 1963) over five generations of implementations to presently include systems covering a price range of five to one.  The origin and evolution of the hardware, operating system, and languages are described in terms of technological change, user requirements, and user developments. The PDP-10's contributions to computing technology include: accelerating the transition from batch oriented to time sharing computing systems; transferring hardware technology within DEC (and elsewhere) to minicomputer design and manufacturing; supporting minicomputer hardware and software development; and serving as a model for single user and timeshared interactive minicomputer/microcomputer systems."}
{"DOCID": "3026", "TEXT": "The Evolution of the Sperry Univac 1100 Series: A His tory, Analysis, and Projection: The 1100 series systems are Sperry Univac's large-scale main frame computer systems.  Beginning with the 1107 in 1962, the 1100 series has progressed through a succession of eight compatible computer models to the latest system, the 1100/80, introduced in 1977.  The 1100 series hardware architecture is based on a 36-bit word, ones complement structure which obtains one operand from storage and one from a high-speed register, or two operands from high-speed registers.  The 1100 Operating System is designed to support a symmetrical multiprocessor configuration simultaneously providing multiprogrammed batch, timesharing, and transaction environments."}
{"DOCID": "3027", "TEXT": "The Development of the MU5 Computer System: Following a brief outline of the background of the MU5 project, the aims and ideas for MU5 are discussed.  A description is then given of the instruction set, which includes a number of features conducive to the production of efficient compiled code from high-level language source programs.  The design of the processor is then traced from the initial ideas for an associatively addressed \"name store\" to the final multistage pipeline structure involving a prediction mechanism for instruction prefetching and a function queue for array element accessing.  An overall view of the complete MU5 complex is presented together with a brief indication of its performance."}
{"DOCID": "3028", "TEXT": "The Manchester Mark I and Atlas: A His torical Perspective: In 30 years of computer design at Manchester University two systems stand out: the Mark I (developed over the period 1946-49) and the Atlas (1955-62). This paper places each computer in its his torical context and then describes the architecture and system software in present-day terminology.  Several design concepts such as address-generation and store management have evolved in the progression from Mark I to Atlas.  The wider impact of Manchester innovations in these and other areas is discussed, and the contemporary performance of the Mark I and Atlas is evaluated."}
{"DOCID": "3029", "TEXT": "Foreword to the Special Issue on Computer Architecture"}
{"DOCID": "3030", "TEXT": "An Example of Hierarchical Design and Proof: Hierarchical programming is being increasingly recognized as helpful in the construction of large programs.  Users of hierarchical techniques claim or predict substantial increases in productivity and in the reliability of the programs produced.  In this paper we describe a formal method for hierarchical program specification, implementation, and proof.  We apply this method to a significant list processing problem and also discuss a number of extensions to current programming languages that ease hierarchical program design and proof."}
{"DOCID": "3031", "TEXT": "Abstract Data Types and Software Validation: A data abstraction can be naturally specified using algebraic axioms.  The virtue of these axioms is that they permit a representation-independent formal specification of a data type.  An example is given which shows how to employ algebraic axioms at successive levels of implementation.  The  major thrust of the paper is twofold.  First, it is shown how the use of algebraic axiomatizations can simplify the process of proving the correctness of an implementation of an abstract data type.  Second, semi-automatic tools are described which can be used both to automate such proofs of correctness and to derive an immediate implementation from the axioms.  This implementation allows for limited testing of programs at design time, before a conventional implementation is accomplished."}
{"DOCID": "3032", "TEXT": "Reverse Path Forwarding of Broadcast Packets: A broadcast packet is for delivery to all nodes of a network.  Algorithms for accomplishing this delivery through a store-and-forward packet switching computer network include (1) transmission of separately addressed packets. (2) multidestination addressing, (3) hot potato forwarding,(4) spanning tree forwarding, and (5) source based forwarding.  To this list of algorithms we add (6) reverse path forwarding, a broadcast routing method which exploits routing procedures and data structures already available for packet switching.  Reverse path forwarding is a practical algorithm for broadcast routing in store-and-forward packet switching computer networks. The algorithm is described as being practical because it is not optimal according to metrics developed for its analysis in this paper, and also because it can be implemented in existing networks with less complexity than that required for the known alternatives."}
{"DOCID": "3033", "TEXT": "Optimizing Decision Trees Through Heuristically Guided Search: Optimal decision table conversion has been tackled in the literature using two approaches, dynamic programming and branch-and-bound.  The former technique is quite effective, but its time and space requirements are independent of how \"easy\" the given table is.  Furthermore, it cannot be used to produce good, quasi optimal solutions.  The branch-and-bound technique uses a good heuristic to direct the search, but is cluttered up by an enormous search space, since the number of solutions increases with the number of test variables according to a double exponential.  In this paper we suggest a heuristically guided top-down search algorithm which, like dynamic programming, recognizes identical subproblems but which can be used to find both optimal and quasi optimal solutions.  The heuristic search method introduced in this paper combines the positive aspects of the above two techniques.  Compressed tables with a large number of variables can be handled without deriving expanded tables first."}
{"DOCID": "3034", "TEXT": "Detection of Logical Errors in Decision Table Programs: In this paper an algorithm to detect logical errors in a limited-entry decision table and in loop-free programs with embedded decision tables is developed. All the conditions in the decision tables are assumed to be inequalities or equalities relating linear expressions.  It is also assumed that actions in a decision table are linear in variables which occur in the condition stub of the decision table (or tables) to which control is transferred from the table. The algorithm is based on determining whether a set of linear inequalities has or does not have a solution.  The algorithm described in the paper is implemented in Fortran IV."}
{"DOCID": "3035", "TEXT": "A Strategic Planning Methodology for the Computing Effort in Higher Education: An Empirical Evaluation: The findings of a study designed to address the pressing problems associated with the strategic planning of the computing effort in higher education are presented here.  A planning methodology was developed and tested through implementation at a university. Two years after the methodology was implemented, the effectiveness of the planning methodology was assessed in terms of the improvement of the delivery of computing services to the major institutional roles of instruction, research, and administration. Two control institutions were employed to contrast the improvements at the test institution.  The results of the research indicate the planning methodology significantly enhanced the delivery of computing services."}
{"DOCID": "3036", "TEXT": "The Selection of Optimal Tab Settings: A new generation of computer terminals allows tab settings to be selected and set by the computer. This feature can be used to reduce the number of characters that are needed to represent a document for transmission and printing.  In this note, an algorithm is given for selecting the optimal set of tab stops for minimizing the number of characters transmitted. An implementation of the algorithm has reduced the number of characters transmitted by from 7 to 30 percent, but requires a prepass through the document to compute a matrix used in determining the optimal set tab stops.  The use of fixed tab stops, as a heuristic alternative, can achieve about 80 percent of optimal with no prepass."}
{"DOCID": "3037", "TEXT": "A Linear Sieve Algorithm for Finding Prime Numbers: A new algorithm is presented for finding all primes between 2 and n.  The algorithm executes in time proportional to n (assuming that multiplication of integers not larger than n can be performed in unit time).  The method has the same arithmetic complexity as the algorithm presented by Mairson [6]; however, our version is perhaps simpler and more elegant. It is also easily extended to find the prime factorization of all integers between 2 and n in time proportional to n."}
{"DOCID": "3038", "TEXT": "Using Encryption for Authentication in Large Networks of Computers: Use of encryption to achieve authenticated communication in computer networks is discussed. Example protocols are presented for the establishment of authenticated connections, for the management of authenticated mail, and for signature verification and document integrity guarantee.  Both conventional and public-key encryption algorithms are considered as the basis for protocols."}
{"DOCID": "3039", "TEXT": "On-the-Fly Garbage Collection: An Exercise in Cooperation: As an example of cooperation between sequential processes with very little mutual interference despite frequent manipulations of a large shared data space,  a technique is developed which allows nearly all of the activity needed for garbage detection and collection to be performed by an additional processor operating con-currently with the processor devoted to the computation proper.  Exclusion and synchronization constraints have been kept as weak as could be achieved; the severe complexities engendered by doing so are illustrated."}
{"DOCID": "3040", "TEXT": "Synthesizing Constraint Expressions: A constraint network representation is presented for a combinatorial search problem: finding values for a set of variables subject to a set of constraints. A theory of consistency levels in such networks is formulated, which is related to problems of backtrack tree search efficiency.  An algorithm is developed that can achieve any level of consistency desired, in order to preprocess the problem for subsequent backtrack search, or to function as an alternative to backtrack search by explicitly determining all solutions."}
{"DOCID": "3041", "TEXT": "Median Split Trees: A Fast Lookup Technique for Frequently Occuring Keys: Split trees are a new technique for searching sets of keys with highly skewed frequency distributions. A split tree is a binary search tree each node of which contains two key values-a node value which is a maximally frequent key in that subtree, and a split value which partitions the remaining keys (with respect to their lexical ordering) between the left and right subtrees.  A median split tree (MST) uses the lexical median of a node's descendents as its split value to force the search tree to be perfectly balanced, achieving both a space efficient representation of the tree and high search speed.  Unlike frequency ordered binary search trees, the cost of a successful search of an MST is log n bounded and very stable around minimal values.  Further, an MST can be built for a given key ordering and set of frequencies in time n log n, as opposed to n2 for an optimum binary search tree.  A discussion of the application of MST's to dictionary lookup for English is presented, and the performance obtained is contrasted with that of other techniques."}
{"DOCID": "3042", "TEXT": "Power Trees: The new class of Pk trees is presented, where height balance is maintained for the nodes Iying on particular paths.  The number of nodes of a Pk tree asymptotically grows as a power of the height, in the worst case.  A procedure for node insertion is given, and the class of trees  considered is restricted to IPk trees, which are buildable by such a procedure. The average behavior of such trees, studied by an extensive set of simulation runs, is close to that of AVL trees.  In particular, the family of IPO trees whose main advantage is the reduced number of restructurings required after node insertion, is analyzed."}
{"DOCID": "3043", "TEXT": "Distributed Processes: A Concurrent Programming Concept: A language concept for concurrent processes without common variables is introduced.  These processes communicate and synchronize by means of procedure calls and guarded regions.  This concept is proposed for real-time applications controlled by microcomputer networks with distributed storage. The paper gives several examples of distributed processes and shows that they include procedures, coroutines, classes, monitors, processes, semaphores, buffers, path expressions, and input/output as special cases."}
{"DOCID": "3044", "TEXT": "A Note on Conditional Expressions: Evaluation of a conditional expression may succeed even when the \"deciding predicate\" diverges and the alternatives are records (or nodes) whose fields have different content."}
{"DOCID": "3045", "TEXT": "A Simple Recovery-Only Procedure For SImple Precedence Parsers: A simple method is described enabling simple precedence parsers to recover from syntax errors. No attempt to repair errors is made, yet parsing and most semantic processing can continue.  The result is a good \"first approximation\" to syntax error handling with negligible increase in parsing time, space, and complexity of both the parser and its table generator."}
{"DOCID": "3046", "TEXT": "Computer Generation of Gamma Random Variables - II: A rejection method is proposed for generating gamma variates with nonintegral shape parameter a, a > 1. This method is similar to other methods given by Fishman, Wallace, and Tadikamalla and is faster than these methods for a> 2.  The core storage requirements and the programming effort for the proposed method are similar to those of Wallace's or Tadikamalla's methods.  The computational times for the proposed method remain fairly constant for medium and large values of a and are superior to times obtained by Ahrens and Dieter's method for all values of a.  The proposed method is simpler than Ahrens and Dieter's method."}
{"DOCID": "3047", "TEXT": "Using Synthetic Images to Register Real Images with Surface Models: A number of image analysis tasks can benefit from registration of the image with a model of the surface being imaged.  Automatic navigation using visible light or radar images requires exact alignment of such images with digital terrain models.  In addition, automatic classification of terrain, using satellite imagery, requires such alignment to deal correctly with the effects of varying sun angle and surface slope.  Even inspection techniques for certain industrial parts may be improved by this means. We achieve the required alignment by matching the real image with a synthetic image obtained from a surface model and known positions of the light sources.  The synthetic image intensity is calculated using the reflectance map, a convenient way of describing surface reflection as a function of surface gradient. We illustrate the technique using LANDSAT images and digital terrain models."}
{"DOCID": "3048", "TEXT": "Performance Evaluation of Highly Concurrent Computers by Deterministic Simulation: Simulation is presented as a practical technique for performance evaluation of alternative configurations of highly concurrent computers.  A technique is described for constructing a detailed deterministic simulation model of a system.  In the model a control stream replaces the instruction and data streams of the real system.  Simulation of the system model yields the timing and resource usage statistics needed for performance evaluation, without the necessity of emulating the system.  As a case study, the implementation of a simulator of a model of the CPU-memory subsystem of the IBM 360/91 is described.  The results of evaluating some alternative system designs are discussed.  The experiments reveal that, for the case study, the major bottlenecks in the system are the memory unit and the fixed point unit.  Further, it appears that many of the sophisticated pipelining and buffering technique simplemented in the architecture of the IBM 360/91 are of little value when high-speed (cache) memory is used, as in the IBM 360/195."}
{"DOCID": "3049", "TEXT": "A Simply Extended and Modified Batch Environment Graphical System (SEMBEGS): SEMBEGS is a complete batch environment graphical system containing components for handling graphical data files, for displaying the contents of these files on a variety of graphical hardware, and for performing graphical batch input operations. SEMBEGS is easy to extend and modify to meet the growing needs of a large batch environment, and is even extendable to a fully interactive system.  The paper presents the conceptual view of graphics leading to the design of SEMBEGS and outlines the major components of the system.  The design of SEMBEGS is founded upon the basic assumption that the true aim of computer graphics is to describe graphical entities, rather than, as commonly held, to provide graphical input and output functional capabilities.  SEMBEGS is built around a Basic Graphical Data Management System (BAGDAMS) which provides a common means of communicating the descriptions of graphical entities between the various components of SEMBEGS.  BAGDAMS provides facilities for storing, retrieving, and manipulating the descriptions of graphical entities provided by, and received by application programs, graphics packages, and graphical devices."}
{"DOCID": "3050", "TEXT": "Systems Design Education: A Gaming Approach: One of the problems facing managers of computer installations is the problem of configuring the computer system to meet the demands made by the mix of jobs that the computer center must service. This paper presents a management game that allows the player to configure a computer system to meet a hypothetical job mix is under the control of a game administrator and can be varied to simulate a variety of real-world situations (I/O bound jobs, compute bound jobs, etc.).  The player of the game receives a set of detailed reports on the cost of his choices and a simulated run of the center operating under his choices."}
{"DOCID": "3051", "TEXT": "A Comparison of Heaps and the TL Structure for the SImulation Event Set: None"}
{"DOCID": "3052", "TEXT": "Cold-Start vs. Warm-Start Miss Ratios: In a two-level computer storage hierarchy, miss ratio measurements are often made from a \"cold start,\" that is made with the first-level  store initially empty.  For large capacities the effect on the measured miss ratio of the misses incurred while filling the first-level store can be significant, even for long reference strings.  Use of \"warm-start\" rather than \"cold-start\" miss ratios cast doubt on the widespread belief that the observed \"S-shape\" of lifetime (reciprocal of miss ratio) versus capacity curve indicates a property of behavior of programs that maintain a constant number of pages in main storage. On the other hand, if cold-start miss ratios are measured as a function of capacity and measurement length, then they are useful in studying systems in which operation of a program is periodically interrupted by task switches.  It is shown how to obtain, under simple assumptions, the cache miss ratio for multiprogramming from cold-start miss ratio values and how to obtain approximate cold-start miss ratios from warm-start miss ratios."}
{"DOCID": "3053", "TEXT": "Packed Scatter Tables: Scatter tables for open addressing benefit from recursive entry displacements, cutoffs for unsuccessful searches, and auxiliary cost functions.  Compared with conventional methods, the new techniques provide substantially improved tables that resemble exact-solution optimal packings.  The displacements are depth-limited approximations to an enumerative (exhaustive) optimization, although packing costs remain linear-O(n)-with table size n.  The techniques are primarily suited for important fixed (but possibly quite large) tables for which reference frequencies may be known: op-code tables,spelling dictionaries, access arrays.  Introduction of frequency weights further improves retrievals, but the enhancement may degrade cutoffs."}
{"DOCID": "3054", "TEXT": "Implementing Quicksort Programs: This paper is a practical study of how to implement the Quicksort sorting algorithm and its best variants on real computers, including how to apply various code optimization techniques.  A detailed implementation combining the most effective improvements to Quicksort is given, along with a discussion of how to implement it in assembly language.  Analytic results describing the performance of the programs are summarized.  A variety of special situations are considered from a practical standpoint to illustrate Quicksort's wide applicability as an internal sorting method which requires negligible extra storage."}
{"DOCID": "3055", "TEXT": "An Analysis of Algorithms for the Dutch National Flag Problem: Solutions to the Dutch National Flag Problem have been given by Dijkstra [1] and Meyer [3]. Dijkstra starts with a simple program and arrives at an improved program by refinement.  Both of the algorithms given by Dijkstra are shown to have an expected number of swaps which is 2/3N + 0(1) and that these values differ at most by 1/3 of a swap and asymptotically by 1/4 of a swap.  The algorithm of Meyer is shown to have expected swap complexity 5/9N."}
{"DOCID": "3056", "TEXT": "Counting Large Numbers of Events in Small Registers: It is possible to use a small counter to keep approximate counts of large numbers.  The resulting expected error can be rather precisely controlled.  An example is given in which 8-bit counters (bytes) are used to keep track of as many as 130,000 events with a relative error which is substantially independent of the number n of events.  This relative error can be expected to be 24 percent or less 95 percent of the time (i.e.o = n/8).  The techniques could be used to advantage in multichannel counting hardware or software used for the monitoring of experiments or processes."}
{"DOCID": "3057", "TEXT": "Optimal His togram Matching by Monotone Gray Level Transformation: This paper investigates the problem of optimal his togram matching using monotone gray level transformation, which always assigns all picture points of a given gray level i to another gray level T(i) such that if i > j, then T(i) > T(j).  The objective is to find a transformed digital picture of a given picture such that the sum of absolute errors between the gray level his togram of the transformed picture and that of a reference picture is minimized. This is equivalent to placing k1 linearly ordered objects of different sized one by one into k2 linearly ordered boxes of assorted sizes, such that the accumulated error of space under packed or overpacked in the boxes is minimized; the placement function is monotonic, which ensures a polynomial time solution to this problem.  A tree search algorithm for optimal his togram matching is presented which has time complexity O(k1 x k2).  If the monotone property is dropped, then the problem becomes NP-complete, even if it is restricted to k2 = 2."}
{"DOCID": "3058", "TEXT": "Jump Searching: A Fast Sequential Search Technique: When sequential file structures must be used and binary searching is not feasible, jump searching becomes an appealing alternative.  This paper explores variants of the classic jump searching scheme where the optimum jump size is the square root of the number of records.  Multiple level and variable size jump strategies are explored, appropriate applications are discussed and performance is evaluated."}
{"DOCID": "3059", "TEXT": "Models for Parallel Processing WIthin Programs: Application to CPU:I/O and I/O:I/O Overlap: Approximate queueing models for internal parallel processing by individual programs in a multiprogrammed system are developed in this paper.  The solution technique is developed by network decomposition.  The models are formulated in terms of CPU:I/O and I/O:I/O overlap and applied to the analysis of these problems. The percentage performance improvement from CPU:I/O overlap is found to be greatest for systems which are in approximate CPU:I/O utilization balance and for low degrees of multiprogramming.  The percentage improvement from I/O:I/O overlap is found to be greatest for systemtems in which the I/O system is more utilized than the CPU."}
{"DOCID": "3060", "TEXT": "Fortran 77: There is a new standard Fortran.  The official title is \"American National Standard Programming Language Fortran, X3.9-1978,\" but it is more commonly referred to as \"Fortran 77,\" since its development was completed in 1977.  It replaces the Fortran standard designated X3.9-1966.  This paper describes many of the features of Fortran 77 and also provides some information about how and why the standard was developed."}
{"DOCID": "3061", "TEXT": "Simulations of Dynamic Sequential Search Algorithms: None"}
{"DOCID": "3062", "TEXT": "Real Time Plotting of Approximate Contour Maps: None"}
{"DOCID": "3063", "TEXT": "A Note on Virtual Memory Indexes: None"}
{"DOCID": "3064", "TEXT": "Event Manipulation for Discrete Simulations Requiring Large Numbers of Events: The event-manipulation system presented here consists of two major parts.  The first part addresses the familiar problem of event scheduling efficiency when the number of scheduled events grows large. The second part deals with the less apparent problem of providing efficiency and flexibility as scheduled events are accessed to be executed.  Additional features and problems dealt with include the proper handling of simultaneous events; that certain events must be created, scheduled, and executed at the same points in simulated time; that infinite loops caused by the concatenation of such \"zero-time\" events are possible and must be diagnosed; that maintaining various event counts is practical and economical; and that a capability for handling  \"time-displaceable\" events is desirable and possible."}
{"DOCID": "3065", "TEXT": "Right Brother Trees: Insertion and deletion are provided for the class of right (or one-sided) brother trees which have O (log n) performance.  The importance of these results stems from the close relationship of right brother trees which have an insertion algorithm operating in O (log2 n).  Further, although both insertion and deletion can be  carried out in O (log n) time for right brother trees, it appears that the insertion algorithm is inherently much more difficult than the deletion algorithm-the reverse of what one usually obtains."}
{"DOCID": "3066", "TEXT": "A Controlled Experiment in Program Testing and Code Walkthroughs/Inspections: This paper describes an experiment in program testing, employing 59 highly experienced data processing professionals using seven methods to test a small PL/I program.  The results show that the popular code walk through/inspection method was as effective as other computer-based methods in finding errors and that the most effective methods (in terms of errors found and cost) employed pairs of subjects who tested the program independently and then pooled their findings.  The study also shows that there is a tremendous amount of variability among subjects and that the ability to detect certain types of errors varies from method to method."}
{"DOCID": "3067", "TEXT": "Generalized Working Sets for Segment Reference Strings: The working-set concept is extended for programs that reference segments of different sizes. The generalized working-set policy (GWS) keeps as its resident set those segments whose retention costs do not exceed their retrieval costs.  The GWS is a model for the entire class of demand-fetching memory policies that satisfy a resident-set inclusion property. A generalized optimal policy (GOPT) is also defined; at its operating points it minimizes aggregated retention and swapping costs.  Special cases of the cost structure allow GWS and GOPT to simulate any known stack algorithm, the working set, and VMIN.  Efficient procedures for computing demand curves showing swapping load as a function of memory usage are developed for GWS and GOPT policies.  Empirical data from an actual system are included."}
{"DOCID": "3068", "TEXT": "A Model for Verification of Data Security in Operating Systems: Program verification applied to kernel architectures forms a promising method for providing uncircumventably secure, shared computer systems.  A precise definition of data security is developed here in terms of a general model for operating systems. This model is suitable as a basis for verifying many of those properties of an operating system which are necessary to assure reliable enforcement of security.  The application of this approach to the UCLA secure operating system is also discussed."}
{"DOCID": "3069", "TEXT": "A Practical Interprocedural Data Flow Analysis Algorithm: A new interprocedural  data flow analysis algorithm is presented and analyzed.  The algorithm associates with each procedure in a program information about which variables may be modified, which may be used, and which are possibly preserved by a call on the procedure, and all of its subcalls.  The algorithm is sufficiently powerful to be used on recursive programs and to deal with the sharing of variables which arises through reference parameters.  The algorithm is unique in that it can compute all of this information in a single pass, not requiring a prepass to compute calling relationships or sharing patterns. The algorithm is asymptotically optimal in time complexity. It has been implemented and is practical even on programs which are quite large."}
{"DOCID": "3070", "TEXT": "Hybrid Simulation Models of Computer Systems: This paper describes the structure and operation of a hybrid simulation model in which both discrete-event simulation and analytic techniques are combined to produce efficient yet accurate system models.  In an example based on a simple hypothetical computer system, discrete-event simulation is used to model the arrival and activation of jobs, and a central-server queueing network models the use of system processors.  The accuracy and efficiency of the hybrid technique are demonstrated by comparing the result and computational costs of the hybrid model of the example with those of an equivalent simulation-only model."}
{"DOCID": "3071", "TEXT": "An Algorithm Using Symbolic Techniques for the Bel-Petrov Classification of Gravitational Fields: In this note, an algorithm is presented for the symbolic calculation of certain algebraic invariants of the Weyl tensor which permits the determination of the Bel-Petrov types of a gravitational field. This algorithm, although more specialized than that of D'Inverno and Russell-Clark, requires neither the use of a special coordinate system nor the spin coefficient formalism.  The algorithm has been implemented in FORMAC and is designed to complete the classification scheme proposed by Petrov in his book.  An appendix contains examples illustrating the use of the algorithm."}
{"DOCID": "3072", "TEXT": "Feedback Coupled Resource Allocation Policies in the Multiprogramming- Multiprocessor Computer System: Model studies of some integrated, feedback-driven scheduling systems for multiprogrammed- multiprocessor computer systems are presented.  The basic control variables used are the data-flow rates for the processes executing on the CPU.  The model systems feature simulated continuous-flow and preempt-resume scheduling of input-output activity.  Attention is given to the amount of memory resource required for effective processing of the I/O activity (buffer space assignment). The model studies used both distribution-driven and trace-driven techniques.  Even relatively simple dynamic schedulers are shown to improve system performance (as measured by user CPU time) over that given by optimal or near-optimal static schedulers imbeded in identical system structures and workload environments. The improvement is greatest under a heavy I/O demand workload."}
{"DOCID": "3073", "TEXT": "Communicating Sequential Processes: This paper suggests that input and output are basic primitives of programming and that parallel composition of communicating sequential processes is a fundamental program structuring method.  When combined with a development of Dijkstra's guarded command, these concepts are surprisingly versatile. Their use is illustrated by sample solutions of a variety of familiar programming exercises."}
{"DOCID": "3074", "TEXT": "A Time- and Space- Efficient Garbage Compaction Algorithm: Given an area of storage containing scattered, marked nodes of differing sizes, one may wish to rearrange them into a compact mass at one end of the area while revising all pointers to marked nodes to show their new locations.  An algorithm is described here which accomplishes this task in linear time relative to the size of the storage area, and in a space of the order of one bit for each pointer.  The algorithm operates by reversibly encoding the situation (that a collection of locations point to a single location) by a linear list, emanating from the pointed-to location, passing through the pointing locations, and terminating with the pointed-to location's transplanted contents."}
{"DOCID": "3075", "TEXT": "Fast Parallel Sorting Algorithms: A parallel bucket-sort algorithm is presented that requires time O(log n) and the use of n processors.  The algorithm makes use of a technique that requires more space than the product of processors and time.  A realistic model is used model is used in which no memory contention is permitted.  A procedure is also presented to sort n numbers in time O(k log n) using n 1 + 1/k processors, for k an arbitrary integer.  The model of computation for this procedure permits simultaneous fetches from the same memory location."}
{"DOCID": "3076", "TEXT": "Value Conflicts and Social Choice in Electronic Funds Transfer System Developments: During the last few years, computer-based systems which automate the transfer and recording of debits and credits have begun to be implemented on a large scale.  These systems promise both financial benefits for the institutions that use them and potential conveniences to their customers.  However, they also raise significant social, legal, and technical questions that must be resolved if full scale systems for Electronic Funds Transfer (EFT) are not to cause more problems for the larger public than they solve.  This paper examines the incentives for EFT developments and the social problems they raise in the context of conflicts between five different value positions that are often implicit in analyses of proposed EFT arrangements.  These conflicts reflect the relative importance of certain problems for specific groups.  The value positions implicit in EFT proposals help to organize analyses of market arrangements, system reliability, and privacy of transactions.  These topics are analyzed in this article and related to the value positions held by concerned parties.  Last, the ways in which the public can learn about the social qualities of different EFT arrangements and the pace of EFT developments are both discussed in the context of social choice."}
{"DOCID": "3077", "TEXT": "Can Programming Be Liberated from the von Neumann Style?  A Functional Style and Its Algebra of Programs: Conventional programming languages are growing ever more enormous, but not stronger.  Inherent defects at the most basic level cause them to be both fat and weak: their primitive word-at-a-time style of programming inherited from their common ancestor-the von Neumann computer, their close coupling off semantics to state transitions, their division of programming into a world of expressions and a world of statements, their inability to effectively use powerful combining forms for building new programs from existing ones, and their lack of useful mathematical properties for reasoning about programs. An alternative functional style of programming is founded on the use of combining forms for creating programs. Functional programs deal with structured data, are often nonrepetitive and nonrecursive, are hierarchically constructed, do not name their arguments, and do not require the complex machinery of procedure declarations to become generally applicable.  Combining forms can use high level programs to build still higher level ones in a style not possible in conventional languages. Associated with the functional style of programming is an algebra of programs whose variables range over programs and whose operations are combining forms. This algebra can be used to transform programs and to solve equations whose \"unknowns\" are programs in much the same way one transforms equations in high school algebra.  These transformations are given by algebraic laws and are carried out in the same language in which programs are written.  Combining forms are chosen not only for their programming power but also for the power of their associated algebraic laws.  General theorems of of the algebra give the detailed behavior and termination conditions for large classes of programs.  A new class of computing systems uses the functional programming style both in its programming language and in its state transition rules.  Unlike von Neumann languages, these systems have semantics loosely coupled to states-only one state transition occurs per major computation."}
{"DOCID": "3078", "TEXT": "Analysis of the Availability of Computer Systems Using Computer- Aided Algebra: Analytical results, related to the availability of a computer system constructed of unreliable processors, are presented in this paper.  These results are obtained by using various computer-aided algebraic manipulation techniques.  A major purpose of this paper is to demonstrate that the difficulties of obtaining analytical solutions to Markov processes can be considerably reduced by the application of symbol manipulation programs.  Since many physical systems can be modeled by Markov and semi-Markov processes, the potential range of application of these techniques is much wider than the problem of availability analyzed here."}
{"DOCID": "3079", "TEXT": "An Algorithm for Reasoning About Equality: A simple technique for reasoning about equalities that is fast and complete for ground formulas with function symbols and equality is presented. A proof of correctness is given as well."}
{"DOCID": "3080", "TEXT": "Proving the Correctness of Heuristically Optimized Code: A system for proving that programs written in a high level language are correctly translated to a low level language is described.  A primary use of the system is as a post optimization step in code generation.  The low level language programs need not be generated by a compiler and in fact could be hand coded.  Examples of the usefulness of such a system are given.  Some interesting results are the ability to handle programs that implement recursion by bypassing the start of the program, and the detection and pinpointing of a wide class of errors in the low level language programs.  The examples demonstrate that optimization of the genre of this paper can result in substantially faster operation and the saving of memory in terms of program and stack sizes."}
{"DOCID": "3081", "TEXT": "Shallow Binding in Lisp 1.5: Shallow binding is a scheme which allows the value of a variable to be accessed in a bounded amount of computation.  An elegant model for shallow binding in  Lisp 1.5 is presented in which context-switching is an environment tree transformation called rerooting. Rerooting is completely general and reversible, and is optional in the sense that a Lisp 1.5 interpreter will operate correctly whether or not rerooting is invoked one very context change.   Since rerooting leaves assoc [v, a] invariant, for all variables v and all environments a, the programmer can have access to a rerooting primitive, shallow[], which gives him dynamic control over whether accesses are shallow or deep, and which affects only the speed of execution of a program, not its semantics.  In addition, multiple processes can be active in the same environment structure, so long as rerooting is an indivisible operation. Finally, the concept of rerooting is shown to combine the concept of shallow binding in Lisp with Dijkstra's display for Algol and hence is a general model for shallow binding."}
{"DOCID": "3082", "TEXT": "Time, Clocks, and the Ordering of Events in a Distributed System: The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events.  The use of the total ordering is illustrated with a method for solving synchronization problems.  The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become."}
{"DOCID": "3083", "TEXT": "Pseudochaining in Hash Tables: This paper presents pseudochaining as a new collision-resolution method.  Pseudochaining is half way between open addressing and chaining.  It owes its name to the fact that link fields are present in each cell of the hash table which permits \"chaining\" of the first overflow items in the table.  The efficiency of the method is derived and a tradeoff analysis is given."}
{"DOCID": "3084", "TEXT": "Interpolation Search -A Log LogN Search: Interpolation search is a method of retrieving a desired record by key in an ordered file by using the value of the key and the statistical distribution of the keys.  It is shown that on the average log logN file accesses are required to retrieve a key, assuming that the N keys are uniformly distributed. The number of extra accesses is also estimated and shown to be very low.  The same holds if the cumulative distribution function of the keys is known.  Computational experiments confirm these results."}
{"DOCID": "3085", "TEXT": "An O(n) Algorithm for Determining a Near-Optimal Computation Order of Matrix Chain Products: This paper discusses the computation of matrix chain products of the form M1 x M2 x ... x Mn where Mi's are matrices.  The order in which the matrices are computed affects the number of operations. A sufficient condition about the association of the matrices in the optimal order is presented.  An O(n) algorithm to find an order of computation which takes less than 25 percent longer than the optimal time Topt is also presented.  In most cases, the algorithm yields the optimal order or an order which takes only a few percent longer than Topt (less than 1 percent on the average)."}
{"DOCID": "3086", "TEXT": "On the Complexity of Computing the Measure of U[ai, bi]: The decision tree complexity of computing the measure of the union of n (possibly overlapping) intervals is shown to be  (n log n), even if comparisons between linear functions of the interval endpoints are allowed.  The existence of an   (n log n) lower bound to determine whether any two of n real numbers are within   of each other is also demonstrated.  These problems provide an excellent opportunity for discussing the effects of the computational model on the ease of analysis and on the results produced."}
{"DOCID": "3087", "TEXT": "An English Language Question Answering System for a Large Relational Database: By typing requests in English, casual users will be able to obtain explicit answers from a large relational database of aircraft flight and maintenance data using a system called PLANES.  The design and implementation of this system is described and illustrated with detailed examples of the operation of system components and examples of overall system operation.  The language processing portion of the system uses a number of augmented transition networks, each of which matches phrases with a specific meaning, along with context registers (his tory keepers) and concept case frames; these are used for judging meaningfulness of questions, generating dialogue for clarifying partially understood questions, and resolving ellipsis and pronoun reference problems.  Other system components construct a formal query for the relational database, and optimize the order of searching relations. Methods are discussed for handling vague or complex questions and for providing browsing ability. Also included are discussions of important issues in programming natural language systems for limited domains, and the relationship of this system to others."}
{"DOCID": "3088", "TEXT": "General Equations for Idealized CPU-I/O Overlap Configurations: General equations are derived for estimating the maximum possible utilization of main storage partitions, CPU and I/O devices under different conditions in an idealized CPU-I/O overlap model of multiprogrammed computer systems.  The equations are directly applicable to any configuration consisting  of sets of identical CPU's I/O processors, main storage partitions and user tasks.  Examples are provided to illustrate the use of the equations to compute effective processing time per record and expected timesharing response time under both balanced and unbalanced resource utilization conditions."}
{"DOCID": "3089", "TEXT": "Performance of Rollback Recovery Systems under Intermittent Failures: A mathematical model of a transaction-oriented system under intermittent failures is proposed. The system is assumed to operate with a checkpointing and rollback/recovery method to ensure reliable information processing.  The model is used to derive the principal performance measures, including availability, response time, and the system saturation point."}
{"DOCID": "3090", "TEXT": "Automated Welfare Client-Tracking and Service Integration: The Political Economy of Computing: The impacts of an automated client-tracking system on the clients, caseworkers, administrators, and operations of the welfare agencies that use it are reported.  The major impact of this system was to enhance the administrative  attractiveness of the using agencies in the eyes of funders rather than to increase their internal administrative efficiency. This impact is a joint product of both the technical features of the computer-based system and of the organizational demands placed upon different agencies, administrators, and caseworkers.  It illustrates the way \"successful\" automated information systems fit the political economies of the groups that use them."}
{"DOCID": "3091", "TEXT": "Some Basic Determinants of Computer Programming Productivity: The propose of this research was to examine the relationship between processing characteristics of programs and experience characteristics of programmers and program development time.  The ultimate objective was to develop a technique for predicting the amount of time necessary to create a computer program.  The fifteen program characteristics hypothesized as being associated with an increase in programming time required are objectively measurable from preprogramming specifications.  The five programmer characteristics are experience-related and are also measurable before a programming task is begun.  Nine program characteristics emerged as major influences on program development time, each associated with increased program development time.  All five programmer characteristics were found to be related to reduced program development time. A multiple regression  equation which contained one programmer characteristic and four program characteristics gave evidence of good predictive power for forecasting program development time."}
{"DOCID": "3092", "TEXT": "Characteristics of Application Software Maintenance: Maintenance and enhancement of application software consume a major portion of the total life cycle cost of a system.  Rough estimates of the total systems and programming resources consumed range as high as 75-80 percent in each category.  However, the area has been given little attention in the literature.  To analyze the problems in this area a questionnaire was developed and pretested.  It was then submitted to 120 organizations.  Respondents totaled 69.  Responses were analyzed with the SPSS statistical package.  The results of the analysis indicate that: (1) maintenance and enhancement do consume much of the total resources of systems and programming groups; (2) maintenance and enhancement tend to be viewed by management as at least somewhat more important than new application software development; (3) in maintenance and enhancement, problems of a management orientation tend to be more significant than those of a technical orientation; and (4) user demands for enhancements and extension constitute the most important management problem area."}
{"DOCID": "3093", "TEXT": "Automatic Error Recovery for LR Parsers: In this paper we present a scheme for detecting and recovering from syntax errors in programs. The scheme, which is based on LR parsing, is driven by information which is directly and automatically obtainable from the information that is already present in an LR parser.  The approach, which is patterned after that of Levy and Graham and Rhodes, appears to provide error recovery which is both simple and powerful."}
{"DOCID": "3094", "TEXT": "Analyses of Deterministic Parsing Algorithms: This paper describes an approach for determining the minimum, maximum, and average times to parse sentences acceptable by a deterministic parser. These quantities are presented in the form of symbolic formulas, called time-formulas.  The variables in these formulas represent not only the length of the input string but also the time to perform elementary operations such as pushing, popping, subscripting, iterating, etc.  By binding to the  variables actual numerical values corresponding to a given compiler-machine configuration, one can determine the execution time for that configuration.  Time-formulas are derived by examining the grammar rules and the program representing the algorithm one wishes to analyze.  The approach is described by using a specific grammar that defines simple arithmetic expressions.  Two deterministic parsers are analyzed: a top-down recursive descent LL(1) parser, and a bottom-up SLR(1) parser.  The paper provides estimates for the relative efficiencies of the two parsers.  The estimates applicable to a specific machine, the PDP-10, are presented and substantiated buy benchmarks.  Finally, the paper illustrates the proposed approach by applying it to the analyses of parsers for a simple programming language."}
{"DOCID": "3095", "TEXT": "A Selective Traversal Algorithm for Binary Search Trees: The problem of selecting data items from a binary search tree according to a list of range conditions is considered.  The process of visiting a minimal number of nodes to retrieve data satisfying the range conditions is called selective traversal.  Presented in this paper is an algorithm for selective traversal which uses a tag field for each node in the tree.  The algorithm is particularly useful and efficient when examination of data is more time consuming than examination of a tag field."}
{"DOCID": "3096", "TEXT": "An Optimal Method for Deletion in One-Sided Height-Balanced Trees: A one-sided height-balanced tree is a binary tree in which every node's right subtree has a height which is equal to or exactly one greater than the height of its left subtree.  It has an advantage over the more general AVL tree in that only one bit of balancing information is required (two bits are required for the ACL tree).  It is shown that deletion of an arbitrary node of such a tree can be accomplished in O(logn) operations, where n is the number of nodes in the tree.  Moreover the method is optimal in the sense that its complexity cannot be reduced in order of magnitude.  This result, coupled with earlier results by Hirschberg, indicates that, of the three basic problems of insertion, deletion, and retrieval, only insertion is adversely affected by this modification of an AVL tree."}
{"DOCID": "3097", "TEXT": "Optimal Shift Strategy for a Block-Transfer CCD Memory: For the purposes of this paper, a block-transfer CCD memory is composed of serial shift registers whose shift rate can vary, but which have a definite minimum shift rate (the refresh rate) and a definite maximum shift rate.  The bits iin the shift registers are numbered 0 to N - 1, and blocks of N bits are always transferred, always starting at bit 0.   What is the best shift strategy so that a block transfer request occurring at a random time will have to wait the minimal amount of time before bit 0 can be reached? The minimum shift rate requirement does not allow one to  simply \"park\" at bit 0 and wait for a transfer request.  The optimal strategy involves shifting as slowly as possible until bit 0 is passed, then shifting as quickly as possible until a critical boundary is reached, shortly before bit 0 comes around again. This is called the \"hurry up and wait\" strategy and is well known outside the computer field.  The block-transfer CCD memory can also be viewed as a paging drum with a variable (bounded) rotation speed."}
{"DOCID": "3098", "TEXT": "Computer Generation of Gamma Random Variables: A new method for generating random variables from the gamma distribution with nonintegral shape parameter a is proposed.  This method is similar to two other methods recently given by Wallace and Fishman. It is compared with Fishman's and Ahrens and Dieter's methods. The core storage requirements and programming effort for this method are similar to those of Fishman's method.  The proposed method is the same as Fishman's method for 1 < a < 2 and is faster than Fishman's method for 3 < a < 19.  Also, the proposed method is much simpler than Ahrens and Dieter's method and is faster for a < 8."}
{"DOCID": "3099", "TEXT": "New Sufficient Optimality Conditions for Integer Programming and their Application: The purpose of this report is to present a new class of sufficient optimality conditions for pure and mixed integer programming problems.  Some of the sets of sufficient conditions presented can be thought of as generalizations of optimality conditions based on primal-dual complementarity in linear programming.  These sufficient conditions are particularly useful for the construction of difficult integer programming problems with known optimal solutions.  These problems may then be used to test and/or \"benchmark\" integer programming codes."}
{"DOCID": "3100", "TEXT": "An Interference Matching Technique for Inducing Abstractions: A method for inducing knowledge by abstraction from a sequence of training examples is described. The proposed method, interference matching, induces abstractions by finding relational properties common to two or more exemplars.  Three tasks solved by a program that uses an interference-matching algorithm are presented.  Several problems concerning the description of the training examples and the adequacy of interference matching are discussed, and directions for future research are considered."}
{"DOCID": "3101", "TEXT": "The SL5 Procedure Mechanism: This paper describes an integrated procedure mechanism that permits procedures to be used as recursive functions or as coroutines.  This integration is accomplished by treating procedures and their activation records (called environments) as data objects and by decomposing procedure invocation into three separate components at the source-language level. In addition, argument binding is under the control of the programmer, permitting the definition of various methods of argument transmission in the source language itself.  The resulting procedure mechanism,which is part of the SL5 programming language, is well suited to goal-oriented problems and to other problems that are more readily programmed by using coroutines. Several examples are given."}
{"DOCID": "3102", "TEXT": "Incorporation of Units into Programming Languages: The issues of how a programming language might aid in keeping track of physical units (feet, sec, etc.) are discussed.  A method is given for the introduction of relationships among units (a watt is volts*amps, a yard is three feet) and subsequent automatic conversion based upon these relationships. Various proposals for syntax are considered."}
{"DOCID": "3103", "TEXT": "Automatic Data Structure Selection: An Example and Overview: The use of several levels of abstraction has proved to be very helpful in constructing and maintaining programs.  When programs are designed with abstract data types such as sets and lists, programmer time can be saved by automating the process of filling in low-level implementation details.  In the past, programming systems have provided only a single general purpose implementation for an abstract type. Thus the programs produced using abstract types were then inefficient in space or time.  In this paper a system for automatically choosing efficient implementations for abstract types from a library of implementations is discussed.  This process is discussed in detail for an example program.  General issues in data structure selection are also reviewed."}
{"DOCID": "3104", "TEXT": "Test Data as an Aid in Proving Program Correctness: Proofs of program correctness tend to be long and tedious, whereas testing, though useful in detecting errors, usually does not guarantee correctness. This paper introduces a techniques whereby test data can be used in proving program correctness. In addition to simplifying the process of proving correctness, this method simplifies the process of providing accurate specification for a program.  The applicability of this technique to procedures and recursive programs is demonstrated."}
{"DOCID": "3105", "TEXT": "A Language Extension for Expressing Constraints on Data Access: Controlled sharing of information is needed and desirable for many applications and is supported in operating systems by access control mechanisms.  This paper shows how to extend programming languages to provide controlled sharing.  The extension permits expression of access constraints on shared data. Access constraints can apply both to simple objects, and to objects that are components of larger objects, such as bank account records in a bank's data base. The constraints are stated declaratively, and can be enforced by static checking similar to type checking. The approach can be used to extend any strongly-typed language, but is particularly suitable for extending languages that support the notion of abstract data types."}
{"DOCID": "3106", "TEXT": "A Fast Algorithm for Copying List Structures: An algorithm is presented for copying an arbitrarily linked list structure into a block of contiguous storage locations without destroying  the original list.  Apart from a fixed number of program variables, no auxiliary storage, such as a stack, is used. The algorithm needs no mark bits and operates in linear time.  It is shown to be significantly faster than Fisher's algorithm, the fastest previous linear-time algorithm for the same problem.  Its speed comes mainly from its efficient list-traversal technique, which folds the processing stack into the structure being built, and from its classification of list cells into nine types, which enables processing operations to be optimized for each type."}
{"DOCID": "3107", "TEXT": "Generating Beta Variates with Nonintegrel Shape Parameters: A new rejection method is described for generating beta variates.  The method is compared with previously published methods both theoretically and through computer timings.  It is suggested that the method has advantages in both speed and programming simplicity over previous methods, especially for \"difficult\" combinations of parameter values."}
{"DOCID": "3108", "TEXT": "Economical Encoding of Commas Between Strings: A method for insertion of delimiters between strings without using new symbols is presented. As the lengths of the strings increase, the extra cost, in terms of prolongation, becomes vanishingly small compared to the lengths of the strings."}
{"DOCID": "3109", "TEXT": "A Data Structure for Manipulating Priority Queues: A data structure is described which can be used for representing a collection of priority queues. The primitive operations are insertion, deletion, union, update, and search for an item of earliest priority."}
{"DOCID": "3110", "TEXT": "Assembling Code for Machines with Span-Dependent Instructions: Many modern computers contain instructions whose lengths depend on the distance from a given instance of such an instruction to the operand of that instruction.  This paper considers the problem of minimizing the lengths of programs for such machines. An efficient solution is presented for the case in which the operand of every such \"span-dependent\" instruction is either a label or an assembly-time expression of a certain restricted form.If this restriction is relaxed by allowing these operands to be more general assembly-time expressions, then the problem is shown to be NP-complete."}
{"DOCID": "3111", "TEXT": "Secure Communications Over Insecure Channels: According to traditional conceptions of cryptographic security, it is necessary to transmit a key, by secret means, before encrypted messages can be sent securely.  This paper shows that it is possible to select a key over open communications channels in such a fashion that communications security can be maintained.  A method is described which forces any enemy to expend an amount of work which increases as the square of the work required of the two communicants to select the key.  The method provides a logically new kind of protection against the passive eaves dropper.  It suggests that further research on this topic will be highly rewarding, both in a theoretical and a practical sense."}
{"DOCID": "3112", "TEXT": "List Processing in Real Time on a Serial Computer: A real-time list processing system is one in which the time required by the elementary list operations (e.g. CONS, CAR, CDR, RPLACA, REPLACD, EQ, and ATOM in LISP) is bounded by a (small) constant. Classical implementations of list processing systems lack this property because allocating a list cell from the heap may cause a garbage collection, which process requires time proportional to the heap size to finish.  A real-time list processing system is presented which continuously reclaims garbage, including directed cycles, while linearizing and compacting the accessible cells into contiguous locations to avoid fragmenting the free storage pool.  The program is small and requires no time-sharing interrupts, making it suitable for microcode.  Finally, the system requires the same average time, and not more than twice the space, of a classical implementation, and those space requirements can be reduced to approximately classical proportions by compact list representation. Arrays of different sizes, a program stack, and hash linking are simple extensions to our system, and reference counting is found to be inferior for many applications."}
{"DOCID": "3113", "TEXT": "Optimal Conversion of Extended-Entry Decision Tables with General Cost Criteria: A general dynamic programming algorithm for converting limited, extended, or mixed entry decision tables to optimal decision trees is presented which can take into account rule frequencies or probabilities, minimum time and/or space cost criteria, common action sets, compressed rules and ELSE rules, sequencing constraints on condition tests, excludable combinations of conditions, certain ambiguities, and interrupted rule masking."}
{"DOCID": "3114", "TEXT": "A Technique for Isolating Differences Between Files: A simple algorithm is described for isolating the differences between two files.  One application is the comparing of two versions of a source program or other file in order to display all differences. The algorithm isolates differences in a way that corresponds closely to our intuitive notion of difference, is easy to implement, and is computationally efficient, with time linear in the file length.  For most applications the algorithm isolates differences similar to those isolated by the longest common subsequence. Another application of this algorithm merges files containing independently generated changes into a single file.  The algorithm can also be used to generate efficient encodings of a file in the form of the differences between itself and a given \"datum\" file, permitting reconstruction of the original file from the difference and datum files."}
{"DOCID": "3115", "TEXT": "Orderly Enumeration of Nonsingular Binary Matrices Applied to Text Encryption: Nonsingular binary matrices of order N, i.e., nonsingular over the field {0, 1}, and an initial segment of the natural numbers are placed in one-to-one correspondence.  Each natural number corresponds to two intermediate vectors.  These vectors are mapped into a nonsingular binary matrix.  Examples of complete enumeration of all 2 x 2 and 3 x 3 nonsingular binary matrices were produced by mapping the intermediate vectors to the matrices.  The mapping has application to the Vernam encipherment method using pseudorandom number sequences.  A bit string formed form bytes of text of a data encryption key can be used as a representation of a natural number. This natural number is transformed to a nonsingular binary matrix.  key leverage is obtained by using the matrix as a\"seed\" in a shift register sequence pseudorandom number generator."}
{"DOCID": "3116", "TEXT": "Interference Detection Among Solids and Surfaces: In many industrial environments it is necessary to determine whether r there is interference among components.  There are many potential interference problems in products made up of assemblies of components and in product manufacturing and testing.  Typically, drawings are used in an attempt to detect such unwanted interferences, but the two-dimensional, static drafting medium does not always show interferences among three-dimensional, moving parts.  This paper presents a computer representation for solids and surfaces and algorithms which carry out interference checking among objects so represented. Objects are represented as polyhedra or as piecewise planar surfaces.  Two types of interference checking are discussed: detection of intersections among objects in fixed positions and detection of collisions among objects moving along specified trajectories."}
{"DOCID": "3117", "TEXT": "The Impact and Use of Computer Technology by the Police: Over the past decade there has been a significant growth in the use of computer technology by U.S. police departments.  This growth, however, has been at a slower rate than predicted in the early 1970's.  Further, when computer applications extend beyond \"routine\" uses to \"nonroutine\" efforts, such as resource allocation or computer-aided-dispatch systems where the machine begins to become a tool for decision making, strategic planning and person/machine interaction, the results of the technology to date have been mixed.  This paper reports on case studies and surveys which provinsights on the implementation and impact of police computer technology and the relationship of this technology to law enforcement and society."}
{"DOCID": "3118", "TEXT": "Permutation of Data Blocks in a Bubble Memory: A common internal organization of bubble memories consists of a set of (minor) loops, connected through another (major) loop.  The problem of obtaining any give n permutation of the minor loop contents in minimum time is studied in this paper.  A lower bound to the number of steps required buy a permutation algorithm is derived, and the class of optimum algorithms is identified."}
{"DOCID": "3119", "TEXT": "The Impact of Distributions and Disciplines on Multiple Processor Systems: Simple queueing models are used to study the performance tradeoffs of multiple processor systems.  Issues considered include the impact of CPU service disciplines and distributions, level of multiprogramming, multitasking, and job priorities."}
{"DOCID": "3120", "TEXT": "An Event-Driven Compiling Technique: Due to the linear structure of source text, difficulties may arise in a one-pass compilation process.  These difficulties occur when an entity cannot be processed because of a forward reference to information only obtainable from subsequent entities.  Classic solutions ask for data structures appropriate for each case.  A technique is presented here which uses instead control structures, namely events and processes.  The work of the compiler-writer becomes easier both conceptually and in practice because he can forget these problems at the outset and he avoids special processing for each problem. This technique has been applied to the construction of an Algol 68 compiler.  Three examples from that implementation are described and discussed here."}
{"DOCID": "3121", "TEXT": "Syntactic Source to Source Transforms and Program Manipulation: Syntactic transforms are the source to source program transformations which preserve the history of computation, and thus do not modify the execution time.  Combined with a small number of primitive semantic transforms, they provide a powerful tool for program manipulation.  A catalogue of syntactic transforms, and its use for solution of a system of program equations, is given.  Examples of derivation of more complex source to source transformations are also presented.  Two case studies illustrate the way in which syntactic and semantic source to source transformations may be used for development of clear, simple, and reasonably efficient programs."}
{"DOCID": "3122", "TEXT": "Production and Employment of Ph.D.'s in Computer Science - 1977 and 1978"}
{"DOCID": "3123", "TEXT": "Employment Characteristics of Doctoral Level Computer Scientists"}
{"DOCID": "3124", "TEXT": "Recursive Data Structures in APL: A mathematical study of three approaches for defining nested arrays in APL is presented.  Theorems exhibiting the relationships between the definitional systems are given and illustrated through graph representations.  One of the approaches is used to define an APL array to be a recursive data structure equivalent to a tree structure in which all data is stored at the leaves as homogeneous arrays of numbers and characters.  An extension of APL is proposed that includes new primitive functions to manipulate the nesting level of arrays and new operators to assist in the construction of data-driven algorithms."}
{"DOCID": "3125", "TEXT": "Global Optimization by Suppression of Partial Redundancies: The elimination of redundant computations and the moving of invariant computations out of loops are often done separately, with invariants moved outward loop by loop.  We propose to do both at once and to move each expression directly to the entrance of the outermost loop in which it is invariant.  This is done by solving a more general problem, i.e. the elimination of computations performed twice on a given execution path.  Such computations are termed partially redundant.  Moreover, the algorithm does not require any graphical information or restrictions on the shape of the program graph. Testing this algorithm has shown that its execution cost is nearly linear with the size of the program, and that it leads to a smaller optimizer that requires less execution time."}
{"DOCID": "3126", "TEXT": "Comments on Perfect Hashing Functions: A Single Probe Retrieving  Method for Static Sets"}
{"DOCID": "3127", "TEXT": "Thoth, a Portable Real-Time Operating System: Thoth isa real-time operating system which is designed to be portable over a large set of machines.  It is currently running on two minicomputers with quite different architectures.  Both the system and application programs which use it are written in a high-level language. Because the system is implemented by the same software on different hardware, it has the same interface to user programs.  Hence, application programs which use Thoth are  highly portable.  Thoth encourages structuring programs as networks of communicating processes by providing efficient interprocess communication primitives."}
{"DOCID": "3128", "TEXT": "Synchronization with Eventcounts and Sequencers: Synchronization of concurrent processes requires controlling the relative ordering of events in the processes. A new synchronization mechanism is proposed, using abstract objects called eventcounts and sequencers, that allows processes to control the ordering of events directly, rather than using mutual exclusion to protect manipulations of shared variables that control ordering of events.  Direct control of ordering seems to simplify correctness arguments and also simplifies implementation in distributed systems.  The mechanism is defined formally, and then several examples of its use are given.  The relationship of the mechanism to protection mechanisms in the system is explained; in particular, eventcounts are shown to be applicable to situations where confinement of information matters.  An implementation of eventcount s and sequencers in a system with shared memory is described."}
{"DOCID": "3129", "TEXT": "Optimal Storage Allocation for Serial Files: A computer system uses several serial files.  The files reside on a direct-access storage device in which storage space is limited.  Records are added to the files either by jobs in batch processing mode, or by on-line transactions. Each transaction (or job) generates a demand vector which designates the space required in each file for record addition. Whenever one file runs out of space, the system must be reorganized.  This paper considers several criteria for best allocating storage space to the files."}
{"DOCID": "3130", "TEXT": "CURRICULUM '78 - Recommendations for the Undergraduate Program in Computer Science: Contained in this report are the recommendations for the undergraduate degree program in Computer Science of the Curriculum Committee on Computer Science (C3S) of the Association for Computing Machinery (ACM).   The core curriculum common to all computer science undergraduate programs is presented in terms of elementary level topics and courses, and intermediate level courses. Elective courses, used to round out an undergraduate program, are then discussed, and the entire program including the computer science component and other material is presented.  Issues related to undergraduate computer science education, such as service courses, supporting areas, continuing education, facilities, staff, and articulation are presented."}
{"DOCID": "3131", "TEXT": "FOCUS Microcomputer Number System: FOCUS is a number system and supporting computational algorithms especially useful for microcomputer control and other signal processing applications.  FOCUS has the wide-ranging character of floating-point numbers with a uniformity of state distributions that give FOCUS better than a twofold accuracy advantage over an equal word length floating-point system.  FOCUS computations are typically five times faster than single precision fixed-point or integer arithmetic for a mixture of operations, comparable in speed with hardware arithmetic for many applications.  Algorithms for 8-bit and 16-bit implementations of FOCUS are included."}
{"DOCID": "3132", "TEXT": "Experiments with Some Algorithms that Find Central Solutions for Pattern Classification: In two-class pattern recognition, it is a standard technique to have an algorithm finding hyperplanes which separates the two classes in a linearly separable training set.  The traditional methods find a hyperplane which separates all points in the other, but such a hyperplane is not necessarily centered in the empty space between the two classes.  Since a central hyperplane does not favor one class or the other, it should have a lower error rate in classifying new points and is therefore better than a noncentral hyperplane.  Six algorithms for finding central hyperplanes are tested on three data sets.  Although frequently used practice, the modified relaxation algorithm is very poor. Three algorithms which are defined in the paper are found to be quite good."}
{"DOCID": "3133", "TEXT": "Logic and Semantic Networks: An extended form of semantic network is defined, which can be regarded as a syntactic variant of the clausal form of logic. By virtue of its relationship with logic, the extended semantic network is provided with a precise semantics, inference rules, and a procedural interpretation.  On the other hand, by regarding semantic networks as an abstract data structure for the representation of clauses, we provide a theorem-prover with a potentially useful indexing scheme and path-following strategy for guiding the search for a proof."}
{"DOCID": "3134", "TEXT": "The Use of Normal Multiplication Tables for Information Storage and Retrieval: This paper describes a method for the organization and retrieval of attribute based information systems, using the normal multiplication table as a directory for the information system.  Algorithms for the organization an d retrieval of information are described.  This method is particularly suitable for queries requesting a group of information items, all of which possess a particular set of attributes (and possibly some other attributes as well).  Several examples are given; the results with respect to the number of disk accesses and disk space are compared to other common approaches.  Algorithms evaluating the appropriateness of the above approach to a given information system are described.  For a certain class of information systems, the normal multiplication table method yields far more rapid retrieval with a more economical space requirement than conventional systems. Moreover this method incorporates an improved modification of the inverted file technique."}
{"DOCID": "3135", "TEXT": "Detection of Three-Dimensional Patterns of Atoms in Chemical Structures: An algorithm for detecting occurrences of a three-dimensional pattern of objects within a larger structure is presented.  The search technique presented uses the geometric structure of the pattern to define characteristics demanded of candidates for matching. This is useful in cases where the properties of each atom, considered individually, do not adequately limit the number of sets of possible matchings. Several applications of this technique in the field of chemistry are: (1) in pharmacology: searching for a common constellation of atoms in molecules possessing similar biological activities; (2) in X-ray crystallography: fitting a structure or a structural fragment to a set of peaks in the electron-density distribution of a Fourier map; (3) in chemical documentation; retrieving from a file the structures containing specified substructures."}
{"DOCID": "3136", "TEXT": "Price/Performance Patterns of U.S. Computer Systems: Econometric models of the U.S. computer market have been developed to study the relationships between system price and hardware performance.  Single measures of price/performance such as \"Grosch's Law\" are shown to be so over simplified as to be meaningless.  Multiple-regression models predicting system cost as a function of several hardware characteristics do, however, reveal a market dichotomy.  On one hand there exists a stable, price predictable market for larger, general purpose computer systems.  The other market is the developing one for small business computer systems, a market which is relatively unstable with low price predictability."}
{"DOCID": "3137", "TEXT": "A Methodology for the Design of Distributed Information Systems: A macro model of a distributed information system in presented.  The model describes the major costs of using an information system from the perspective of the end-user.  The making evident the effect of various design and operating parameters on overall cost per transaction. The technique is illustrated by application to the design of an interactive transaction processing system."}
{"DOCID": "3138", "TEXT": "A Mathematical Programming Updating Method Using Modified Givens Transformations and Applied to LP Problems: An efficient and numerically stable method is presented for the problem of updating an orthogonal decomposition of a matrix of column (or row) vectors. The fundamental idea is to add a column (or row) analogous to adding an additional row of data in a linear least squares problem. A column (or row) is dropped by a formal scaling with the imaginary unit,  -1, followed by least squares addition of the column (or row).  The elimination process for the procedure is successive ssive application of the Givens transformation in modified (more efficient) form.  These ideas are illustrated with an implementation of the revised simplex method.  The algorithm is a general purpose one that does not account for any particular structure or sparsity in the equations.  Some suggested computational tests for determining signs of various controlling parameters in the revised simplex algorithm are mentioned.  A simple means of constructing test cases and some sample computing times are presented."}
{"DOCID": "3139", "TEXT": "New Methods to Color the Vertices of a Graph: This paper describes efficient new heuristic methods to color the vertices of a graph which rely upon the comparison of the degrees and structure of a graph.  A method is developed which is exact for bipartite graphs and is an important part of heuristic procedures to find maximal cliques in general graphs.  Finally an exact method is given which performs better than the Randall-Brown algorithm and is able to color larger graphs, and the new heuristic methods, the classical methods, and the exact method are compared."}
{"DOCID": "3140", "TEXT": "Social Processes and Proofs of Theorems and Programs: It is argued that formal verifications of programs, no matter how obtained, will not play the same key role in the development of computer science and software engineering as proofs do in mathematics.  Furthermore the absence of continuity, the inevitability of change, and the complexity of specification of significantly many real programs make the form al verification process difficult to justify and manage.  It is felt that ease of formal verification should not dominate program language design."}
{"DOCID": "3141", "TEXT": "An Improved Algorithm for Decentralized Extrema-Finding in Circular Configurations of Processes: This note presents an improvement to LeLann's algorithm for finding the largest (or smallest) of a set of uniquely numbered processes arranged in a circle, in which no central controller exists and the number of processes is not known a priori. This decentralized algorithm uses a technique of selective message extinction in order to achieve an average number of message passes of order (n log n) rather than O(n2)."}
{"DOCID": "3142", "TEXT": "Consumer Difficulties With Computerized Transactions: An Empirical Investigation: The prevalence with which errors may be encountered by the end targets of a computerized process is assessed.  How many and what type of errors occur?  How easily are they corrected?  What is the reaction of consumers to errors-to a failure to correct them?  What can be learned by designers of large management packages from such data? Results show that with the present state of the art, approximately 40 percent of individuals (or households) having average contacts with different types of accounts experience one or more errors per year.  Eighty percent relate to billing.  Attempts to correct errors often turned out to be difficult and not always successful. There appears to be some conflict between computer-using organizations and their public.  Also the role of poor man agement packages including poor software is indicated.  While most management systems may be adequate, results of the survey raise concerns about the timeliness and the number of designs of very large linked program packages (as EFT for instance)."}
{"DOCID": "3143", "TEXT": "Reasoning About Arrays: A variety of concepts, laws, and notations are presented which facilitate reasoning about arrays.  The basic concepts include intervals and their partitions, functional restriction, images, pointwise extension of relations, ordering, single-point variation of functions, various equivalence relations for array values, and concatenation.  The effectiveness of these ideas is illustrated by informal descriptions of algorithms for binary search and merging, and by a short formal proof."}
{"DOCID": "3144", "TEXT": "A Model for and DIscussion of Multi-Interpreter Systems: A multi-interpreter system is a system in which programs execute by virtue of being interpreted by other programs, which themselves may either be interpreted (i.e. nested interpreters) or run directly on the host machine.  The model reveals the anatomy of interpreters and how these differ from procedures, and exhibits links to protection domains and multiprocessor architectures."}
{"DOCID": "3145", "TEXT": "An Implementation of Structured Walk-Throughs in Teaching Cobol Programming: The effectiveness of structured walk-throughs in teaching introductory Cobol programming was empirically assessed with a sample of 215 under-graduate business administration majors.  Cobol proficiency was measured by a final examination testing (a) knowledge of language rules, (b) ability to read and debug a program, and (c) the ability to write a program.  Analysis of multiple covariance was used to statistically adjust test scores for age and conditional reasoning scores. The findings provide empirical support for incorporating structured walk-throughs into the programming learning process more effectively develop student proficiency in writing Cobol programs."}
{"DOCID": "3146", "TEXT": "An Academic Program Providing Realistic Training in Software Engineering: An academic program at Harvey Mudd College, called the Clinic program, brings projects from industry on campus to be studied and solved by student teams.  The objective of the Clinic is to provide students, working as small teams under careful faculty supervision, an opportunity to work on real world problems of sufficient magnitude and complexity.  Under this program, students can acquire essential skills of software engineering, such as team work, software project management, software design methodology, and communication skills, in a realistic environment. Sample software projects undertaken by the Clinic are described. Experience so far has shown that the program is a viable transition from an academic to industrial world."}
{"DOCID": "3147", "TEXT": "A Model for Automating File and Program Design in Business Application Systems: This paper discusses a model for finding an efficient implementation of a business application system whose logical specifications have been determined in advance.  The model views file and program design as a problem of systematically coordinating the configurations of datasets and computations.  It uses a straight forward search technique to determine aggregations of computations, aggregations of datasets, device, organization, and key order for each data set, key order for each computation, and access method  for each dataset-computation pair.  Although computational results are presented for a sample problem involving 54 computations and 49 datasets, the main point of the paper is that the underlying model works computationally an d is simple enough to be adapted to many file design situations."}
{"DOCID": "3148", "TEXT": "High Level Programming for Distributed Computing: Programming for distributed and other loosely coupled systems is a problem of growing interest.  This paper describes an approach to distributed computing at the level of general purpose programming languages.  Based on primitive notions of module, message, and transaction key, the methodology is shown to be independent of particular languages and machines.  It appears to be useful for programming a wide range of tasks.  This is part of an ambitious program of development in advanced programming languages, and relations with other aspects of the project are also discussed."}
{"DOCID": "3149", "TEXT": "The Cyclic Order Property of Vertices as an Aid in Scene Analysis: A cyclic-order property is defined for bodies bounded by smooth-curved faces. The property is shown to be useful for analyzing pictures of such bodies, particularly when the line data extracted from the pictures are imperfect. This property augments previously known grammatical rules that determine the existence of three-dimensional bodies corresponding to given two-dimensional line-structure data."}
{"DOCID": "3150", "TEXT": "Beyond Programming Languages: As computer technology matures, our growing ability to create large systems is leading to basic changes in the nature of programming.  Current programming language concepts will not be adequate for building and maintaining systems of the complexity called for by the tasks we attempt.  Just as high level languages enabled the programmer to escape from the intricacies of a machine's order code, higher level programming systems can provide the means to understand and manipulate complex systems and components.  In order to develop such systems, we need to shift our attention away from the detailed specification of algorithms, towards the description of the properties of the packages and objects with which we build.  This paper analyzes some of the shortcomings of programming languages as they now exist, and lays out some possible directions for future research."}
{"DOCID": "3151", "TEXT": "An Optimal Real-Time Algorithm for Planar Convex Hulls: An algorithm is described for the construction in real-time of the convex hull of a set of n points in the plane.   Using an appropriate data structure, the algorithm constructs the convex hull by successive updates, each taking time O(log n), thereby achieving a total processing time O(n log n)."}
{"DOCID": "3152", "TEXT": "Storage Reorganization Techniques for Matrix Computation in a Paging Environment: In order to multiply matrices while minimizing the number of page fetches required, it is often more efficient to reorganize the data into submatrix form and to use block multiplication rather than to use the best known algorithms which leave the matrices stored in row-(or column-)oriented form.  An efficient method for accomplishing this reorganization is given.  This also makes possible the derivation of an asymptotically better bound for multiplication of matrices given in row-oriented form by adapting the technique of Strassen to the reorganized data.  The reorganization/block multiplication scheme is shown to be advantageous for matrices and pages of realistic size; the Strassen adaptation is not.  The former scheme is also shown to be advantageous even if the transpose of one of the matrices is available at no additional cost."}
{"DOCID": "3153", "TEXT": "The Control of Response Times in Multi-Class Systems by Memory Allocations: The possibility of giving different quality of service to jobs of different classes by regulating their memory allocation is examined in the context of a paged computer system.  Two parameterized algorithms which partition the main memory between two classes of jobs are considered.  Initially, a closed system consisting of a process or and paging and file devices, with fixed numbers of jobs, is studied to determine optimal degrees of multiprogramming and the proportion of processor time devoted to each class.  Applying a decomposition approach and treating the closed system as a single server, the response times in an open system with external arrivals are studied.  The object is to investigate the effect of the memory alocation parameters on the expected response times under the two algorithms. Numerical solutions and economical lower bounds for the expected response times as functions of the control parameters are obtained.  A way of applying the results to systems with more than two job classes is indicated."}
{"DOCID": "3154", "TEXT": "Algorithm = Logic + Control: An algorithm can be regarded as consisting of a logic component, which specifies the knowledge to be used in solving problems, and a control component, which determines the problem-solving strategies by means of which that knowledge is used.  The logic component determines the meaning of the algorithm whereas the control component only affects its efficiency.  The efficiency of an algorithm can often by improving the control component without changing the logic of the algorithm.  We argue that computer programs would be more often correct and more easily improved and modified if their logic and control aspects were identified and separated in the program text."}
{"DOCID": "3155", "TEXT": "The Paradigms of Programming"}
{"DOCID": "3156", "TEXT": "Computing Connected Components on Parallel Computers: We present a parallel algorithm which uses n2 processors to find the connected components of an undirected graph with n vertices in time O(log2n).  An O(log2n) time bound also can be achieved using only n$n/$log2n)) processors. The algorithm can be used to find the transitive closure of a symmetric Boolean matrix.  We assume that the processors have access to a common memory.  Simultaneous access to the same location is permitted for fetch instructions but not for store instructions."}
{"DOCID": "3157", "TEXT": "Proving Termination with Multiset Orderings: A common tool for proving the termination of programs is the well-founded set, a set ordered in such a way as to admit no infinite descending sequences. The basic approach is to find a termination function  that maps the values of the program variables into some well-founded set, such that the value of the termination function is repeatedly reduced throughout the computation.  All too often, the termination functions required are difficult to find and are of a complexity out of proportion to the program under consideration. Multisets (bags) over a given well-founded set S are sets that admit multiple occurrences of elements taken from S.  The given ordering on S induces an ordering on the finite multisets over S.  This multiset ordering is shown to be well-founded.  The multiset ordering enables the use of relatively simple and intuitive termination functions in otherwise difficult termination proofs.  In particular, the multiset ordering is used to prove the termination of production systems, programs defined in terms of sets of rewriting rules."}
{"DOCID": "3158", "TEXT": "Secure Personal Computing in an Insecure Network: A method for implementing secure personal computing in a network with one or more central facilities is proposed.  The method employs a public-key encryption device and hardware keys.  Each user is responsible for his own security and need not rely on the security of the central facility or the communication links.  A user can safely store confidential files in the central facility or transmit confidential data to other users on the network."}
{"DOCID": "3159", "TEXT": "Further Remark on Stably Updating Mean and Standard Deviation Estimates"}
{"DOCID": "3160", "TEXT": "Rejuvenating Experimental Computer Science: This report is based on the results of an NSF sponsored workshop held in Wasington, D.C. on November 2, 1978.  The co-authors of the report are: Gordon Bell, Digital Equipment Corporation; Bernard A. Galler, University of Michigan; Patricia Goldberg, IBM Corporation; John Hamblen, University of Missouri at Rolla; Elliot Pinson, Bell Telephone Laboratories; and Ivan Sutherland, California Institute of Technology.  Also participating in the workshop were representatives of NSF and other government agencies.  In addition to the authors, a number of other people have contributed to the contents of this report.  In preparation for the original workshop, all doctorate-granting computer science departments in the nation were asked for comments and suggestions on the problems of experimental computer science. A version of the current report dated January 15 was circulated to these departments and to a number of industrial and government groups for criticism. The editors and authors of this final version gratefully acknowledge the contribution of a large number of other people at all stages in the preparation of the report. $Note: Following this presentation of the report, there is a position paper on the crisis in experimental computer science written by the ACM Executive Committee.)"}
{"DOCID": "3161", "TEXT": "An ACM Executive Committee Position on the Crisis in Experimental Computer Science"}
{"DOCID": "3162", "TEXT": "On Improving the Worst Case Running Time of the Boyer-Moore String Matching Algorithm: It is shown how to modify the Boyer-Moore string matching algorithm so that its worst case running time is linear even when multiple occurrences of the pattern are present in the text."}
{"DOCID": "3163", "TEXT": "An Optimal Insertion Algorithm for One-Sided Height-Balanced BInary Search Trees: An algorithm for inserting an element into a one-sided height-balanced (OSHB) binary search tree is presented.  The algorithm operates in time O(log n), where n is the number of nodes in the tree.  This represents an improvement over the best previous ly known insertion algorithms of Hirschberg and Kosaraju, which require time O(log 2n).  Moreover, the O(log n) complexity is optimal. Earlier results have shown that deletion in such a structure can also be performed in O(log n) time.  Thus the result of this paper gives a negative answer to the question of whether such trees should be the first examples of their kind, where deletion has a smaller time complexity than insertion.  Furthermore, it can now be concluded that insertion, deletion, and retrieval in OSHB trees can be performed in the same time as the corresponding operations for the more general AVL trees, to within a constant factor.  However, the insertion and deletion algorithms for OSHB trees appear much more complicated than the corresponding algorithms for AVL trees."}
{"DOCID": "3164", "TEXT": "Progressive Acyclic Digraphs-A Tool for Database Integrity: A progressive acyclic digraph (PAD) algorithm accepts are requests and maintains a graph in an acyclic state.  When a request creates a cycle, nodes are, \"detached\" until the new are can be entered acyclically This process is important in certain areas of database implementation in which there are constraints on the permissible sequences of actions. Two PAD algorithms are presented; one uses a simple path matrix representation and the other uses a list with an \"artificial gradient.\"  Experiments suggest that for large N the second is considerably faster, though both are asymptotically O(NR), where N is the number of nodes and R is the expected number of nodes reachable along paths from any given node."}
{"DOCID": "3165", "TEXT": "Approximation of Polygonal Maps by Cellular Maps: The approximation of polygonal thematic maps by cellular maps, an important operation in geographical data processing, is analyzed.  The data organization used for representing the polygonal maps is a widely used segment-based data structure, where class labels identify the regions bordering each segment on either side. The approximation algorithm presented operates on such an organization, eliminating the need for the recognition of region boundaries. Each segment is examined only once.  The versatility of the new organization is further illustrated by the outline of algorithms for area computation and point inclusion.  The algorithm is applied to a set of soil maps converted to computer-readable form by means of a coordinate digitizer."}
{"DOCID": "3166", "TEXT": "Computing Standard Deviations: Accuracy: Four algorithms for the numerical computation of the standard deviation of (unweighted) sampled data are analyzed.  Two of the algorithms are well-known in the statistical and computational literature; the other two are new algorithms specifically intended for automatic computation.  Our discussion is expository, with emphasis on reaching a suitable definition of \"accuracy.\"  Each of the four algorithms is analyzed for the conditions under which it will be accurate.  We conclude that all four algorithms will provide accurate answers for many problems, but two of the algorithms, one new, one old, are substantially more accurate on difficult problems than are the other two."}
{"DOCID": "3167", "TEXT": "Updating Mean and Variance Estimates: An Improved Method: A method of improved efficiency is given for updating the mean and variance of weighted sampled data when an additional data value is included in the set.  Evidence is presented that the method is stable and at least as accurate as the best existing updating method."}
{"DOCID": "3168", "TEXT": "Comment on \"An Optimal Evaluation of Boolean Expressions in an Online Query System.\""}
{"DOCID": "3169", "TEXT": "Note on \"An Optimal Evaluation of Boolean Expressions in an Online Query System.\""}
{"DOCID": "3170", "TEXT": "On the Proof of Correctness of a Calendar Program: A formal specification is given for a simple calendar program, and the derivation and proof of correctness of the program are sketched.  The specification is easy to understand, and its correctness is manifest to humans."}
{"DOCID": "3171", "TEXT": "Line Numbers Made Cheap: A technique is described for run-time line number administration to be used for implementations of high level languages.  Under suitable circumstances, this method requires absolutely no overhead, in either time or space, during execution of the program."}
{"DOCID": "3172", "TEXT": "An Algorithm for Planning Collision-Free Paths Among Polyhedral Obstacles: This paper describes a collision avoidance algorithm for planning a safe path for a polyhedral object moving among known polyhedral objects.  The algorithm transforms the obstacles so that they represent the locus of forbidden positions for an arbitrary reference point on the moving object.  A trajectory of this reference point which avoids all forbidden regions is free of collisions. Trajectories are found by searching a network which indicates, for each vertex in the transformed obstacles, which other vertices can be reached safely."}
{"DOCID": "3173", "TEXT": "A Psychology of Learning BASIC: This paper addresses the question: What does a person know following learning of BASIC programming?  Several underlying conceptual structures are identified: (1) a transaction is an event that occurs in the computer and involves some operation on some object at some location, (2) a prestatement is a set of transactions corresponding to a line of code, (3) chunks are frequently occurring configurations of prestatements corresponding to several lines of code."}
{"DOCID": "3174", "TEXT": "Password Security: A Case History: This paper describes the history of the design of the password security scheme on a remotely accessed time-sharing system. The present design was the result of countering observed attempts to penetrate the system.  The result is a compromise between extreme security and ease of use."}
{"DOCID": "3175", "TEXT": "Breaking Substitution Ciphers Using a Relaxation Algorithm: Substitution ciphers are codes in which each letter of the alphabet has one fixed substitute, and the word divisions do not change.  In this paper the problem of breaking substitution ciphers is represented as a probabilistic labeling problem. Every code letter is assigned probabilities of representing plain text letters.  These probabilities are updated in parallel for all code letters, using joint letter probabilities.  Iterating the updating scheme results in improved estimates that finally lead to breaking the cipher.  The method is applies successfully to two examples."}
{"DOCID": "3176", "TEXT": "Storing a Sparse Table: The problem of storing and searching large sparse tables is ubiquitous in computer science.  The standard technique for storing such tables is hashing, but hashing has poor worst-case performance.  We propose a good worst-case method for storing a static table of n entries, each an integer between 0 and N - 1.  The method requires 0(n) w words of storage and allows O(logn N) access time.  Although our method is a little complicated to use in practice, our analysis shows why a simpler algorithm used for compressing LR parsing tables works so well."}
{"DOCID": "3177", "TEXT": "How to Share a Secret: In this paper we show how to divide data D into n pieces in such a way that D is easily reconstructable from any k pieces, but even complete knowledge of k - 1 pieces reveals olutely no information about D.  This technique enables the construction of robust key management schemes for cryptographic systems that can function securely and reliably even when misfortunes destroy half the pieces and security breaches expose all but one of the remaining pieces."}
{"DOCID": "3178", "TEXT": "Introduction to the EFT Symposium"}
{"DOCID": "3179", "TEXT": "Overview of the EFT Symposium: It is increasingly recognized that large-scale technologies such as EFT have the potential for aiding in the solution of current societal problems. Yet, these technologies also generate problems.  This symposium presents selected papers from a conference that sought to discover what is currently known about EFT impacts in society and what research is needed in the future."}
{"DOCID": "3180", "TEXT": "Costs of the Current U.S. Payments System: Neither the banking industry nor public policy makers have good information on the comparative costs of alternative payment systems such as cash, checks, credit cards, and EFT transactions.  As a result, EFT systems and services are likely to be implemented without a valid assessment of whether they are cost-justified, lst alone justified in terms of other criteria."}
{"DOCID": "3181", "TEXT": "Public Protection and Education with EFT: Research has revealed the existence of widespread misinformation and lack of knowledge about EFT among business and government as well as consumers.  As a result, any effort to stimulate meaningful public participation in decisions on the introduction of EFT systems will require a coordinated educational effort of considerable scale.  In addition, research has revealed shortcomings in the present system for defining responsibilities, liabilities, and avenues of recourse.  THis article presents several possible alternatives for improving the current system, but ongoing research is also needed to assure that actions taken will be responsive to the changing environment and consumer needs."}
{"DOCID": "3182", "TEXT": "Vulnerabilities of EFTs to Intentionally Caused Losses: The hypothesis that consumers are provided greater accuracy and freedom from error and fraud with electronic funds transfer systems (EFTs) is discussed in light of the technical capabilities and potential of the computer to protect against both accidentally and intentionally caused losses. Although the nomenclature for business crimes remains the same as for manual depository and other financial service systems - for example, fraud, theft, embezzlement - the characteristics of the crimes are new. The changes resulting from the accelerating use of EFTs and its continual technological advances broaden the scope of security issues to be examined.  Factors such as backup requirements, regulatory and legislative actions, and economics give rise to the urgency for immediate research into solutions for emerging EFTs - related vulnerabilities."}
{"DOCID": "3183", "TEXT": "Policy, Values, and EFT Research: Anatomy of a Research Agenda: There is an emerging recognition that EFT systems have the potential to vastly alter the payment and fund transfer system in American society. A number of forces and actors are involved in this evolution, and the values vary significantly depending on individual and institutional perspectives. These value conflicts are highlighted in a six-part research agenda: technological issues in EFT, EFT impacts on people, economic impact of EFT, regulation and control of EFT, and evaluating and monitoring EFT systems."}
{"DOCID": "3184", "TEXT": "Revised Report on the Algorithmic Language ALGOL 60: The report gives a complete defining description of the international algorithmic language ALGOL 60. This is a language suitable for expressing a large class of numerical processes in a form sufficiently concise for direct automatic translation into the language of programmed automatic computers."}
{"DOCID": "3185", "TEXT": "The Humble Programmer: We shall do a much better programming job, provided that we approach the task with a full appreciation if its tremendous difficulty, provided that we stick to modest and elegant programming languages, provided that we respect the intrinsic limitations of the human mind and approach the task as Very Humble Programmers."}
{"DOCID": "3186", "TEXT": "GO TO Statement Considerd Harmful"}
{"DOCID": "3187", "TEXT": "Certification of Algorithm 271 (QUICKERSORT): QUICKERSORT compiled and run without correction through the ALDEP translator for the CDC 1604A. Comparison of average sorting items with other recently published algorithms demonstrates QUICKERSORT's superior performance."}
{"DOCID": "3188", "TEXT": "Semiotics and Programming Languages: I have based my paper on semiotics and its three dimension. I should insert at this point that language has many aspects and that pragmatics, semantics and syntactics do not necessary cover all of them. One can, however, project most aspects into the three semiotic dimension and there seems to be a strong tendency to do so today."}
{"DOCID": "3189", "TEXT": "An Algebraic Compiler for the FORTRAN Assembly Program: An algebraic compiler has been written which may be added to the FORTRAN Assembly Program. This compiler will expand all algebraic statements with the following operations: addition, subtraction, multiplication and division. It will compile multi-level expressions in floating-point arithmetic (this is easily be revised to fixed-point)."}
{"DOCID": "3190", "TEXT": "Correction to Economies of Scale and the IBM System/360: On page 439, a \"typical\" instruction mix id discussed and the timing computed as outlined in that page. Through an undetected programming error, the times and the resulting regression equation are slightly in error."}
{"DOCID": "3191", "TEXT": "Generating Permutations by Nested Cycling: The purpose of this letter is two_fold: first to give due credit to the Tompkins-Paige algorithm, and second to clarify a comment by Hill, CR Review 13891 on \"Programs for Permutations\"."}
{"DOCID": "3192", "TEXT": "The Lincoln Keyboard - a Typewriter Keyboard Designed for Computers Input Flexibility: A new typewriter keyboard, for direct and punched paper tape computer input will replace the usual commercial keyboard with 88 characters chosen for the convenience  of programmers. The Lincoln Keyboard is expected to facilitate the programming of algorithmic process and should allow considerable flexibility in assembly and utility routines."}
{"DOCID": "3193", "TEXT": ": Work is in progress on a formula coding technique allowing direct entry into the computer of formulae typed on an 84 character Flexo-writer. This Flexo-writer will be modified for automatic half-line advance and retract, without carriage return, to permit completely general sub and superscripting."}
{"DOCID": "3194", "TEXT": "A Non-heuristic Program for Proving Elementary Logical Theorems: The paper discusses problems involved in designing a device capable of distinguishing among speech events that are normally recognized as different by native speakers of a particular language. Parallels between these problems and those of chemical analysis are pointed out."}
{"DOCID": "3195", "TEXT": "Reiteration of ACM Policy Toward Standardization: The periodic change in officers, chairman and editors which usually follows as election occasionally results in a change in policy. In the case of this department there is no radical change, but this is nevertheless the proper time to reiterate ans underline ACM's policy with respect to standardization in the computer area."}
{"DOCID": "3196", "TEXT": "The Reactive Typewriter Program: 84-character keyboard including alphabetical upper and lower case for good readability. If the machine is restricted to only a single case, the lower case is preferred. The reactive typewriter should be portable. the reactive typewriter should operate over any commercially used, dial-type telephone (voice) or telegraph (Telex) line or over leased (nondial) telegraph lines interchangeably."}
{"DOCID": "3197", "TEXT": "Structures of Standards-Processing Organizations in the Computer Area: In line with the ACM's policy statement [Comm. ACM 5 (Nov. 1962), 547-549], the following organizational descriptions have been provided in order to describe standardization activities pertinent to computers and information processing."}
{"DOCID": "3198", "TEXT": "Microprogramming, Emulators and Programming Languages: The problem we have been concerned with is that of converting language to action - or intellectual energy to mechanical energy. The medium that we use for this purpose is language and therefore we are preoccupied with the subject of language. In the areas of language investigation we have concentrated first on formalizing syntax and then on semantics."}
{"DOCID": "3199", "TEXT": "ALGEM - An Algebraic Manipulator: ALGEM is a package of subprograms written in Slip, FORTRAN IV and MAP 7094 II to manipulate algebraic expressions. Algem's basic algebraic operations are additions, subtractions, multiplications, division and exponentiation. It is capable of handling any number of single letter variables, variable exponents, and of finding the highest common factor of two polynomials. Also included are such functions as substitution, differentiation, determining coefficients of specified variables, solving a linear equation, basic I/O routines plus other special purpose and arithmetic routines. The major innovation of Algem over other manipulators is the assignment of types to all expressions and the use of a standard ordering procedure."}
{"DOCID": "3200", "TEXT": "A FORMAC Program for the Solution of Linear Boundary and Initial Value Problems: A computer program is described which has been developed for obtaining approximate solutions to linear initial and boundary-value problems involving differential equations. For each problem, input to the program includes: 1. The equations (in symbolic form) to be satisfied  -  the differential equations, equations describing auxiliary conditions such as boundary conditions, etc. 2. A numerical description of the regions in which each of the equations are to be satisfied. 3. Sets of functions (in symbolic form) to be used in linear combinations to approximate the solution functions. Give the above input, the program generates an approximation to the solutions of the specified problemm in terms of the specified functions which is optimum in the least-squares sense."}
{"DOCID": "3201", "TEXT": "Symbolic Manipulation of Poisson Series: Poisson series of three variables are manageable symbolically through as a set of formal subroutines written partially in the IBM 7094 machine language, but to be called in the FORTRAN language for use in Fortran  programs. An effort has been made to supply those operations which are most required by celestial mechanics. The routines are entirely self-contained subroutines and require only standard Fortran input/output units 5 and 6; they are design to avoid waste and overflow of core storage space."}
{"DOCID": "3202", "TEXT": "MANIP: A Computer System for Algebra and Analytic Differentiation: A mathematical expression to be operated upon is written in FORTRAN-like notation and stored in the computer as a string of BCD characters with all blanks removed. It may be as complicated as desired (parentheses nested without restriction, etc.) so long as the entire expression (or any subsequent form) does not exceed 5000 characters. The problemm of performing algebraic operations and obtaining analytic derivatives was translated into that of identifying and manipulating character sequences. Programs which resulted were written in FORTRAN IV for a CDC 3600 and are discussed in detail."}
{"DOCID": "3203", "TEXT": "GRAD Assistant - A Program for Symbolic Algebraic Manipulation and Differentiation: The General Recursive Algebra and Differentiation Assistant (GRAD Assistant) now under development is a set of LISP functions which symbolically manipulate abd differentiate algebraic expressions. It is designed for use with problemms in which a large amount of routine manipulation is to be done by a program without human intervention. Thus, GRAD must recognize necessary simplifications without external guidance. While some complicated expressions (notably ones involving nested radicals and trigonometric functions) do not yield completely to the present version, it has proved quite useful indeed."}
{"DOCID": "3204", "TEXT": "An On-Line Program for Non-Numerical Algebra: The goal of this program is to make a step toward te design of an automated mathematical assistant. Some requirements for such a program are: it must be easy to access, and that the result must be obtained in a reasonably short time. Accordingly the program is written for a time-shared computer. The Q-32 computer as System Development Corporation, Santa Monica, California, was chosen because it also had a LISP 1.5 compiler. Programming and debugging was done from a remote teletype console at Stanford University."}

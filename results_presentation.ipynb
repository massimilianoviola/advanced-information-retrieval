{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results presentation\n",
    "In this notebook, the outputs of the experiment folders are loaded for visualization purposes. The analysis made by trec_eval shows the MAP for each query and the overall MAP. When executing the notebook, all the plots get saved in the `plots/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, initialization of constants, general set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "DATASETS = [\"cacm\", \"med\", \"npl\"]\n",
    "!mkdir -p plots/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(result_file_names, axis_descriptions, title, fig_filename=\"\"):\n",
    "    figure, axis = plt.subplots(1, 3, figsize=(14, 5))\n",
    "    for i, dataset in enumerate(DATASETS):\n",
    "        # load results from trec_eval format\n",
    "        x = pd.read_csv(result_file_names[0] % dataset, sep=\"\\t\", header=None).sort_values(by=[1])[2].to_numpy()\n",
    "        y = pd.read_csv(result_file_names[1] % dataset, sep=\"\\t\", header=None).sort_values(by=[1])[2].to_numpy()\n",
    "\n",
    "        # plot the results nicely\n",
    "        axis[i].set_aspect(1)\n",
    "        axis[i].scatter(x[:-1], y[:-1], c=\"m\", label=\"Score for each query\")\n",
    "        axis[i].scatter(x[-1], y[-1], s=[200], c=\"c\", label=\"Average score\")\n",
    "        axis[i].plot(np.linspace(-0.025, 1.025), np.linspace(-0.025, 1.025), label=\"Identity line\", c=\"orange\", alpha=0.75)\n",
    "        axis[i].set_xlim(-0.025, 1.025)\n",
    "        axis[i].set_ylim(-0.025, 1.025)\n",
    "        axis[i].set_xlabel(axis_descriptions[0])\n",
    "        axis[i].set_ylabel(axis_descriptions[1])\n",
    "        axis[i].set_title(f\"{dataset} dataset\")\n",
    "        axis[i].legend()\n",
    "    \n",
    "    # remove unnecessary white space on the plot\n",
    "    plt.subplots_adjust(left=0.05, bottom=0.05, right=0.98, top=0.91)\n",
    "    figure.suptitle(title, fontsize=16)\n",
    "\n",
    "    # save the plot if a file is specified\n",
    "    if fig_filename:\n",
    "        plt.savefig(fig_filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def bar_chart_plot(x_input_template, y_input_template, legend_names, barchart_name, fig_filename=\"\"):\n",
    "    # Create an empty list to store the data frames\n",
    "    df_list_one = []\n",
    "    df_list_two = []\n",
    "\n",
    "    # Iterate over the input files\n",
    "    for dataset in DATASETS:\n",
    "        # Read the file into a data frame\n",
    "        df = pd.read_csv(x_input_template % dataset, sep='\\t', names=['map', 'index', 'value'])\n",
    "\n",
    "        # Extract the rows with the 'all' index\n",
    "        df_all = df[df['index'] == 'all']\n",
    "        value = df_all['value'].iloc[0]\n",
    "        # Add the data frame to the list\n",
    "        df_list_one.append(value)\n",
    "\n",
    "        # Read the file into a data frame\n",
    "        df = pd.read_csv(y_input_template % dataset, sep='\\t', names=['map', 'index', 'value'])\n",
    "\n",
    "        # Extract the rows with the 'all' index\n",
    "        df_all = df[df['index'] == 'all']\n",
    "        value = df_all['value'].iloc[0]\n",
    "        # Add the data frame to the list\n",
    "        df_list_two.append(value)\n",
    "\n",
    "\n",
    "    # Create a data frame with the values and names\n",
    "    df = pd.DataFrame({'values_one': df_list_one, 'values_two': df_list_two, 'names': DATASETS})\n",
    "\n",
    "    # Set the bar width\n",
    "    bar_width = 0.4\n",
    "\n",
    "    # Calculate the x-coordinates for the bars\n",
    "    x = range(len(df))\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x, df['values_one'], width=bar_width, label=legend_names[0], color='m')\n",
    "    ax.bar([i + bar_width for i in x], df['values_two'], width=bar_width, label=legend_names[1], color='c')\n",
    "    ax.set_ylabel(\"Mean Average Precision\")\n",
    "    ax.set_title(barchart_name)\n",
    "    ax.set_xticks([i + bar_width / 2 for i in x], df['names'])\n",
    "    ax.legend()\n",
    "\n",
    "    if fig_filename:\n",
    "        plt.savefig(fig_filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def bar_chart_plot_4_in_1(x_input_templates, y_input_templates, legend_names, barchart_name, fig_filename=\"\"):\n",
    "    \n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    for i, (x_input_template, y_input_template) in enumerate(zip(x_input_templates, y_input_templates)):\n",
    "        # Create an empty list to store the data frames\n",
    "        df_list_one = []\n",
    "        df_list_two = []\n",
    "\n",
    "        # Iterate over the input files\n",
    "        for dataset in DATASETS:\n",
    "            # Read the file into a data frame\n",
    "            df = pd.read_csv(x_input_template % dataset, sep='\\t', names=['map', 'index', 'value'])\n",
    "\n",
    "            # Extract the rows with the 'all' index\n",
    "            df_all = df[df['index'] == 'all']\n",
    "            value = df_all['value'].iloc[0]\n",
    "            # Add the data frame to the list\n",
    "            df_list_one.append(value)\n",
    "\n",
    "            # Read the file into a data frame\n",
    "            df = pd.read_csv(y_input_template % dataset, sep='\\t', names=['map', 'index', 'value'])\n",
    "\n",
    "            # Extract the rows with the 'all' index\n",
    "            df_all = df[df['index'] == 'all']\n",
    "            value = df_all['value'].iloc[0]\n",
    "            # Add the data frame to the list\n",
    "            df_list_two.append(value)\n",
    "\n",
    "\n",
    "        # Create a data frame with the values and names\n",
    "        df = pd.DataFrame({'values_one': df_list_one, 'values_two': df_list_two, 'names': DATASETS})\n",
    "\n",
    "        # Set the bar width\n",
    "        bar_width = 0.4\n",
    "\n",
    "        # Calculate the x-coordinates for the bars\n",
    "        x = range(len(df))\n",
    "\n",
    "        # Create the plot\n",
    "        j = i // 2\n",
    "        k = i & 1\n",
    "        ax[j, k].bar(x, df['values_one'], width=bar_width, label=legend_names[i][0], color='m')\n",
    "        ax[j, k].bar([i + bar_width for i in x], df['values_two'], width=bar_width, label=legend_names[i][1], color='c')\n",
    "        ax[j, k].set_ylabel(\"Mean Average Precision\")\n",
    "        ax[j, k].set_xticks([i + bar_width / 2 for i in x], df['names'])\n",
    "        ax[j, k].legend()\n",
    "\n",
    "    fig.suptitle(barchart_name, fontsize=16)\n",
    "    plt.subplots_adjust(left=0.055, bottom=0.03, right=0.985, top=0.95, hspace=0.08, wspace=0.15)\n",
    "\n",
    "    if fig_filename:\n",
    "        plt.savefig(fig_filename, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 01\n",
    "In this task, we compare the BM25 ranking function with the cosine similarity search using embeddings created by different language models from sentence transformers. The behaviour for individual queries and the overall MAP for each dataset can be seen in the scatterplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"average_word_embeddings_glove.6B.300d\", \"all-MiniLM-L6-v2\", \"all-MiniLM-L12-v2\", \"all-mpnet-base-v2\"]\n",
    "shortcuts = [\"glove\", \"minil6\", \"minil12\", \"mpnetv2\"]\n",
    "\n",
    "for m, s in zip(models, shortcuts):\n",
    "    # model m vs BM25\n",
    "    scatter_plot([f\"./task_01/results/map_%s_{s}.txt\", \"./task_01/results/map_%s_bm25.txt\"],\n",
    "                 [m, \"bm25\"], \"Algorithmic vs ML empowered search\", f\"plots/{s}_vs_bm25\")\n",
    "\n",
    "# glove vs mpnetv2\n",
    "scatter_plot([\"./task_01/results/map_%s_mpnetv2.txt\", \"./task_01/results/map_%s_glove.txt\"],\n",
    "             [\"all-mpnet-base-v2\", \"average_word_embeddings_glove.6B.300d\"],\n",
    "             \"Language model vs GloVe\", \"plots/glove_vs_mpnetv2\")\n",
    "\n",
    "# bar plot models vs BM25\n",
    "bar_chart_plot_4_in_1([\"./task_01/results/map_%s_bm25.txt\"] * 4, \n",
    "                      [\"./task_01/results/map_%s_glove.txt\", \"./task_01/results/map_%s_minil6.txt\",\n",
    "                       \"./task_01/results/map_%s_minil12.txt\", \"./task_01/results/map_%s_mpnetv2.txt\"], \n",
    "                      [[\"BM25\", m] for m in models], \n",
    "                      \"Algorithmic vs ML empowered search\", \"plots/task_01_bar_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 02\n",
    "In this task, we use Google pegasus-xsum to summarize all the data, distilbart-cnn-12-6 to summarize only documents whose length is greater than 700 characters, and compare the results of the unsummarized and summarized versions using BM25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not summarized vs fully summarized\n",
    "scatter_plot([\"./task_02/results/map_full_summarized_%s_bm25.txt\", \"./task_02/results/map_%s_bm25.txt\"],\n",
    "             [\"full summarized bm25\", \"bm25\"], \"Fullly summarized and not summarized search comparison\", \"plots/full_summarized_bm25_vs_bm25\")\n",
    "bar_chart_plot(\"./task_02/results/map_%s_bm25.txt\", \"./task_02/results/map_full_summarized_%s_bm25.txt\",\n",
    "               [\"Not summarized\", \"Fullly summarized\"], \"Fully summarized and not summarized search comparison\", \"plots/fully_summarized_bm25_vs_bm25_bar\")\n",
    "\n",
    "# not summarized vs partly summarized\n",
    "scatter_plot([\"./task_02/results/map_partly_summarized_%s_bm25.txt\", \"./task_02/results/map_%s_bm25.txt\"],\n",
    "             [\"partly summarized bm25\", \"bm25\"], \"Partly summarized and not summarized search comparison\", \"plots/partly_summarized_bm25_vs_bm25\")\n",
    "bar_chart_plot(\"./task_02/results/map_%s_bm25.txt\", \"./task_02/results/map_partly_summarized_%s_bm25.txt\", \n",
    "               [\"Not summarized\", \"Partly summarized\"], \"Partly summarized and not summarized search comparison\", \"plots/partly_summarized_bm25_vs_bm25_bar\")\n",
    "\n",
    "# plot percentage loss in length and accuracy\n",
    "plot_4_bar_chart(\"./task_02/results/summary_strength_vs_accuracy_loss.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 03\n",
    "In this task, we translate documents and queries from English to German and perform a monolingual search with both BM25 and a language model to see the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translated vs not translated BM25\n",
    "scatter_plot([\"./task_03/results/map_ger_%s_bm25.txt\", \"./task_03/results/map_%s_bm25.txt\"],\n",
    "             [\"Pre-translated German bm25\", \"Original English bm25\"], \"Translated and not translated search comparison\", \"plots/translated_not_translated_bm25\")\n",
    "bar_chart_plot(\"./task_03/results/map_%s_bm25.txt\", \"./task_03/results/map_ger_%s_bm25.txt\",\n",
    "               [\"Original English bm25\", \"Pre-translated German bm25\"], \"Translated and not translated search comparison\", \"plots/translated_not_translated_bm25_bar\")\n",
    "\n",
    "# translated vs not translated ml_minilm_l12_v2\n",
    "scatter_plot([\"./task_03/results/map_ger_%s_ml_minilm_l12_v2.txt\", \"./task_03/results/map_%s_ml_minilm_l12_v2.txt\"],\n",
    "             [\"Pre-translated German ml_minilm_l12_v2\", \"Original English ml_minilm_l12_v2\"],\n",
    "             \"Translated and not translated search comparison\", \"plots/translated_not_translated_ml_minilm_l12_v2\")\n",
    "bar_chart_plot(\"./task_03/results/map_%s_ml_minilm_l12_v2.txt\", \"./task_03/results/map_ger_%s_ml_minilm_l12_v2.txt\",\n",
    "               [\"Original English ml_minilm_l12_v2\", \"Pre-translated German ml_minilm_l12_v2\"],\n",
    "               \"Translated and not translated search comparison\", \"plots/translated_not_translated_ml_minilm_l12_v2_bar\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 04\n",
    "In this task we evaluated the performance of pre-trained multilingual sentence transformers compared to their monolingual counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multilingual sentence transformers models\n",
    "models = [\"paraphrase-multilingual-MiniLM-L12-v2\", \"paraphrase-multilingual-mpnet-base-v2\"]\n",
    "shortcuts = [\"ml_minilm_l12_v2\", \"ml_mpnet_base_v2\"]\n",
    "\n",
    "# compare performance of German queries vs English queries on English docs\n",
    "for shortcut in shortcuts:\n",
    "    scatter_plot([\n",
    "        f\"./task_04/results/pretrained/map_{shortcut}_%s_DE.txt\",\n",
    "        f\"./task_04/results/pretrained/map_{shortcut}_%s_EN.txt\"\n",
    "    ], [f\"{shortcut}_DE\", f\"{shortcut}_EN\"], \"Original vs translated queries on English docs\",\n",
    "                 f\"plots/DE_vs_EN_{shortcut}_on_EN\")\n",
    "\n",
    "\n",
    "# compare performance of German queries vs English queries on German docs\n",
    "for shortcut in shortcuts:\n",
    "    scatter_plot([\n",
    "        f\"./task_04/results/DE/map_{shortcut}_%s_DE.txt\", f\"./task_04/results/DE/map_{shortcut}_%s_EN.txt\"\n",
    "    ], [f\"{shortcut}_DE\", f\"{shortcut}_EN\"], \"Original vs translated queries on German docs\",\n",
    "                 f\"plots/DE_vs_EN_{shortcut}_on_DE\")\n",
    "\n",
    "\n",
    "# mono-lingual sentence transformers models\n",
    "models = [\"all-MiniLM-L12-v2\", \"all-mpnet-base-v2\"]\n",
    "shortcuts = [\"miniLM_L12_v2\", \"mpnet_base_v2\"]\n",
    "\n",
    "for shortcut in shortcuts:\n",
    "    # compare performance of German queries vs English queries (monolingual)\n",
    "    scatter_plot([\n",
    "        f\"./task_04/results/monolingual/map_{shortcut}_%s_DE.txt\",\n",
    "        f\"./task_04/results/monolingual/map_{shortcut}_%s_EN.txt\"\n",
    "    ], [f\"{shortcut}_DE\", f\"{shortcut}_EN\"], \"German (translated to English) vs English queries \",\n",
    "                 f\"plots/DE_vs_EN_{shortcut}_monolingual\")\n",
    "\n",
    "    # compare performance of monolingual models vs their multilingual counterparts\n",
    "    # on English queries\n",
    "    scatter_plot(\n",
    "        [\n",
    "            f\"./task_04/results/monolingual/map_{shortcut}_%s_EN.txt\",  # monolingual\n",
    "            f\"./task_04/results/pretrained/map_ml_{shortcut}_%s_EN.txt\"  # multilingual\n",
    "        ],\n",
    "        [f\"{shortcut}_DE\", f\"{shortcut}_EN\"],\n",
    "        f\"Monolingual vs multilingual {shortcut} on English queries\",\n",
    "        f\"plots/DE_vs_EN_{shortcut}_vs_ml_{shortcut}_on_EN\")\n",
    "\n",
    "    # on German queries (translated to English before search)\n",
    "    scatter_plot(\n",
    "        [\n",
    "            f\"./task_04/results/monolingual/map_{shortcut}_%s_DE.txt\",  # monolingual\n",
    "            f\"./task_04/results/pretrained/map_ml_{shortcut}_%s_DE.txt\"  # multilingual\n",
    "        ],\n",
    "        [f\"{shortcut}_DE\", f\"{shortcut}_EN\"],\n",
    "        f\"Monolingual vs multilingual {shortcut} on German queries\",\n",
    "        f\"plots/DE_vs_EN_{shortcut}_vs_ml_{shortcut}_on_DE\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
